{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d078e5d1-57df-4c73-88ba-3d88e415615b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: efficientnet_pytorch in /opt/conda/lib/python3.7/site-packages (0.7.1)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from efficientnet_pytorch) (1.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet_pytorch) (4.5.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet_pytorch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet_pytorch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet_pytorch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet_pytorch) (11.7.99)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->efficientnet_pytorch) (67.7.2)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->efficientnet_pytorch) (0.40.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install efficientnet_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeb1878d-d01a-49b5-ab43-653f9a5a7558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from efficientnet_pytorch import EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94c6a783-ee5f-4577-9a36-1d6569414ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b811925-88b1-4ac4-aee1-efe144c82b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.classes = self.data['hotel_id'].unique().tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, str(self.data.iloc[idx, 1]), str(self.data.iloc[idx, 0]))\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        label = self.classes.index(self.data.iloc[idx, 1])\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5baab643-992b-427c-b711-e96873f4612a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CustomDataset('train.csv', 'final/train_images', transform=data_transforms)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "num_features = model._fc.in_features\n",
    "model._fc = nn.Linear(num_features, len(train_dataset.classes))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee19ca87-e580-4ec4-a03b-be0def0f19b5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (_conv_stem): Conv2dStaticSamePadding(\n",
       "    3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
       "    (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
       "  )\n",
       "  (_bn0): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (_blocks): ModuleList(\n",
       "    (0): MBConvBlock(\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n",
       "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(16, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (1): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n",
       "        (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (2): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (3): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d((1, 2, 1, 2))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (4): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (5): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (6): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (7): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (8): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (9): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (10): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (11): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d((1, 2, 1, 2))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (12): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (13): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (14): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (15): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(320, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "  )\n",
       "  (_conv_head): Conv2dStaticSamePadding(\n",
       "    320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (static_padding): Identity()\n",
       "  )\n",
       "  (_bn1): BatchNorm2d(1280, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
       "  (_dropout): Dropout(p=0.2, inplace=False)\n",
       "  (_fc): Linear(in_features=1280, out_features=3116, bias=True)\n",
       "  (_swish): MemoryEfficientSwish()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77e34eb7-b0ed-45a1-917c-614eb9a77905",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "best_accuracy = 0.0  # Track the best validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea8bd9ba-d689-445b-a25f-db85e1ed9e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []  \n",
    "val_losses = []  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5502fe-17e8-4fc2-9313-dd80d4cf07b2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/1072, Loss: 8.0216\n",
      "Iteration 2/1072, Loss: 7.9548\n",
      "Iteration 3/1072, Loss: 7.9567\n",
      "Iteration 4/1072, Loss: 7.9579\n",
      "Iteration 5/1072, Loss: 8.0003\n",
      "Iteration 6/1072, Loss: 8.0103\n",
      "Iteration 7/1072, Loss: 7.9899\n",
      "Iteration 8/1072, Loss: 7.9685\n",
      "Iteration 9/1072, Loss: 8.0483\n",
      "Iteration 10/1072, Loss: 7.9695\n",
      "Iteration 11/1072, Loss: 8.0151\n",
      "Iteration 12/1072, Loss: 8.0471\n",
      "Iteration 13/1072, Loss: 7.9698\n",
      "Iteration 14/1072, Loss: 7.9923\n",
      "Iteration 15/1072, Loss: 7.9362\n",
      "Iteration 16/1072, Loss: 7.9968\n",
      "Iteration 17/1072, Loss: 8.0134\n",
      "Iteration 18/1072, Loss: 7.9999\n",
      "Iteration 19/1072, Loss: 7.9504\n",
      "Iteration 20/1072, Loss: 8.0040\n",
      "Iteration 21/1072, Loss: 7.9802\n",
      "Iteration 22/1072, Loss: 8.0226\n",
      "Iteration 23/1072, Loss: 8.0174\n",
      "Iteration 24/1072, Loss: 7.9674\n",
      "Iteration 25/1072, Loss: 8.0241\n",
      "Iteration 26/1072, Loss: 7.9192\n",
      "Iteration 27/1072, Loss: 7.9585\n",
      "Iteration 28/1072, Loss: 8.0350\n",
      "Iteration 29/1072, Loss: 7.9736\n",
      "Iteration 30/1072, Loss: 7.9973\n",
      "Iteration 31/1072, Loss: 7.9063\n",
      "Iteration 32/1072, Loss: 7.9973\n",
      "Iteration 33/1072, Loss: 7.9378\n",
      "Iteration 34/1072, Loss: 7.9650\n",
      "Iteration 35/1072, Loss: 7.9834\n",
      "Iteration 36/1072, Loss: 7.9731\n",
      "Iteration 37/1072, Loss: 8.0101\n",
      "Iteration 38/1072, Loss: 8.0269\n",
      "Iteration 39/1072, Loss: 7.9646\n",
      "Iteration 40/1072, Loss: 8.0137\n",
      "Iteration 41/1072, Loss: 7.9908\n",
      "Iteration 42/1072, Loss: 7.9601\n",
      "Iteration 43/1072, Loss: 8.0114\n",
      "Iteration 44/1072, Loss: 7.9340\n",
      "Iteration 45/1072, Loss: 7.9382\n",
      "Iteration 46/1072, Loss: 7.9821\n",
      "Iteration 47/1072, Loss: 7.9453\n",
      "Iteration 48/1072, Loss: 7.9740\n",
      "Iteration 49/1072, Loss: 7.9856\n",
      "Iteration 50/1072, Loss: 7.9904\n",
      "Iteration 51/1072, Loss: 7.9763\n",
      "Iteration 52/1072, Loss: 7.9461\n",
      "Iteration 53/1072, Loss: 7.9536\n",
      "Iteration 54/1072, Loss: 8.0210\n",
      "Iteration 55/1072, Loss: 7.9751\n",
      "Iteration 56/1072, Loss: 8.0101\n",
      "Iteration 57/1072, Loss: 8.0017\n",
      "Iteration 58/1072, Loss: 7.9879\n",
      "Iteration 59/1072, Loss: 7.9616\n",
      "Iteration 60/1072, Loss: 8.0247\n",
      "Iteration 61/1072, Loss: 7.9900\n",
      "Iteration 62/1072, Loss: 8.0275\n",
      "Iteration 63/1072, Loss: 7.9861\n",
      "Iteration 64/1072, Loss: 7.9629\n",
      "Iteration 65/1072, Loss: 7.9931\n",
      "Iteration 66/1072, Loss: 8.0102\n",
      "Iteration 67/1072, Loss: 7.9778\n",
      "Iteration 68/1072, Loss: 7.9569\n",
      "Iteration 69/1072, Loss: 8.0067\n",
      "Iteration 70/1072, Loss: 7.8984\n",
      "Iteration 71/1072, Loss: 7.9682\n",
      "Iteration 72/1072, Loss: 8.0008\n",
      "Iteration 73/1072, Loss: 7.9088\n",
      "Iteration 74/1072, Loss: 7.9633\n",
      "Iteration 75/1072, Loss: 7.9670\n",
      "Iteration 76/1072, Loss: 7.9658\n",
      "Iteration 77/1072, Loss: 7.9809\n",
      "Iteration 78/1072, Loss: 7.9678\n",
      "Iteration 79/1072, Loss: 7.9553\n",
      "Iteration 80/1072, Loss: 7.9921\n",
      "Iteration 81/1072, Loss: 8.0531\n",
      "Iteration 82/1072, Loss: 7.9249\n",
      "Iteration 83/1072, Loss: 8.0120\n",
      "Iteration 84/1072, Loss: 8.0003\n",
      "Iteration 85/1072, Loss: 7.9830\n",
      "Iteration 86/1072, Loss: 8.0251\n",
      "Iteration 87/1072, Loss: 7.9825\n",
      "Iteration 88/1072, Loss: 8.0014\n",
      "Iteration 89/1072, Loss: 8.0325\n",
      "Iteration 90/1072, Loss: 8.0232\n",
      "Iteration 91/1072, Loss: 8.0012\n",
      "Iteration 92/1072, Loss: 7.9616\n",
      "Iteration 93/1072, Loss: 7.9712\n",
      "Iteration 94/1072, Loss: 8.0197\n",
      "Iteration 95/1072, Loss: 7.9996\n",
      "Iteration 96/1072, Loss: 7.9748\n",
      "Iteration 97/1072, Loss: 7.9370\n",
      "Iteration 98/1072, Loss: 7.9071\n",
      "Iteration 99/1072, Loss: 8.0076\n",
      "Iteration 100/1072, Loss: 8.0551\n",
      "Iteration 101/1072, Loss: 7.9993\n",
      "Iteration 102/1072, Loss: 7.9332\n",
      "Iteration 103/1072, Loss: 7.9636\n",
      "Iteration 104/1072, Loss: 7.9591\n",
      "Iteration 105/1072, Loss: 7.9875\n",
      "Iteration 106/1072, Loss: 7.9835\n",
      "Iteration 107/1072, Loss: 8.0541\n",
      "Iteration 108/1072, Loss: 8.0205\n",
      "Iteration 109/1072, Loss: 7.9853\n",
      "Iteration 110/1072, Loss: 7.9229\n",
      "Iteration 111/1072, Loss: 7.9618\n",
      "Iteration 112/1072, Loss: 7.9993\n",
      "Iteration 113/1072, Loss: 7.9805\n",
      "Iteration 114/1072, Loss: 8.0006\n",
      "Iteration 115/1072, Loss: 7.9488\n",
      "Iteration 116/1072, Loss: 8.0060\n",
      "Iteration 117/1072, Loss: 8.0082\n",
      "Iteration 118/1072, Loss: 7.9454\n",
      "Iteration 119/1072, Loss: 7.9332\n",
      "Iteration 120/1072, Loss: 7.9919\n",
      "Iteration 121/1072, Loss: 8.0031\n",
      "Iteration 122/1072, Loss: 7.9397\n",
      "Iteration 123/1072, Loss: 7.9753\n",
      "Iteration 124/1072, Loss: 8.0012\n",
      "Iteration 125/1072, Loss: 7.9601\n",
      "Iteration 126/1072, Loss: 7.9846\n",
      "Iteration 127/1072, Loss: 7.9893\n",
      "Iteration 128/1072, Loss: 7.9802\n",
      "Iteration 129/1072, Loss: 7.9818\n",
      "Iteration 130/1072, Loss: 7.9518\n",
      "Iteration 131/1072, Loss: 7.9333\n",
      "Iteration 132/1072, Loss: 7.9934\n",
      "Iteration 133/1072, Loss: 7.9952\n",
      "Iteration 134/1072, Loss: 7.9698\n",
      "Iteration 135/1072, Loss: 7.9739\n",
      "Iteration 136/1072, Loss: 8.0668\n",
      "Iteration 137/1072, Loss: 7.9682\n",
      "Iteration 138/1072, Loss: 7.9201\n",
      "Iteration 139/1072, Loss: 7.9607\n",
      "Iteration 140/1072, Loss: 7.9547\n",
      "Iteration 141/1072, Loss: 7.9807\n",
      "Iteration 142/1072, Loss: 8.0505\n",
      "Iteration 143/1072, Loss: 7.9394\n",
      "Iteration 144/1072, Loss: 7.9852\n",
      "Iteration 145/1072, Loss: 7.9650\n",
      "Iteration 146/1072, Loss: 7.9566\n",
      "Iteration 147/1072, Loss: 7.9568\n",
      "Iteration 148/1072, Loss: 8.0083\n",
      "Iteration 149/1072, Loss: 7.9989\n",
      "Iteration 150/1072, Loss: 7.9918\n",
      "Iteration 151/1072, Loss: 7.9610\n",
      "Iteration 152/1072, Loss: 8.0325\n",
      "Iteration 153/1072, Loss: 7.9290\n",
      "Iteration 154/1072, Loss: 7.9821\n",
      "Iteration 155/1072, Loss: 7.9399\n",
      "Iteration 156/1072, Loss: 7.9703\n",
      "Iteration 157/1072, Loss: 7.9740\n",
      "Iteration 158/1072, Loss: 8.0198\n",
      "Iteration 159/1072, Loss: 7.9562\n",
      "Iteration 160/1072, Loss: 7.9752\n",
      "Iteration 161/1072, Loss: 7.9800\n",
      "Iteration 162/1072, Loss: 8.0025\n",
      "Iteration 163/1072, Loss: 7.9915\n",
      "Iteration 164/1072, Loss: 8.0095\n",
      "Iteration 165/1072, Loss: 7.9906\n",
      "Iteration 166/1072, Loss: 7.9833\n",
      "Iteration 167/1072, Loss: 7.9486\n",
      "Iteration 168/1072, Loss: 8.0183\n",
      "Iteration 169/1072, Loss: 7.9573\n",
      "Iteration 170/1072, Loss: 7.9672\n",
      "Iteration 171/1072, Loss: 7.9931\n",
      "Iteration 172/1072, Loss: 7.9760\n",
      "Iteration 173/1072, Loss: 7.9866\n",
      "Iteration 174/1072, Loss: 7.9790\n",
      "Iteration 175/1072, Loss: 8.0175\n",
      "Iteration 176/1072, Loss: 7.9961\n",
      "Iteration 177/1072, Loss: 8.0081\n",
      "Iteration 178/1072, Loss: 7.9587\n",
      "Iteration 179/1072, Loss: 7.9929\n",
      "Iteration 180/1072, Loss: 7.9633\n",
      "Iteration 181/1072, Loss: 7.9786\n",
      "Iteration 182/1072, Loss: 8.0156\n",
      "Iteration 183/1072, Loss: 7.9781\n",
      "Iteration 184/1072, Loss: 7.9934\n",
      "Iteration 185/1072, Loss: 7.9678\n",
      "Iteration 186/1072, Loss: 7.9983\n",
      "Iteration 187/1072, Loss: 7.9608\n",
      "Iteration 188/1072, Loss: 8.0020\n",
      "Iteration 189/1072, Loss: 7.9768\n",
      "Iteration 190/1072, Loss: 8.0227\n",
      "Iteration 191/1072, Loss: 8.0040\n",
      "Iteration 192/1072, Loss: 7.9914\n",
      "Iteration 193/1072, Loss: 7.9894\n",
      "Iteration 194/1072, Loss: 7.9962\n",
      "Iteration 195/1072, Loss: 7.9686\n",
      "Iteration 196/1072, Loss: 8.0352\n",
      "Iteration 197/1072, Loss: 7.9695\n",
      "Iteration 198/1072, Loss: 8.0261\n",
      "Iteration 199/1072, Loss: 8.0189\n",
      "Iteration 200/1072, Loss: 7.9394\n",
      "Iteration 201/1072, Loss: 7.9142\n",
      "Iteration 202/1072, Loss: 8.0015\n",
      "Iteration 203/1072, Loss: 7.9724\n",
      "Iteration 204/1072, Loss: 7.9833\n",
      "Iteration 205/1072, Loss: 7.9900\n",
      "Iteration 206/1072, Loss: 7.9285\n",
      "Iteration 207/1072, Loss: 8.0191\n",
      "Iteration 208/1072, Loss: 7.9740\n",
      "Iteration 209/1072, Loss: 7.8836\n",
      "Iteration 210/1072, Loss: 8.0154\n",
      "Iteration 211/1072, Loss: 7.9329\n",
      "Iteration 212/1072, Loss: 7.9708\n",
      "Iteration 213/1072, Loss: 7.9676\n",
      "Iteration 214/1072, Loss: 7.9381\n",
      "Iteration 215/1072, Loss: 7.9988\n",
      "Iteration 216/1072, Loss: 7.9278\n",
      "Iteration 217/1072, Loss: 7.9753\n",
      "Iteration 218/1072, Loss: 7.9947\n",
      "Iteration 219/1072, Loss: 8.0038\n",
      "Iteration 220/1072, Loss: 7.9897\n",
      "Iteration 221/1072, Loss: 7.9335\n",
      "Iteration 222/1072, Loss: 8.0008\n",
      "Iteration 223/1072, Loss: 7.9689\n",
      "Iteration 224/1072, Loss: 7.9512\n",
      "Iteration 225/1072, Loss: 8.0325\n",
      "Iteration 226/1072, Loss: 7.9256\n",
      "Iteration 227/1072, Loss: 7.9817\n",
      "Iteration 228/1072, Loss: 8.0179\n",
      "Iteration 229/1072, Loss: 8.0141\n",
      "Iteration 230/1072, Loss: 7.9730\n",
      "Iteration 231/1072, Loss: 7.9236\n",
      "Iteration 232/1072, Loss: 7.9566\n",
      "Iteration 233/1072, Loss: 8.0091\n",
      "Iteration 234/1072, Loss: 8.0330\n",
      "Iteration 235/1072, Loss: 8.0022\n",
      "Iteration 236/1072, Loss: 7.9063\n",
      "Iteration 237/1072, Loss: 7.9667\n",
      "Iteration 238/1072, Loss: 8.0135\n",
      "Iteration 239/1072, Loss: 7.9755\n",
      "Iteration 240/1072, Loss: 7.9754\n",
      "Iteration 241/1072, Loss: 7.9994\n",
      "Iteration 242/1072, Loss: 7.9757\n",
      "Iteration 243/1072, Loss: 8.0201\n",
      "Iteration 244/1072, Loss: 8.0341\n",
      "Iteration 245/1072, Loss: 7.9713\n",
      "Iteration 246/1072, Loss: 7.9326\n",
      "Iteration 247/1072, Loss: 7.9823\n",
      "Iteration 248/1072, Loss: 7.9383\n",
      "Iteration 249/1072, Loss: 8.0379\n",
      "Iteration 250/1072, Loss: 7.9618\n",
      "Iteration 251/1072, Loss: 7.9668\n",
      "Iteration 252/1072, Loss: 7.9898\n",
      "Iteration 253/1072, Loss: 8.0151\n",
      "Iteration 254/1072, Loss: 7.9949\n",
      "Iteration 255/1072, Loss: 7.9868\n",
      "Iteration 256/1072, Loss: 7.9842\n",
      "Iteration 257/1072, Loss: 7.9641\n",
      "Iteration 258/1072, Loss: 7.9806\n",
      "Iteration 259/1072, Loss: 7.9603\n",
      "Iteration 260/1072, Loss: 8.0054\n",
      "Iteration 261/1072, Loss: 8.0121\n",
      "Iteration 262/1072, Loss: 7.9719\n",
      "Iteration 263/1072, Loss: 7.9810\n",
      "Iteration 264/1072, Loss: 7.9891\n",
      "Iteration 265/1072, Loss: 8.0088\n",
      "Iteration 266/1072, Loss: 7.9641\n",
      "Iteration 267/1072, Loss: 7.9942\n",
      "Iteration 268/1072, Loss: 8.0011\n",
      "Iteration 269/1072, Loss: 7.9764\n",
      "Iteration 270/1072, Loss: 7.9810\n",
      "Iteration 271/1072, Loss: 7.9925\n",
      "Iteration 272/1072, Loss: 7.9769\n",
      "Iteration 273/1072, Loss: 7.9724\n",
      "Iteration 274/1072, Loss: 7.9520\n",
      "Iteration 275/1072, Loss: 7.9911\n",
      "Iteration 276/1072, Loss: 8.0307\n",
      "Iteration 277/1072, Loss: 7.9956\n",
      "Iteration 278/1072, Loss: 7.9895\n",
      "Iteration 279/1072, Loss: 7.9440\n",
      "Iteration 280/1072, Loss: 7.9761\n",
      "Iteration 281/1072, Loss: 7.9538\n",
      "Iteration 282/1072, Loss: 7.9799\n",
      "Iteration 283/1072, Loss: 7.9704\n",
      "Iteration 284/1072, Loss: 8.0212\n",
      "Iteration 285/1072, Loss: 7.9896\n",
      "Iteration 286/1072, Loss: 7.9803\n",
      "Iteration 287/1072, Loss: 7.9647\n",
      "Iteration 288/1072, Loss: 7.9955\n",
      "Iteration 289/1072, Loss: 8.0090\n",
      "Iteration 290/1072, Loss: 7.9882\n",
      "Iteration 291/1072, Loss: 7.9419\n",
      "Iteration 292/1072, Loss: 8.0308\n",
      "Iteration 293/1072, Loss: 7.9453\n",
      "Iteration 294/1072, Loss: 8.0040\n",
      "Iteration 295/1072, Loss: 8.0773\n",
      "Iteration 296/1072, Loss: 7.9898\n",
      "Iteration 297/1072, Loss: 7.9902\n",
      "Iteration 298/1072, Loss: 7.9748\n",
      "Iteration 299/1072, Loss: 7.9447\n",
      "Iteration 300/1072, Loss: 8.0057\n",
      "Iteration 301/1072, Loss: 7.9835\n",
      "Iteration 302/1072, Loss: 7.9925\n",
      "Iteration 303/1072, Loss: 7.9684\n",
      "Iteration 304/1072, Loss: 7.9619\n",
      "Iteration 305/1072, Loss: 7.9442\n",
      "Iteration 306/1072, Loss: 7.9376\n",
      "Iteration 307/1072, Loss: 8.0345\n",
      "Iteration 308/1072, Loss: 7.9995\n",
      "Iteration 309/1072, Loss: 7.9473\n",
      "Iteration 310/1072, Loss: 7.9233\n",
      "Iteration 311/1072, Loss: 7.9921\n",
      "Iteration 312/1072, Loss: 7.9683\n",
      "Iteration 313/1072, Loss: 7.9839\n",
      "Iteration 314/1072, Loss: 7.9343\n",
      "Iteration 315/1072, Loss: 7.9560\n",
      "Iteration 316/1072, Loss: 8.0007\n",
      "Iteration 317/1072, Loss: 7.9670\n",
      "Iteration 318/1072, Loss: 7.9854\n",
      "Iteration 319/1072, Loss: 7.9335\n",
      "Iteration 320/1072, Loss: 7.9734\n",
      "Iteration 321/1072, Loss: 8.0328\n",
      "Iteration 322/1072, Loss: 8.0094\n",
      "Iteration 323/1072, Loss: 7.9137\n",
      "Iteration 324/1072, Loss: 7.9982\n",
      "Iteration 325/1072, Loss: 7.9216\n",
      "Iteration 326/1072, Loss: 7.9894\n",
      "Iteration 327/1072, Loss: 7.9842\n",
      "Iteration 328/1072, Loss: 7.9596\n",
      "Iteration 329/1072, Loss: 7.9726\n",
      "Iteration 330/1072, Loss: 7.9850\n",
      "Iteration 331/1072, Loss: 7.9377\n",
      "Iteration 332/1072, Loss: 8.0097\n",
      "Iteration 333/1072, Loss: 8.0247\n",
      "Iteration 334/1072, Loss: 8.0095\n",
      "Iteration 335/1072, Loss: 7.9883\n",
      "Iteration 336/1072, Loss: 7.9848\n",
      "Iteration 337/1072, Loss: 8.0012\n",
      "Iteration 338/1072, Loss: 7.9635\n",
      "Iteration 339/1072, Loss: 7.9427\n",
      "Iteration 340/1072, Loss: 7.9484\n",
      "Iteration 341/1072, Loss: 7.9659\n",
      "Iteration 342/1072, Loss: 7.9519\n",
      "Iteration 343/1072, Loss: 8.0130\n",
      "Iteration 344/1072, Loss: 7.9204\n",
      "Iteration 345/1072, Loss: 8.0012\n",
      "Iteration 346/1072, Loss: 7.9335\n",
      "Iteration 347/1072, Loss: 7.9968\n",
      "Iteration 348/1072, Loss: 7.9804\n",
      "Iteration 349/1072, Loss: 7.9837\n",
      "Iteration 350/1072, Loss: 7.9648\n",
      "Iteration 351/1072, Loss: 7.9897\n",
      "Iteration 352/1072, Loss: 7.9851\n",
      "Iteration 353/1072, Loss: 8.0215\n",
      "Iteration 354/1072, Loss: 7.9731\n",
      "Iteration 355/1072, Loss: 7.9943\n",
      "Iteration 356/1072, Loss: 7.9624\n",
      "Iteration 357/1072, Loss: 7.9842\n",
      "Iteration 358/1072, Loss: 7.9547\n",
      "Iteration 359/1072, Loss: 7.9471\n",
      "Iteration 360/1072, Loss: 7.9988\n",
      "Iteration 361/1072, Loss: 7.9252\n",
      "Iteration 362/1072, Loss: 7.9873\n",
      "Iteration 363/1072, Loss: 7.9702\n",
      "Iteration 364/1072, Loss: 7.9497\n",
      "Iteration 365/1072, Loss: 7.9120\n",
      "Iteration 366/1072, Loss: 7.9446\n",
      "Iteration 367/1072, Loss: 7.9703\n",
      "Iteration 368/1072, Loss: 8.0334\n",
      "Iteration 369/1072, Loss: 8.0557\n",
      "Iteration 370/1072, Loss: 8.0241\n",
      "Iteration 371/1072, Loss: 8.0200\n",
      "Iteration 372/1072, Loss: 7.9466\n",
      "Iteration 373/1072, Loss: 8.0054\n",
      "Iteration 374/1072, Loss: 8.0078\n",
      "Iteration 375/1072, Loss: 7.8923\n",
      "Iteration 376/1072, Loss: 7.9654\n",
      "Iteration 377/1072, Loss: 7.9393\n",
      "Iteration 378/1072, Loss: 7.9838\n",
      "Iteration 379/1072, Loss: 8.0026\n",
      "Iteration 380/1072, Loss: 7.9930\n",
      "Iteration 381/1072, Loss: 8.0052\n",
      "Iteration 382/1072, Loss: 7.9377\n",
      "Iteration 383/1072, Loss: 7.9522\n",
      "Iteration 384/1072, Loss: 8.0125\n",
      "Iteration 385/1072, Loss: 7.9517\n",
      "Iteration 386/1072, Loss: 7.9603\n",
      "Iteration 387/1072, Loss: 8.0327\n",
      "Iteration 388/1072, Loss: 7.9601\n",
      "Iteration 389/1072, Loss: 8.0334\n",
      "Iteration 390/1072, Loss: 8.0560\n",
      "Iteration 391/1072, Loss: 7.9192\n",
      "Iteration 392/1072, Loss: 7.9565\n",
      "Iteration 393/1072, Loss: 8.0234\n",
      "Iteration 394/1072, Loss: 7.9931\n",
      "Iteration 395/1072, Loss: 8.0221\n",
      "Iteration 396/1072, Loss: 8.0030\n",
      "Iteration 397/1072, Loss: 8.0235\n",
      "Iteration 398/1072, Loss: 7.9347\n",
      "Iteration 399/1072, Loss: 7.9968\n",
      "Iteration 400/1072, Loss: 7.9778\n",
      "Iteration 401/1072, Loss: 7.9926\n",
      "Iteration 402/1072, Loss: 7.9987\n",
      "Iteration 403/1072, Loss: 7.9758\n",
      "Iteration 404/1072, Loss: 7.9635\n",
      "Iteration 405/1072, Loss: 7.9167\n",
      "Iteration 406/1072, Loss: 7.9714\n",
      "Iteration 407/1072, Loss: 8.0139\n",
      "Iteration 408/1072, Loss: 7.9718\n",
      "Iteration 409/1072, Loss: 7.9304\n",
      "Iteration 410/1072, Loss: 8.0134\n",
      "Iteration 411/1072, Loss: 7.9790\n",
      "Iteration 412/1072, Loss: 7.9936\n",
      "Iteration 413/1072, Loss: 7.9636\n",
      "Iteration 414/1072, Loss: 8.0478\n",
      "Iteration 415/1072, Loss: 7.9967\n",
      "Iteration 416/1072, Loss: 7.9442\n",
      "Iteration 417/1072, Loss: 7.9462\n",
      "Iteration 418/1072, Loss: 7.9975\n",
      "Iteration 419/1072, Loss: 7.9954\n",
      "Iteration 420/1072, Loss: 7.9609\n",
      "Iteration 421/1072, Loss: 7.9766\n",
      "Iteration 422/1072, Loss: 7.9395\n",
      "Iteration 423/1072, Loss: 7.9875\n",
      "Iteration 424/1072, Loss: 7.9680\n",
      "Iteration 425/1072, Loss: 7.9951\n",
      "Iteration 426/1072, Loss: 7.9824\n",
      "Iteration 427/1072, Loss: 7.9321\n",
      "Iteration 428/1072, Loss: 8.0327\n",
      "Iteration 429/1072, Loss: 7.9277\n",
      "Iteration 430/1072, Loss: 8.0010\n",
      "Iteration 431/1072, Loss: 7.9288\n",
      "Iteration 432/1072, Loss: 7.9955\n",
      "Iteration 433/1072, Loss: 7.9952\n",
      "Iteration 434/1072, Loss: 7.9469\n",
      "Iteration 435/1072, Loss: 7.9667\n",
      "Iteration 436/1072, Loss: 8.0010\n",
      "Iteration 437/1072, Loss: 7.9907\n",
      "Iteration 438/1072, Loss: 7.9586\n",
      "Iteration 439/1072, Loss: 7.9929\n",
      "Iteration 440/1072, Loss: 7.9520\n",
      "Iteration 441/1072, Loss: 7.9668\n",
      "Iteration 442/1072, Loss: 7.9415\n",
      "Iteration 443/1072, Loss: 7.9517\n",
      "Iteration 444/1072, Loss: 7.9782\n",
      "Iteration 445/1072, Loss: 7.9886\n",
      "Iteration 446/1072, Loss: 7.9615\n",
      "Iteration 447/1072, Loss: 7.9423\n",
      "Iteration 448/1072, Loss: 7.9561\n",
      "Iteration 449/1072, Loss: 8.0324\n",
      "Iteration 450/1072, Loss: 7.9993\n",
      "Iteration 451/1072, Loss: 7.9429\n",
      "Iteration 452/1072, Loss: 7.9737\n",
      "Iteration 453/1072, Loss: 8.0054\n",
      "Iteration 454/1072, Loss: 7.9831\n",
      "Iteration 455/1072, Loss: 7.9688\n",
      "Iteration 456/1072, Loss: 7.9373\n",
      "Iteration 457/1072, Loss: 7.9601\n",
      "Iteration 458/1072, Loss: 7.9944\n",
      "Iteration 459/1072, Loss: 7.9506\n",
      "Iteration 460/1072, Loss: 7.9806\n",
      "Iteration 461/1072, Loss: 7.9687\n",
      "Iteration 462/1072, Loss: 8.0135\n",
      "Iteration 463/1072, Loss: 7.9678\n",
      "Iteration 464/1072, Loss: 8.0070\n",
      "Iteration 465/1072, Loss: 7.9746\n",
      "Iteration 466/1072, Loss: 8.0256\n",
      "Iteration 467/1072, Loss: 7.9661\n",
      "Iteration 468/1072, Loss: 7.9616\n",
      "Iteration 469/1072, Loss: 7.9494\n",
      "Iteration 470/1072, Loss: 7.9743\n",
      "Iteration 471/1072, Loss: 8.0183\n",
      "Iteration 472/1072, Loss: 7.9435\n",
      "Iteration 473/1072, Loss: 7.9409\n",
      "Iteration 474/1072, Loss: 7.9348\n",
      "Iteration 475/1072, Loss: 7.9780\n",
      "Iteration 476/1072, Loss: 7.9468\n",
      "Iteration 477/1072, Loss: 8.0456\n",
      "Iteration 478/1072, Loss: 7.9468\n",
      "Iteration 479/1072, Loss: 8.0344\n",
      "Iteration 480/1072, Loss: 7.9196\n",
      "Iteration 481/1072, Loss: 8.0034\n",
      "Iteration 482/1072, Loss: 7.9528\n",
      "Iteration 483/1072, Loss: 8.0356\n",
      "Iteration 484/1072, Loss: 7.9136\n",
      "Iteration 485/1072, Loss: 7.9742\n",
      "Iteration 486/1072, Loss: 8.0356\n",
      "Iteration 487/1072, Loss: 7.9637\n",
      "Iteration 488/1072, Loss: 7.9794\n",
      "Iteration 489/1072, Loss: 7.9091\n",
      "Iteration 490/1072, Loss: 7.9649\n",
      "Iteration 491/1072, Loss: 8.0044\n",
      "Iteration 492/1072, Loss: 7.9558\n",
      "Iteration 493/1072, Loss: 8.0102\n",
      "Iteration 494/1072, Loss: 7.9888\n",
      "Iteration 495/1072, Loss: 7.9564\n",
      "Iteration 496/1072, Loss: 7.9271\n",
      "Iteration 497/1072, Loss: 8.0144\n",
      "Iteration 498/1072, Loss: 7.9422\n",
      "Iteration 499/1072, Loss: 7.9741\n",
      "Iteration 500/1072, Loss: 7.9601\n",
      "Iteration 501/1072, Loss: 7.9466\n",
      "Iteration 502/1072, Loss: 8.0286\n",
      "Iteration 503/1072, Loss: 7.9300\n",
      "Iteration 504/1072, Loss: 7.9422\n",
      "Iteration 505/1072, Loss: 7.9721\n",
      "Iteration 506/1072, Loss: 8.0255\n",
      "Iteration 507/1072, Loss: 7.9239\n",
      "Iteration 508/1072, Loss: 7.9000\n",
      "Iteration 509/1072, Loss: 7.9857\n",
      "Iteration 510/1072, Loss: 7.9679\n",
      "Iteration 511/1072, Loss: 7.9550\n",
      "Iteration 512/1072, Loss: 7.9927\n",
      "Iteration 513/1072, Loss: 7.9230\n",
      "Iteration 514/1072, Loss: 7.9605\n",
      "Iteration 515/1072, Loss: 7.9758\n",
      "Iteration 516/1072, Loss: 8.0118\n",
      "Iteration 517/1072, Loss: 7.9306\n",
      "Iteration 518/1072, Loss: 7.9533\n",
      "Iteration 519/1072, Loss: 8.0517\n",
      "Iteration 520/1072, Loss: 7.9927\n",
      "Iteration 521/1072, Loss: 7.9445\n",
      "Iteration 522/1072, Loss: 7.9945\n",
      "Iteration 523/1072, Loss: 8.0126\n",
      "Iteration 524/1072, Loss: 7.9597\n",
      "Iteration 525/1072, Loss: 7.9991\n",
      "Iteration 526/1072, Loss: 7.9951\n",
      "Iteration 527/1072, Loss: 7.9519\n",
      "Iteration 528/1072, Loss: 7.9634\n",
      "Iteration 529/1072, Loss: 7.9849\n",
      "Iteration 530/1072, Loss: 7.9437\n",
      "Iteration 531/1072, Loss: 7.9816\n",
      "Iteration 532/1072, Loss: 7.9749\n",
      "Iteration 533/1072, Loss: 7.9635\n",
      "Iteration 534/1072, Loss: 7.9570\n",
      "Iteration 535/1072, Loss: 8.0223\n",
      "Iteration 536/1072, Loss: 8.0167\n",
      "Iteration 537/1072, Loss: 7.9818\n",
      "Iteration 538/1072, Loss: 7.9924\n",
      "Iteration 539/1072, Loss: 7.9791\n",
      "Iteration 540/1072, Loss: 7.9591\n",
      "Iteration 541/1072, Loss: 7.9785\n",
      "Iteration 542/1072, Loss: 7.9764\n",
      "Iteration 543/1072, Loss: 7.9701\n",
      "Iteration 544/1072, Loss: 7.9656\n",
      "Iteration 545/1072, Loss: 7.9423\n",
      "Iteration 546/1072, Loss: 7.9199\n",
      "Iteration 547/1072, Loss: 7.9452\n",
      "Iteration 548/1072, Loss: 7.9706\n",
      "Iteration 549/1072, Loss: 7.9480\n",
      "Iteration 550/1072, Loss: 7.9905\n",
      "Iteration 551/1072, Loss: 7.9638\n",
      "Iteration 552/1072, Loss: 7.9965\n",
      "Iteration 553/1072, Loss: 8.0115\n",
      "Iteration 554/1072, Loss: 7.9626\n",
      "Iteration 555/1072, Loss: 7.9549\n",
      "Iteration 556/1072, Loss: 7.9850\n",
      "Iteration 557/1072, Loss: 8.0387\n",
      "Iteration 558/1072, Loss: 7.9646\n",
      "Iteration 559/1072, Loss: 7.9488\n",
      "Iteration 560/1072, Loss: 8.0372\n",
      "Iteration 561/1072, Loss: 7.9783\n",
      "Iteration 562/1072, Loss: 7.9629\n",
      "Iteration 563/1072, Loss: 8.0258\n",
      "Iteration 564/1072, Loss: 8.0096\n",
      "Iteration 565/1072, Loss: 7.9732\n",
      "Iteration 566/1072, Loss: 7.9394\n",
      "Iteration 567/1072, Loss: 7.9787\n",
      "Iteration 568/1072, Loss: 7.9581\n",
      "Iteration 569/1072, Loss: 7.9887\n",
      "Iteration 570/1072, Loss: 8.0210\n",
      "Iteration 571/1072, Loss: 7.9817\n",
      "Iteration 572/1072, Loss: 7.9743\n",
      "Iteration 573/1072, Loss: 7.9832\n",
      "Iteration 574/1072, Loss: 7.9994\n",
      "Iteration 575/1072, Loss: 7.9309\n",
      "Iteration 576/1072, Loss: 7.9788\n",
      "Iteration 577/1072, Loss: 7.9520\n",
      "Iteration 578/1072, Loss: 7.9956\n",
      "Iteration 579/1072, Loss: 7.9760\n",
      "Iteration 580/1072, Loss: 7.8691\n",
      "Iteration 581/1072, Loss: 7.9784\n",
      "Iteration 582/1072, Loss: 7.9573\n",
      "Iteration 583/1072, Loss: 7.9830\n",
      "Iteration 584/1072, Loss: 7.9584\n",
      "Iteration 585/1072, Loss: 7.9978\n",
      "Iteration 586/1072, Loss: 7.9835\n",
      "Iteration 587/1072, Loss: 8.0084\n",
      "Iteration 588/1072, Loss: 7.9816\n",
      "Iteration 589/1072, Loss: 7.9971\n",
      "Iteration 590/1072, Loss: 7.9622\n",
      "Iteration 591/1072, Loss: 7.9632\n",
      "Iteration 592/1072, Loss: 8.0379\n",
      "Iteration 593/1072, Loss: 8.0131\n",
      "Iteration 594/1072, Loss: 7.9653\n",
      "Iteration 595/1072, Loss: 7.9444\n",
      "Iteration 596/1072, Loss: 8.0021\n",
      "Iteration 597/1072, Loss: 7.9809\n",
      "Iteration 598/1072, Loss: 7.9550\n",
      "Iteration 599/1072, Loss: 7.9662\n",
      "Iteration 600/1072, Loss: 7.9949\n",
      "Iteration 601/1072, Loss: 7.9988\n",
      "Iteration 602/1072, Loss: 7.9828\n",
      "Iteration 603/1072, Loss: 7.9385\n",
      "Iteration 604/1072, Loss: 7.9728\n",
      "Iteration 605/1072, Loss: 7.9968\n",
      "Iteration 606/1072, Loss: 7.9434\n",
      "Iteration 607/1072, Loss: 7.9455\n",
      "Iteration 608/1072, Loss: 7.9406\n",
      "Iteration 609/1072, Loss: 7.9543\n",
      "Iteration 610/1072, Loss: 8.0016\n",
      "Iteration 611/1072, Loss: 7.9377\n",
      "Iteration 612/1072, Loss: 7.9808\n",
      "Iteration 613/1072, Loss: 7.9567\n",
      "Iteration 614/1072, Loss: 7.9369\n",
      "Iteration 615/1072, Loss: 7.9988\n",
      "Iteration 616/1072, Loss: 7.9349\n",
      "Iteration 617/1072, Loss: 7.9976\n",
      "Iteration 618/1072, Loss: 8.0104\n",
      "Iteration 619/1072, Loss: 8.0068\n",
      "Iteration 620/1072, Loss: 7.9850\n",
      "Iteration 621/1072, Loss: 7.9472\n",
      "Iteration 622/1072, Loss: 7.9577\n",
      "Iteration 623/1072, Loss: 7.9780\n",
      "Iteration 624/1072, Loss: 7.9860\n",
      "Iteration 625/1072, Loss: 7.9086\n",
      "Iteration 626/1072, Loss: 7.9537\n",
      "Iteration 627/1072, Loss: 7.9500\n",
      "Iteration 628/1072, Loss: 7.9850\n",
      "Iteration 629/1072, Loss: 7.9828\n",
      "Iteration 630/1072, Loss: 7.9659\n",
      "Iteration 631/1072, Loss: 7.9855\n",
      "Iteration 632/1072, Loss: 8.0006\n",
      "Iteration 633/1072, Loss: 7.9505\n",
      "Iteration 634/1072, Loss: 7.9627\n",
      "Iteration 635/1072, Loss: 7.9513\n",
      "Iteration 636/1072, Loss: 8.0181\n",
      "Iteration 637/1072, Loss: 7.9879\n",
      "Iteration 638/1072, Loss: 8.0010\n",
      "Iteration 639/1072, Loss: 7.9441\n",
      "Iteration 640/1072, Loss: 7.9808\n",
      "Iteration 641/1072, Loss: 7.9575\n",
      "Iteration 642/1072, Loss: 7.9806\n",
      "Iteration 643/1072, Loss: 7.9704\n",
      "Iteration 644/1072, Loss: 7.9713\n",
      "Iteration 645/1072, Loss: 8.0013\n",
      "Iteration 646/1072, Loss: 7.9342\n",
      "Iteration 647/1072, Loss: 7.9778\n",
      "Iteration 648/1072, Loss: 7.9658\n",
      "Iteration 649/1072, Loss: 7.9463\n",
      "Iteration 650/1072, Loss: 7.9675\n",
      "Iteration 651/1072, Loss: 7.9780\n",
      "Iteration 652/1072, Loss: 7.9369\n",
      "Iteration 653/1072, Loss: 7.8972\n",
      "Iteration 654/1072, Loss: 7.9915\n",
      "Iteration 655/1072, Loss: 7.9546\n",
      "Iteration 656/1072, Loss: 7.9459\n",
      "Iteration 657/1072, Loss: 7.9406\n",
      "Iteration 658/1072, Loss: 8.0132\n",
      "Iteration 659/1072, Loss: 7.9921\n",
      "Iteration 660/1072, Loss: 7.9820\n",
      "Iteration 661/1072, Loss: 7.9295\n",
      "Iteration 662/1072, Loss: 7.9534\n",
      "Iteration 663/1072, Loss: 7.9528\n",
      "Iteration 664/1072, Loss: 7.9871\n",
      "Iteration 665/1072, Loss: 8.0048\n",
      "Iteration 666/1072, Loss: 7.9770\n",
      "Iteration 667/1072, Loss: 7.9304\n",
      "Iteration 668/1072, Loss: 7.9566\n",
      "Iteration 669/1072, Loss: 7.9356\n",
      "Iteration 670/1072, Loss: 7.9447\n",
      "Iteration 671/1072, Loss: 7.9439\n",
      "Iteration 672/1072, Loss: 7.9784\n",
      "Iteration 673/1072, Loss: 7.9611\n",
      "Iteration 674/1072, Loss: 7.9934\n",
      "Iteration 675/1072, Loss: 7.9773\n",
      "Iteration 676/1072, Loss: 7.9469\n",
      "Iteration 677/1072, Loss: 7.9522\n",
      "Iteration 678/1072, Loss: 7.9122\n",
      "Iteration 679/1072, Loss: 7.9766\n",
      "Iteration 680/1072, Loss: 7.9676\n",
      "Iteration 681/1072, Loss: 7.9979\n",
      "Iteration 682/1072, Loss: 7.9542\n",
      "Iteration 683/1072, Loss: 8.0177\n",
      "Iteration 684/1072, Loss: 8.0171\n",
      "Iteration 685/1072, Loss: 7.9894\n",
      "Iteration 686/1072, Loss: 7.9513\n",
      "Iteration 687/1072, Loss: 7.9882\n",
      "Iteration 688/1072, Loss: 7.9758\n",
      "Iteration 689/1072, Loss: 8.0384\n",
      "Iteration 690/1072, Loss: 7.9753\n",
      "Iteration 691/1072, Loss: 7.9888\n",
      "Iteration 692/1072, Loss: 7.9847\n",
      "Iteration 693/1072, Loss: 7.9176\n",
      "Iteration 694/1072, Loss: 7.9976\n",
      "Iteration 695/1072, Loss: 7.9849\n",
      "Iteration 696/1072, Loss: 8.0126\n",
      "Iteration 697/1072, Loss: 8.0036\n",
      "Iteration 698/1072, Loss: 7.9368\n",
      "Iteration 699/1072, Loss: 8.0175\n",
      "Iteration 700/1072, Loss: 8.0477\n",
      "Iteration 701/1072, Loss: 7.9549\n",
      "Iteration 702/1072, Loss: 7.9301\n",
      "Iteration 703/1072, Loss: 7.9773\n",
      "Iteration 704/1072, Loss: 7.9409\n",
      "Iteration 705/1072, Loss: 7.9941\n",
      "Iteration 706/1072, Loss: 7.9230\n",
      "Iteration 707/1072, Loss: 7.9365\n",
      "Iteration 708/1072, Loss: 7.9955\n",
      "Iteration 709/1072, Loss: 7.9776\n",
      "Iteration 710/1072, Loss: 7.9434\n",
      "Iteration 711/1072, Loss: 7.9211\n",
      "Iteration 712/1072, Loss: 7.9551\n",
      "Iteration 713/1072, Loss: 7.9908\n",
      "Iteration 714/1072, Loss: 7.9621\n",
      "Iteration 715/1072, Loss: 7.9887\n",
      "Iteration 716/1072, Loss: 7.9199\n",
      "Iteration 717/1072, Loss: 8.0331\n",
      "Iteration 718/1072, Loss: 7.9493\n",
      "Iteration 719/1072, Loss: 8.0327\n",
      "Iteration 720/1072, Loss: 7.9653\n",
      "Iteration 721/1072, Loss: 8.0046\n",
      "Iteration 722/1072, Loss: 7.9578\n",
      "Iteration 723/1072, Loss: 7.9805\n",
      "Iteration 724/1072, Loss: 7.9502\n",
      "Iteration 725/1072, Loss: 7.9992\n",
      "Iteration 726/1072, Loss: 7.9855\n",
      "Iteration 727/1072, Loss: 7.9298\n",
      "Iteration 728/1072, Loss: 7.9752\n",
      "Iteration 729/1072, Loss: 7.9466\n",
      "Iteration 730/1072, Loss: 7.9125\n",
      "Iteration 731/1072, Loss: 7.9361\n",
      "Iteration 732/1072, Loss: 7.9595\n",
      "Iteration 733/1072, Loss: 7.9451\n",
      "Iteration 734/1072, Loss: 7.9183\n",
      "Iteration 735/1072, Loss: 7.9540\n",
      "Iteration 736/1072, Loss: 7.9690\n",
      "Iteration 737/1072, Loss: 7.9409\n",
      "Iteration 738/1072, Loss: 7.9872\n",
      "Iteration 739/1072, Loss: 8.0016\n",
      "Iteration 740/1072, Loss: 7.9065\n",
      "Iteration 741/1072, Loss: 8.0027\n",
      "Iteration 742/1072, Loss: 7.9527\n",
      "Iteration 743/1072, Loss: 7.9260\n",
      "Iteration 744/1072, Loss: 7.9326\n",
      "Iteration 745/1072, Loss: 7.9131\n",
      "Iteration 746/1072, Loss: 7.9791\n",
      "Iteration 747/1072, Loss: 8.0002\n",
      "Iteration 748/1072, Loss: 7.9342\n",
      "Iteration 749/1072, Loss: 7.8991\n",
      "Iteration 750/1072, Loss: 8.0052\n",
      "Iteration 751/1072, Loss: 8.0291\n",
      "Iteration 752/1072, Loss: 7.9631\n",
      "Iteration 753/1072, Loss: 8.0018\n",
      "Iteration 754/1072, Loss: 7.9107\n",
      "Iteration 755/1072, Loss: 7.9653\n",
      "Iteration 756/1072, Loss: 7.9026\n",
      "Iteration 757/1072, Loss: 8.0356\n",
      "Iteration 758/1072, Loss: 7.8835\n",
      "Iteration 759/1072, Loss: 7.9339\n",
      "Iteration 760/1072, Loss: 7.9328\n",
      "Iteration 761/1072, Loss: 8.0045\n",
      "Iteration 762/1072, Loss: 7.9657\n",
      "Iteration 763/1072, Loss: 7.9698\n",
      "Iteration 764/1072, Loss: 7.9045\n",
      "Iteration 765/1072, Loss: 7.9858\n",
      "Iteration 766/1072, Loss: 7.9525\n",
      "Iteration 767/1072, Loss: 7.9813\n",
      "Iteration 768/1072, Loss: 7.9648\n",
      "Iteration 769/1072, Loss: 7.9433\n",
      "Iteration 770/1072, Loss: 7.9618\n",
      "Iteration 771/1072, Loss: 7.9532\n",
      "Iteration 772/1072, Loss: 7.9172\n",
      "Iteration 773/1072, Loss: 7.9389\n",
      "Iteration 774/1072, Loss: 7.8953\n",
      "Iteration 775/1072, Loss: 7.9358\n",
      "Iteration 776/1072, Loss: 7.9601\n",
      "Iteration 777/1072, Loss: 8.0080\n",
      "Iteration 778/1072, Loss: 7.9976\n",
      "Iteration 779/1072, Loss: 7.9536\n",
      "Iteration 780/1072, Loss: 7.9755\n",
      "Iteration 781/1072, Loss: 7.9355\n",
      "Iteration 782/1072, Loss: 7.9831\n",
      "Iteration 783/1072, Loss: 7.9743\n",
      "Iteration 784/1072, Loss: 7.9316\n",
      "Iteration 785/1072, Loss: 7.9138\n",
      "Iteration 786/1072, Loss: 7.9549\n",
      "Iteration 787/1072, Loss: 7.9653\n",
      "Iteration 788/1072, Loss: 7.9205\n",
      "Iteration 789/1072, Loss: 7.9379\n",
      "Iteration 790/1072, Loss: 7.9441\n",
      "Iteration 791/1072, Loss: 7.9658\n",
      "Iteration 792/1072, Loss: 7.9345\n",
      "Iteration 793/1072, Loss: 7.9796\n",
      "Iteration 794/1072, Loss: 7.9890\n",
      "Iteration 795/1072, Loss: 7.9802\n",
      "Iteration 796/1072, Loss: 7.9444\n",
      "Iteration 797/1072, Loss: 7.9860\n",
      "Iteration 798/1072, Loss: 7.9472\n",
      "Iteration 799/1072, Loss: 7.9754\n",
      "Iteration 800/1072, Loss: 8.0282\n",
      "Iteration 801/1072, Loss: 7.9850\n",
      "Iteration 802/1072, Loss: 7.9844\n",
      "Iteration 803/1072, Loss: 8.0178\n",
      "Iteration 804/1072, Loss: 7.9722\n",
      "Iteration 805/1072, Loss: 7.9403\n",
      "Iteration 806/1072, Loss: 8.0199\n",
      "Iteration 807/1072, Loss: 7.9403\n",
      "Iteration 808/1072, Loss: 7.9196\n",
      "Iteration 809/1072, Loss: 7.9197\n",
      "Iteration 810/1072, Loss: 7.9561\n",
      "Iteration 811/1072, Loss: 7.9639\n",
      "Iteration 812/1072, Loss: 7.8868\n",
      "Iteration 813/1072, Loss: 7.9529\n",
      "Iteration 814/1072, Loss: 7.9540\n",
      "Iteration 815/1072, Loss: 7.9038\n",
      "Iteration 816/1072, Loss: 7.9924\n",
      "Iteration 817/1072, Loss: 7.9491\n",
      "Iteration 818/1072, Loss: 8.0038\n",
      "Iteration 819/1072, Loss: 8.0163\n",
      "Iteration 820/1072, Loss: 7.9089\n",
      "Iteration 821/1072, Loss: 7.9336\n",
      "Iteration 822/1072, Loss: 7.9585\n",
      "Iteration 823/1072, Loss: 8.0665\n",
      "Iteration 824/1072, Loss: 7.9907\n",
      "Iteration 825/1072, Loss: 7.8908\n",
      "Iteration 826/1072, Loss: 7.9512\n",
      "Iteration 827/1072, Loss: 7.9431\n",
      "Iteration 828/1072, Loss: 7.9419\n",
      "Iteration 829/1072, Loss: 7.9628\n",
      "Iteration 830/1072, Loss: 7.9602\n",
      "Iteration 831/1072, Loss: 7.9010\n",
      "Iteration 832/1072, Loss: 7.9712\n",
      "Iteration 833/1072, Loss: 7.9452\n",
      "Iteration 834/1072, Loss: 7.9780\n",
      "Iteration 835/1072, Loss: 7.9868\n",
      "Iteration 836/1072, Loss: 7.9282\n",
      "Iteration 837/1072, Loss: 7.9052\n",
      "Iteration 838/1072, Loss: 7.9737\n",
      "Iteration 839/1072, Loss: 7.9805\n",
      "Iteration 840/1072, Loss: 7.9602\n",
      "Iteration 841/1072, Loss: 7.9796\n",
      "Iteration 842/1072, Loss: 7.9202\n",
      "Iteration 843/1072, Loss: 7.9553\n",
      "Iteration 844/1072, Loss: 7.9691\n",
      "Iteration 845/1072, Loss: 7.9787\n",
      "Iteration 846/1072, Loss: 7.9537\n",
      "Iteration 847/1072, Loss: 7.9620\n",
      "Iteration 848/1072, Loss: 7.9787\n",
      "Iteration 849/1072, Loss: 7.9345\n",
      "Iteration 850/1072, Loss: 7.9265\n",
      "Iteration 851/1072, Loss: 7.9579\n",
      "Iteration 852/1072, Loss: 7.9319\n",
      "Iteration 853/1072, Loss: 7.9600\n",
      "Iteration 854/1072, Loss: 7.9672\n",
      "Iteration 855/1072, Loss: 8.0263\n",
      "Iteration 856/1072, Loss: 7.9768\n",
      "Iteration 857/1072, Loss: 8.0242\n",
      "Iteration 858/1072, Loss: 7.9994\n",
      "Iteration 859/1072, Loss: 7.9378\n",
      "Iteration 860/1072, Loss: 7.9484\n",
      "Iteration 861/1072, Loss: 7.9540\n",
      "Iteration 862/1072, Loss: 8.0134\n",
      "Iteration 863/1072, Loss: 8.0666\n",
      "Iteration 864/1072, Loss: 7.9266\n",
      "Iteration 865/1072, Loss: 7.9461\n",
      "Iteration 866/1072, Loss: 7.9413\n",
      "Iteration 867/1072, Loss: 7.9597\n",
      "Iteration 868/1072, Loss: 7.9770\n",
      "Iteration 869/1072, Loss: 7.9869\n",
      "Iteration 870/1072, Loss: 7.9548\n",
      "Iteration 871/1072, Loss: 7.9561\n",
      "Iteration 872/1072, Loss: 7.9921\n",
      "Iteration 873/1072, Loss: 8.0193\n",
      "Iteration 874/1072, Loss: 7.9088\n",
      "Iteration 875/1072, Loss: 7.9776\n",
      "Iteration 876/1072, Loss: 7.9646\n",
      "Iteration 877/1072, Loss: 8.0279\n",
      "Iteration 878/1072, Loss: 7.9587\n",
      "Iteration 879/1072, Loss: 8.0326\n",
      "Iteration 880/1072, Loss: 7.9695\n",
      "Iteration 881/1072, Loss: 7.9736\n",
      "Iteration 882/1072, Loss: 7.9727\n",
      "Iteration 883/1072, Loss: 8.0147\n",
      "Iteration 884/1072, Loss: 7.9492\n",
      "Iteration 885/1072, Loss: 7.8931\n",
      "Iteration 886/1072, Loss: 8.0100\n",
      "Iteration 887/1072, Loss: 7.9884\n",
      "Iteration 888/1072, Loss: 7.9808\n",
      "Iteration 889/1072, Loss: 7.9504\n",
      "Iteration 890/1072, Loss: 7.9995\n",
      "Iteration 891/1072, Loss: 7.9730\n",
      "Iteration 892/1072, Loss: 8.0004\n",
      "Iteration 893/1072, Loss: 7.9739\n",
      "Iteration 894/1072, Loss: 7.9745\n",
      "Iteration 895/1072, Loss: 7.9361\n",
      "Iteration 896/1072, Loss: 7.9345\n",
      "Iteration 897/1072, Loss: 7.9399\n",
      "Iteration 898/1072, Loss: 7.9886\n",
      "Iteration 899/1072, Loss: 7.9205\n",
      "Iteration 900/1072, Loss: 7.9816\n",
      "Iteration 901/1072, Loss: 8.0375\n",
      "Iteration 902/1072, Loss: 7.9642\n",
      "Iteration 903/1072, Loss: 7.9789\n",
      "Iteration 904/1072, Loss: 7.9423\n",
      "Iteration 905/1072, Loss: 7.9876\n",
      "Iteration 906/1072, Loss: 8.0193\n",
      "Iteration 907/1072, Loss: 7.9744\n",
      "Iteration 908/1072, Loss: 7.9779\n",
      "Iteration 909/1072, Loss: 7.9871\n",
      "Iteration 910/1072, Loss: 7.9826\n",
      "Iteration 911/1072, Loss: 7.9727\n",
      "Iteration 912/1072, Loss: 7.9450\n",
      "Iteration 913/1072, Loss: 7.9796\n",
      "Iteration 914/1072, Loss: 7.9800\n",
      "Iteration 915/1072, Loss: 8.0026\n",
      "Iteration 916/1072, Loss: 7.9100\n",
      "Iteration 917/1072, Loss: 7.9549\n",
      "Iteration 918/1072, Loss: 7.8861\n",
      "Iteration 919/1072, Loss: 7.9881\n",
      "Iteration 920/1072, Loss: 7.9869\n",
      "Iteration 921/1072, Loss: 7.9855\n",
      "Iteration 922/1072, Loss: 7.9213\n",
      "Iteration 923/1072, Loss: 7.9544\n",
      "Iteration 924/1072, Loss: 7.9591\n",
      "Iteration 925/1072, Loss: 7.9965\n",
      "Iteration 926/1072, Loss: 7.9319\n",
      "Iteration 927/1072, Loss: 7.9618\n",
      "Iteration 928/1072, Loss: 7.9659\n",
      "Iteration 929/1072, Loss: 7.9786\n",
      "Iteration 930/1072, Loss: 7.9812\n",
      "Iteration 931/1072, Loss: 7.9252\n",
      "Iteration 932/1072, Loss: 7.9768\n",
      "Iteration 933/1072, Loss: 7.9971\n",
      "Iteration 934/1072, Loss: 7.9689\n",
      "Iteration 935/1072, Loss: 7.9687\n",
      "Iteration 936/1072, Loss: 7.9971\n",
      "Iteration 937/1072, Loss: 7.8862\n",
      "Iteration 938/1072, Loss: 7.9230\n",
      "Iteration 939/1072, Loss: 7.9781\n",
      "Iteration 940/1072, Loss: 7.9488\n",
      "Iteration 941/1072, Loss: 7.9341\n",
      "Iteration 942/1072, Loss: 7.9811\n",
      "Iteration 943/1072, Loss: 8.0512\n",
      "Iteration 944/1072, Loss: 7.9626\n",
      "Iteration 945/1072, Loss: 7.9927\n",
      "Iteration 946/1072, Loss: 7.9873\n",
      "Iteration 947/1072, Loss: 7.9467\n",
      "Iteration 948/1072, Loss: 7.9390\n",
      "Iteration 949/1072, Loss: 7.9542\n",
      "Iteration 950/1072, Loss: 7.9400\n",
      "Iteration 951/1072, Loss: 7.9741\n",
      "Iteration 952/1072, Loss: 8.0233\n",
      "Iteration 953/1072, Loss: 8.0209\n",
      "Iteration 954/1072, Loss: 7.9980\n",
      "Iteration 955/1072, Loss: 7.9268\n",
      "Iteration 956/1072, Loss: 7.9324\n",
      "Iteration 957/1072, Loss: 7.9487\n",
      "Iteration 958/1072, Loss: 7.9411\n",
      "Iteration 959/1072, Loss: 7.9360\n",
      "Iteration 960/1072, Loss: 7.9853\n",
      "Iteration 961/1072, Loss: 7.9782\n",
      "Iteration 962/1072, Loss: 8.0110\n",
      "Iteration 963/1072, Loss: 7.9370\n",
      "Iteration 964/1072, Loss: 8.0232\n",
      "Iteration 965/1072, Loss: 7.9405\n",
      "Iteration 966/1072, Loss: 7.9868\n",
      "Iteration 967/1072, Loss: 7.9954\n",
      "Iteration 968/1072, Loss: 7.9725\n",
      "Iteration 969/1072, Loss: 7.9880\n",
      "Iteration 970/1072, Loss: 8.0108\n",
      "Iteration 971/1072, Loss: 7.9932\n",
      "Iteration 972/1072, Loss: 7.9611\n",
      "Iteration 973/1072, Loss: 7.9689\n",
      "Iteration 974/1072, Loss: 7.9689\n",
      "Iteration 975/1072, Loss: 7.9430\n",
      "Iteration 976/1072, Loss: 7.9534\n",
      "Iteration 977/1072, Loss: 7.9914\n",
      "Iteration 978/1072, Loss: 7.9506\n",
      "Iteration 979/1072, Loss: 7.9912\n",
      "Iteration 980/1072, Loss: 7.9708\n",
      "Iteration 981/1072, Loss: 7.9513\n",
      "Iteration 982/1072, Loss: 7.9249\n",
      "Iteration 983/1072, Loss: 7.9304\n",
      "Iteration 984/1072, Loss: 8.0014\n",
      "Iteration 985/1072, Loss: 7.9446\n",
      "Iteration 986/1072, Loss: 7.9481\n",
      "Iteration 987/1072, Loss: 7.9659\n",
      "Iteration 988/1072, Loss: 7.9836\n",
      "Iteration 989/1072, Loss: 7.9850\n",
      "Iteration 990/1072, Loss: 7.9615\n",
      "Iteration 991/1072, Loss: 7.9433\n",
      "Iteration 992/1072, Loss: 7.9068\n",
      "Iteration 993/1072, Loss: 7.9244\n",
      "Iteration 994/1072, Loss: 7.9549\n",
      "Iteration 995/1072, Loss: 7.9384\n",
      "Iteration 996/1072, Loss: 7.9637\n",
      "Iteration 997/1072, Loss: 7.9553\n",
      "Iteration 998/1072, Loss: 7.9450\n",
      "Iteration 999/1072, Loss: 7.9415\n",
      "Iteration 1000/1072, Loss: 7.9743\n",
      "Iteration 1001/1072, Loss: 7.9005\n",
      "Iteration 1002/1072, Loss: 7.9889\n",
      "Iteration 1003/1072, Loss: 7.9899\n",
      "Iteration 1004/1072, Loss: 7.9530\n",
      "Iteration 1005/1072, Loss: 7.9516\n",
      "Iteration 1006/1072, Loss: 7.9261\n",
      "Iteration 1007/1072, Loss: 7.9572\n",
      "Iteration 1008/1072, Loss: 8.0114\n",
      "Iteration 1009/1072, Loss: 7.9165\n",
      "Iteration 1010/1072, Loss: 7.9912\n",
      "Iteration 1011/1072, Loss: 7.9512\n",
      "Iteration 1012/1072, Loss: 7.9250\n",
      "Iteration 1013/1072, Loss: 7.9140\n",
      "Iteration 1014/1072, Loss: 7.9313\n",
      "Iteration 1015/1072, Loss: 8.0648\n",
      "Iteration 1016/1072, Loss: 7.9968\n",
      "Iteration 1017/1072, Loss: 7.9708\n",
      "Iteration 1018/1072, Loss: 7.9485\n",
      "Iteration 1019/1072, Loss: 7.9481\n",
      "Iteration 1020/1072, Loss: 7.9673\n",
      "Iteration 1021/1072, Loss: 7.9544\n",
      "Iteration 1022/1072, Loss: 7.9502\n",
      "Iteration 1023/1072, Loss: 7.9802\n",
      "Iteration 1024/1072, Loss: 7.9538\n",
      "Iteration 1025/1072, Loss: 8.0233\n",
      "Iteration 1026/1072, Loss: 7.9203\n",
      "Iteration 1027/1072, Loss: 7.9646\n",
      "Iteration 1028/1072, Loss: 7.9956\n",
      "Iteration 1029/1072, Loss: 7.9062\n",
      "Iteration 1030/1072, Loss: 7.9573\n",
      "Iteration 1031/1072, Loss: 7.9670\n",
      "Iteration 1032/1072, Loss: 7.8938\n",
      "Iteration 1033/1072, Loss: 7.9571\n",
      "Iteration 1034/1072, Loss: 8.0057\n",
      "Iteration 1035/1072, Loss: 7.9670\n",
      "Iteration 1036/1072, Loss: 7.9638\n",
      "Iteration 1037/1072, Loss: 7.9764\n",
      "Iteration 1038/1072, Loss: 7.9542\n",
      "Iteration 1039/1072, Loss: 7.9651\n",
      "Iteration 1040/1072, Loss: 7.9038\n",
      "Iteration 1041/1072, Loss: 8.0189\n",
      "Iteration 1042/1072, Loss: 7.9463\n",
      "Iteration 1043/1072, Loss: 7.9970\n",
      "Iteration 1044/1072, Loss: 7.9205\n",
      "Iteration 1045/1072, Loss: 8.0506\n",
      "Iteration 1046/1072, Loss: 7.9969\n",
      "Iteration 1047/1072, Loss: 7.9651\n",
      "Iteration 1048/1072, Loss: 7.9002\n",
      "Iteration 1049/1072, Loss: 7.9275\n",
      "Iteration 1050/1072, Loss: 7.9277\n",
      "Iteration 1051/1072, Loss: 7.9262\n",
      "Iteration 1052/1072, Loss: 7.9629\n",
      "Iteration 1053/1072, Loss: 7.9720\n",
      "Iteration 1054/1072, Loss: 7.9508\n",
      "Iteration 1055/1072, Loss: 7.9493\n",
      "Iteration 1056/1072, Loss: 7.9851\n",
      "Iteration 1057/1072, Loss: 7.9261\n",
      "Iteration 1058/1072, Loss: 8.0141\n",
      "Iteration 1059/1072, Loss: 8.0087\n",
      "Iteration 1060/1072, Loss: 7.9039\n",
      "Iteration 1061/1072, Loss: 7.8994\n",
      "Iteration 1062/1072, Loss: 7.9565\n",
      "Iteration 1063/1072, Loss: 7.9116\n",
      "Iteration 1064/1072, Loss: 7.9468\n",
      "Iteration 1065/1072, Loss: 7.9594\n",
      "Iteration 1066/1072, Loss: 7.8919\n",
      "Iteration 1067/1072, Loss: 8.0044\n",
      "Iteration 1068/1072, Loss: 7.9399\n",
      "Iteration 1069/1072, Loss: 8.0026\n",
      "Iteration 1070/1072, Loss: 7.9512\n",
      "Iteration 1071/1072, Loss: 7.9668\n",
      "Iteration 1072/1072, Loss: 7.9587\n",
      "Epoch 1/10, Loss: 7.9726\n",
      "Checkpoint saved at checkpoints/efficientnet-b0_epoch_1.pth\n",
      "Validation Accuracy: 0.34%\n",
      "Iteration 1/1072, Loss: 7.8959\n",
      "Iteration 2/1072, Loss: 7.9839\n",
      "Iteration 3/1072, Loss: 7.9899\n",
      "Iteration 4/1072, Loss: 7.9277\n",
      "Iteration 5/1072, Loss: 7.8712\n",
      "Iteration 6/1072, Loss: 7.9261\n",
      "Iteration 7/1072, Loss: 7.9266\n",
      "Iteration 8/1072, Loss: 7.9684\n",
      "Iteration 9/1072, Loss: 7.8793\n",
      "Iteration 10/1072, Loss: 7.9217\n",
      "Iteration 11/1072, Loss: 7.9319\n",
      "Iteration 12/1072, Loss: 7.8990\n",
      "Iteration 13/1072, Loss: 7.9667\n",
      "Iteration 14/1072, Loss: 7.8819\n",
      "Iteration 15/1072, Loss: 7.9803\n",
      "Iteration 16/1072, Loss: 7.9189\n",
      "Iteration 17/1072, Loss: 7.8680\n",
      "Iteration 18/1072, Loss: 7.9978\n",
      "Iteration 19/1072, Loss: 7.9644\n",
      "Iteration 20/1072, Loss: 7.9183\n",
      "Iteration 21/1072, Loss: 7.8956\n",
      "Iteration 22/1072, Loss: 7.9844\n",
      "Iteration 23/1072, Loss: 7.9095\n",
      "Iteration 24/1072, Loss: 7.9156\n",
      "Iteration 25/1072, Loss: 7.8925\n",
      "Iteration 26/1072, Loss: 7.9448\n",
      "Iteration 27/1072, Loss: 7.9141\n",
      "Iteration 28/1072, Loss: 7.9138\n",
      "Iteration 29/1072, Loss: 7.9408\n",
      "Iteration 30/1072, Loss: 7.8493\n",
      "Iteration 31/1072, Loss: 7.8958\n",
      "Iteration 32/1072, Loss: 7.8882\n",
      "Iteration 33/1072, Loss: 7.9287\n",
      "Iteration 34/1072, Loss: 7.9232\n",
      "Iteration 35/1072, Loss: 7.9314\n",
      "Iteration 36/1072, Loss: 7.9557\n",
      "Iteration 37/1072, Loss: 7.9487\n",
      "Iteration 38/1072, Loss: 7.9725\n",
      "Iteration 39/1072, Loss: 7.9456\n",
      "Iteration 40/1072, Loss: 7.9344\n",
      "Iteration 41/1072, Loss: 7.9267\n",
      "Iteration 42/1072, Loss: 7.9048\n",
      "Iteration 43/1072, Loss: 7.9707\n",
      "Iteration 44/1072, Loss: 7.9438\n",
      "Iteration 45/1072, Loss: 7.9381\n",
      "Iteration 46/1072, Loss: 8.0107\n",
      "Iteration 47/1072, Loss: 7.8890\n",
      "Iteration 48/1072, Loss: 7.9400\n",
      "Iteration 49/1072, Loss: 7.9335\n",
      "Iteration 50/1072, Loss: 7.9659\n",
      "Iteration 51/1072, Loss: 7.9346\n",
      "Iteration 52/1072, Loss: 7.9375\n",
      "Iteration 53/1072, Loss: 7.9653\n",
      "Iteration 54/1072, Loss: 7.8697\n",
      "Iteration 55/1072, Loss: 7.9464\n",
      "Iteration 56/1072, Loss: 7.9515\n",
      "Iteration 57/1072, Loss: 8.0027\n",
      "Iteration 58/1072, Loss: 7.9496\n",
      "Iteration 59/1072, Loss: 7.8962\n",
      "Iteration 60/1072, Loss: 7.9277\n",
      "Iteration 61/1072, Loss: 7.9493\n",
      "Iteration 62/1072, Loss: 7.9053\n",
      "Iteration 63/1072, Loss: 7.9452\n",
      "Iteration 64/1072, Loss: 7.9547\n",
      "Iteration 65/1072, Loss: 7.9746\n",
      "Iteration 66/1072, Loss: 7.9159\n",
      "Iteration 67/1072, Loss: 7.9015\n",
      "Iteration 68/1072, Loss: 7.9707\n",
      "Iteration 69/1072, Loss: 7.9359\n",
      "Iteration 70/1072, Loss: 7.9101\n",
      "Iteration 71/1072, Loss: 7.9403\n",
      "Iteration 72/1072, Loss: 7.9346\n",
      "Iteration 73/1072, Loss: 7.9549\n",
      "Iteration 74/1072, Loss: 7.9463\n",
      "Iteration 75/1072, Loss: 7.9091\n",
      "Iteration 76/1072, Loss: 7.9717\n",
      "Iteration 77/1072, Loss: 7.9184\n",
      "Iteration 78/1072, Loss: 7.8925\n",
      "Iteration 79/1072, Loss: 7.9752\n",
      "Iteration 80/1072, Loss: 7.9704\n",
      "Iteration 81/1072, Loss: 7.9298\n",
      "Iteration 82/1072, Loss: 7.9197\n",
      "Iteration 83/1072, Loss: 7.8912\n",
      "Iteration 84/1072, Loss: 7.9525\n",
      "Iteration 85/1072, Loss: 7.9305\n",
      "Iteration 86/1072, Loss: 7.9123\n",
      "Iteration 87/1072, Loss: 7.9009\n",
      "Iteration 88/1072, Loss: 7.9173\n",
      "Iteration 89/1072, Loss: 7.9052\n",
      "Iteration 90/1072, Loss: 7.9353\n",
      "Iteration 91/1072, Loss: 7.9021\n",
      "Iteration 92/1072, Loss: 7.9210\n",
      "Iteration 93/1072, Loss: 7.8472\n",
      "Iteration 94/1072, Loss: 7.9096\n",
      "Iteration 95/1072, Loss: 7.8751\n",
      "Iteration 96/1072, Loss: 7.9487\n",
      "Iteration 97/1072, Loss: 7.9149\n",
      "Iteration 98/1072, Loss: 7.9138\n",
      "Iteration 99/1072, Loss: 7.9279\n",
      "Iteration 100/1072, Loss: 7.9187\n",
      "Iteration 101/1072, Loss: 7.9121\n",
      "Iteration 102/1072, Loss: 7.9236\n",
      "Iteration 103/1072, Loss: 7.8953\n",
      "Iteration 104/1072, Loss: 7.9053\n",
      "Iteration 105/1072, Loss: 7.9294\n",
      "Iteration 106/1072, Loss: 7.9617\n",
      "Iteration 107/1072, Loss: 7.9681\n",
      "Iteration 108/1072, Loss: 7.9596\n",
      "Iteration 109/1072, Loss: 7.9409\n",
      "Iteration 110/1072, Loss: 7.9226\n",
      "Iteration 111/1072, Loss: 7.9300\n",
      "Iteration 112/1072, Loss: 7.9104\n",
      "Iteration 113/1072, Loss: 7.9451\n",
      "Iteration 114/1072, Loss: 7.9883\n",
      "Iteration 115/1072, Loss: 7.8807\n",
      "Iteration 116/1072, Loss: 7.9213\n",
      "Iteration 117/1072, Loss: 7.9735\n",
      "Iteration 118/1072, Loss: 7.9887\n",
      "Iteration 119/1072, Loss: 7.9298\n",
      "Iteration 120/1072, Loss: 7.9464\n",
      "Iteration 121/1072, Loss: 7.9458\n",
      "Iteration 122/1072, Loss: 7.9194\n",
      "Iteration 123/1072, Loss: 7.8880\n",
      "Iteration 124/1072, Loss: 7.9071\n",
      "Iteration 125/1072, Loss: 7.9157\n",
      "Iteration 126/1072, Loss: 7.9560\n",
      "Iteration 127/1072, Loss: 7.8963\n",
      "Iteration 128/1072, Loss: 7.9870\n",
      "Iteration 129/1072, Loss: 7.8747\n",
      "Iteration 130/1072, Loss: 7.9771\n",
      "Iteration 131/1072, Loss: 7.9121\n",
      "Iteration 132/1072, Loss: 7.9113\n",
      "Iteration 133/1072, Loss: 7.9312\n",
      "Iteration 134/1072, Loss: 7.9726\n",
      "Iteration 135/1072, Loss: 7.9141\n",
      "Iteration 136/1072, Loss: 7.9198\n",
      "Iteration 137/1072, Loss: 7.9791\n",
      "Iteration 138/1072, Loss: 7.9806\n",
      "Iteration 139/1072, Loss: 7.9972\n",
      "Iteration 140/1072, Loss: 7.9711\n",
      "Iteration 141/1072, Loss: 7.9461\n",
      "Iteration 142/1072, Loss: 7.9174\n",
      "Iteration 143/1072, Loss: 7.9426\n",
      "Iteration 144/1072, Loss: 7.9283\n",
      "Iteration 145/1072, Loss: 7.9212\n",
      "Iteration 146/1072, Loss: 7.9441\n",
      "Iteration 147/1072, Loss: 7.9282\n",
      "Iteration 148/1072, Loss: 7.9499\n",
      "Iteration 149/1072, Loss: 7.9589\n",
      "Iteration 150/1072, Loss: 7.9302\n",
      "Iteration 151/1072, Loss: 7.9226\n",
      "Iteration 152/1072, Loss: 7.9488\n",
      "Iteration 153/1072, Loss: 7.9537\n",
      "Iteration 154/1072, Loss: 7.9760\n",
      "Iteration 155/1072, Loss: 7.9316\n",
      "Iteration 156/1072, Loss: 7.9666\n",
      "Iteration 157/1072, Loss: 7.9381\n",
      "Iteration 158/1072, Loss: 7.9339\n",
      "Iteration 159/1072, Loss: 7.9811\n",
      "Iteration 160/1072, Loss: 7.9072\n",
      "Iteration 161/1072, Loss: 7.9469\n",
      "Iteration 162/1072, Loss: 7.9571\n",
      "Iteration 163/1072, Loss: 7.8893\n",
      "Iteration 164/1072, Loss: 7.8924\n",
      "Iteration 165/1072, Loss: 7.9013\n",
      "Iteration 166/1072, Loss: 7.9042\n",
      "Iteration 167/1072, Loss: 7.9355\n",
      "Iteration 168/1072, Loss: 7.8622\n",
      "Iteration 169/1072, Loss: 7.9260\n",
      "Iteration 170/1072, Loss: 7.9255\n",
      "Iteration 171/1072, Loss: 7.8833\n",
      "Iteration 172/1072, Loss: 7.9519\n",
      "Iteration 173/1072, Loss: 7.9011\n",
      "Iteration 174/1072, Loss: 7.9919\n",
      "Iteration 175/1072, Loss: 7.9364\n",
      "Iteration 176/1072, Loss: 7.9322\n",
      "Iteration 177/1072, Loss: 7.9486\n",
      "Iteration 178/1072, Loss: 7.9099\n",
      "Iteration 179/1072, Loss: 7.9321\n",
      "Iteration 180/1072, Loss: 7.9047\n",
      "Iteration 181/1072, Loss: 7.9645\n",
      "Iteration 182/1072, Loss: 7.9677\n",
      "Iteration 183/1072, Loss: 7.9962\n",
      "Iteration 184/1072, Loss: 7.9510\n",
      "Iteration 185/1072, Loss: 7.9120\n",
      "Iteration 186/1072, Loss: 7.8765\n",
      "Iteration 187/1072, Loss: 7.9474\n",
      "Iteration 188/1072, Loss: 7.9304\n",
      "Iteration 189/1072, Loss: 7.9483\n",
      "Iteration 190/1072, Loss: 7.9557\n",
      "Iteration 191/1072, Loss: 7.8977\n",
      "Iteration 192/1072, Loss: 7.9155\n",
      "Iteration 193/1072, Loss: 7.9730\n",
      "Iteration 194/1072, Loss: 7.9158\n",
      "Iteration 195/1072, Loss: 7.9249\n",
      "Iteration 196/1072, Loss: 7.9142\n",
      "Iteration 197/1072, Loss: 7.8963\n",
      "Iteration 198/1072, Loss: 7.8789\n",
      "Iteration 199/1072, Loss: 7.8920\n",
      "Iteration 200/1072, Loss: 7.9042\n",
      "Iteration 201/1072, Loss: 7.9541\n",
      "Iteration 202/1072, Loss: 7.8963\n",
      "Iteration 203/1072, Loss: 7.9349\n",
      "Iteration 204/1072, Loss: 7.9236\n",
      "Iteration 205/1072, Loss: 7.8662\n",
      "Iteration 206/1072, Loss: 7.9080\n",
      "Iteration 207/1072, Loss: 7.9521\n",
      "Iteration 208/1072, Loss: 7.9303\n",
      "Iteration 209/1072, Loss: 7.8684\n",
      "Iteration 210/1072, Loss: 7.9561\n",
      "Iteration 211/1072, Loss: 7.8747\n",
      "Iteration 212/1072, Loss: 7.9067\n",
      "Iteration 213/1072, Loss: 7.9537\n",
      "Iteration 214/1072, Loss: 7.9119\n",
      "Iteration 215/1072, Loss: 7.9333\n",
      "Iteration 216/1072, Loss: 7.8794\n",
      "Iteration 217/1072, Loss: 7.9341\n",
      "Iteration 218/1072, Loss: 7.9434\n",
      "Iteration 219/1072, Loss: 7.8904\n",
      "Iteration 220/1072, Loss: 7.9448\n",
      "Iteration 221/1072, Loss: 7.9208\n",
      "Iteration 222/1072, Loss: 7.9136\n",
      "Iteration 223/1072, Loss: 7.9173\n",
      "Iteration 224/1072, Loss: 7.9536\n",
      "Iteration 225/1072, Loss: 7.9050\n",
      "Iteration 226/1072, Loss: 7.9739\n",
      "Iteration 227/1072, Loss: 7.9338\n",
      "Iteration 228/1072, Loss: 7.9262\n",
      "Iteration 229/1072, Loss: 7.8889\n",
      "Iteration 230/1072, Loss: 7.8464\n",
      "Iteration 231/1072, Loss: 7.9557\n",
      "Iteration 232/1072, Loss: 7.9735\n",
      "Iteration 233/1072, Loss: 7.9476\n",
      "Iteration 234/1072, Loss: 7.9071\n",
      "Iteration 235/1072, Loss: 7.9016\n",
      "Iteration 236/1072, Loss: 7.9006\n",
      "Iteration 237/1072, Loss: 7.8964\n",
      "Iteration 238/1072, Loss: 7.8804\n",
      "Iteration 239/1072, Loss: 7.9540\n",
      "Iteration 240/1072, Loss: 7.9166\n",
      "Iteration 241/1072, Loss: 7.9185\n",
      "Iteration 242/1072, Loss: 7.9704\n",
      "Iteration 243/1072, Loss: 7.8690\n",
      "Iteration 244/1072, Loss: 7.8819\n",
      "Iteration 245/1072, Loss: 7.9607\n",
      "Iteration 246/1072, Loss: 7.9237\n",
      "Iteration 247/1072, Loss: 7.9526\n",
      "Iteration 248/1072, Loss: 7.9403\n",
      "Iteration 249/1072, Loss: 7.9292\n",
      "Iteration 250/1072, Loss: 7.8894\n",
      "Iteration 251/1072, Loss: 7.8764\n",
      "Iteration 252/1072, Loss: 7.9213\n",
      "Iteration 253/1072, Loss: 7.9398\n",
      "Iteration 254/1072, Loss: 7.9459\n",
      "Iteration 255/1072, Loss: 7.8744\n",
      "Iteration 256/1072, Loss: 7.9093\n",
      "Iteration 257/1072, Loss: 7.9377\n",
      "Iteration 258/1072, Loss: 7.9258\n",
      "Iteration 259/1072, Loss: 7.9695\n",
      "Iteration 260/1072, Loss: 7.8958\n",
      "Iteration 261/1072, Loss: 7.9177\n",
      "Iteration 262/1072, Loss: 7.8985\n",
      "Iteration 263/1072, Loss: 7.9568\n",
      "Iteration 264/1072, Loss: 7.8686\n",
      "Iteration 265/1072, Loss: 7.9661\n",
      "Iteration 266/1072, Loss: 7.9691\n",
      "Iteration 267/1072, Loss: 7.8999\n",
      "Iteration 268/1072, Loss: 7.9237\n",
      "Iteration 269/1072, Loss: 7.9566\n",
      "Iteration 270/1072, Loss: 7.8983\n",
      "Iteration 271/1072, Loss: 7.9618\n",
      "Iteration 272/1072, Loss: 7.9024\n",
      "Iteration 273/1072, Loss: 7.9173\n",
      "Iteration 274/1072, Loss: 7.9565\n",
      "Iteration 275/1072, Loss: 7.8607\n",
      "Iteration 276/1072, Loss: 7.9182\n",
      "Iteration 277/1072, Loss: 7.8889\n",
      "Iteration 278/1072, Loss: 7.9360\n",
      "Iteration 279/1072, Loss: 7.9286\n",
      "Iteration 280/1072, Loss: 7.9229\n",
      "Iteration 281/1072, Loss: 7.9695\n",
      "Iteration 282/1072, Loss: 7.9286\n",
      "Iteration 283/1072, Loss: 7.9570\n",
      "Iteration 284/1072, Loss: 7.9080\n",
      "Iteration 285/1072, Loss: 7.9200\n",
      "Iteration 286/1072, Loss: 7.9115\n",
      "Iteration 287/1072, Loss: 7.9532\n",
      "Iteration 288/1072, Loss: 7.8705\n",
      "Iteration 289/1072, Loss: 7.9380\n",
      "Iteration 290/1072, Loss: 7.9388\n",
      "Iteration 291/1072, Loss: 7.8450\n",
      "Iteration 292/1072, Loss: 7.9510\n",
      "Iteration 293/1072, Loss: 7.9827\n",
      "Iteration 294/1072, Loss: 7.8963\n",
      "Iteration 295/1072, Loss: 7.9205\n",
      "Iteration 296/1072, Loss: 7.8953\n",
      "Iteration 297/1072, Loss: 7.8779\n",
      "Iteration 298/1072, Loss: 7.8798\n",
      "Iteration 299/1072, Loss: 7.9129\n",
      "Iteration 300/1072, Loss: 7.8164\n",
      "Iteration 301/1072, Loss: 7.9387\n",
      "Iteration 302/1072, Loss: 7.8750\n",
      "Iteration 303/1072, Loss: 7.9277\n",
      "Iteration 304/1072, Loss: 7.8463\n",
      "Iteration 305/1072, Loss: 7.9456\n",
      "Iteration 306/1072, Loss: 7.8731\n",
      "Iteration 307/1072, Loss: 7.9237\n",
      "Iteration 308/1072, Loss: 7.9690\n",
      "Iteration 309/1072, Loss: 7.9363\n",
      "Iteration 310/1072, Loss: 7.9211\n",
      "Iteration 311/1072, Loss: 7.8851\n",
      "Iteration 312/1072, Loss: 7.8622\n",
      "Iteration 313/1072, Loss: 7.9535\n",
      "Iteration 314/1072, Loss: 7.8782\n",
      "Iteration 315/1072, Loss: 7.9252\n",
      "Iteration 316/1072, Loss: 7.9137\n",
      "Iteration 317/1072, Loss: 7.9018\n",
      "Iteration 318/1072, Loss: 7.9667\n",
      "Iteration 319/1072, Loss: 7.9730\n",
      "Iteration 320/1072, Loss: 7.9599\n",
      "Iteration 321/1072, Loss: 7.8649\n",
      "Iteration 322/1072, Loss: 7.9423\n",
      "Iteration 323/1072, Loss: 7.9447\n",
      "Iteration 324/1072, Loss: 7.9479\n",
      "Iteration 325/1072, Loss: 7.8781\n",
      "Iteration 326/1072, Loss: 7.9031\n",
      "Iteration 327/1072, Loss: 7.9124\n",
      "Iteration 328/1072, Loss: 7.9222\n",
      "Iteration 329/1072, Loss: 7.8617\n",
      "Iteration 330/1072, Loss: 7.9172\n",
      "Iteration 331/1072, Loss: 7.9208\n",
      "Iteration 332/1072, Loss: 7.9154\n",
      "Iteration 333/1072, Loss: 7.9114\n",
      "Iteration 334/1072, Loss: 7.8958\n",
      "Iteration 335/1072, Loss: 7.9670\n",
      "Iteration 336/1072, Loss: 7.8303\n",
      "Iteration 337/1072, Loss: 7.8802\n",
      "Iteration 338/1072, Loss: 7.9405\n",
      "Iteration 339/1072, Loss: 7.9240\n",
      "Iteration 340/1072, Loss: 7.9524\n",
      "Iteration 341/1072, Loss: 7.8985\n",
      "Iteration 342/1072, Loss: 7.9517\n",
      "Iteration 343/1072, Loss: 7.9618\n",
      "Iteration 344/1072, Loss: 7.8792\n",
      "Iteration 345/1072, Loss: 7.8969\n",
      "Iteration 346/1072, Loss: 7.8317\n",
      "Iteration 347/1072, Loss: 7.9209\n",
      "Iteration 348/1072, Loss: 8.0039\n",
      "Iteration 349/1072, Loss: 7.8680\n",
      "Iteration 350/1072, Loss: 7.8659\n",
      "Iteration 351/1072, Loss: 7.9424\n",
      "Iteration 352/1072, Loss: 7.8772\n",
      "Iteration 353/1072, Loss: 7.8753\n",
      "Iteration 354/1072, Loss: 7.9206\n",
      "Iteration 355/1072, Loss: 7.9011\n",
      "Iteration 356/1072, Loss: 7.8924\n",
      "Iteration 357/1072, Loss: 7.9798\n",
      "Iteration 358/1072, Loss: 7.9677\n",
      "Iteration 359/1072, Loss: 7.8996\n",
      "Iteration 360/1072, Loss: 7.9821\n",
      "Iteration 361/1072, Loss: 7.8808\n",
      "Iteration 362/1072, Loss: 7.9803\n",
      "Iteration 363/1072, Loss: 7.9379\n",
      "Iteration 364/1072, Loss: 7.9188\n",
      "Iteration 365/1072, Loss: 7.9334\n",
      "Iteration 366/1072, Loss: 7.9675\n",
      "Iteration 367/1072, Loss: 7.8515\n",
      "Iteration 368/1072, Loss: 7.9096\n",
      "Iteration 369/1072, Loss: 7.9331\n",
      "Iteration 370/1072, Loss: 7.8737\n",
      "Iteration 371/1072, Loss: 7.9473\n",
      "Iteration 372/1072, Loss: 7.8726\n",
      "Iteration 373/1072, Loss: 7.9665\n",
      "Iteration 374/1072, Loss: 7.9546\n",
      "Iteration 375/1072, Loss: 7.9337\n",
      "Iteration 376/1072, Loss: 7.9346\n",
      "Iteration 377/1072, Loss: 7.9329\n",
      "Iteration 378/1072, Loss: 7.9818\n",
      "Iteration 379/1072, Loss: 7.9138\n",
      "Iteration 380/1072, Loss: 7.9276\n",
      "Iteration 381/1072, Loss: 7.9170\n",
      "Iteration 382/1072, Loss: 7.9049\n",
      "Iteration 383/1072, Loss: 7.9257\n",
      "Iteration 384/1072, Loss: 7.9661\n",
      "Iteration 385/1072, Loss: 7.9089\n",
      "Iteration 386/1072, Loss: 7.9121\n",
      "Iteration 387/1072, Loss: 7.9505\n",
      "Iteration 388/1072, Loss: 7.8973\n",
      "Iteration 389/1072, Loss: 7.8318\n",
      "Iteration 390/1072, Loss: 7.9020\n",
      "Iteration 391/1072, Loss: 7.9572\n",
      "Iteration 392/1072, Loss: 7.8979\n",
      "Iteration 393/1072, Loss: 7.9078\n",
      "Iteration 394/1072, Loss: 7.9166\n",
      "Iteration 395/1072, Loss: 7.9243\n",
      "Iteration 396/1072, Loss: 7.9212\n",
      "Iteration 397/1072, Loss: 7.9449\n",
      "Iteration 398/1072, Loss: 7.9567\n",
      "Iteration 399/1072, Loss: 7.9350\n",
      "Iteration 400/1072, Loss: 7.9405\n",
      "Iteration 401/1072, Loss: 7.9142\n",
      "Iteration 402/1072, Loss: 7.8785\n",
      "Iteration 403/1072, Loss: 7.8843\n",
      "Iteration 404/1072, Loss: 7.9219\n",
      "Iteration 405/1072, Loss: 7.8858\n",
      "Iteration 406/1072, Loss: 7.8858\n",
      "Iteration 407/1072, Loss: 7.9434\n",
      "Iteration 408/1072, Loss: 7.8836\n",
      "Iteration 409/1072, Loss: 7.9569\n",
      "Iteration 410/1072, Loss: 7.8527\n",
      "Iteration 411/1072, Loss: 7.9083\n",
      "Iteration 412/1072, Loss: 7.8872\n",
      "Iteration 413/1072, Loss: 8.0011\n",
      "Iteration 414/1072, Loss: 7.9688\n",
      "Iteration 415/1072, Loss: 7.8764\n",
      "Iteration 416/1072, Loss: 7.8938\n",
      "Iteration 417/1072, Loss: 7.9292\n",
      "Iteration 418/1072, Loss: 7.8793\n",
      "Iteration 419/1072, Loss: 7.8849\n",
      "Iteration 420/1072, Loss: 7.9763\n",
      "Iteration 421/1072, Loss: 7.9074\n",
      "Iteration 422/1072, Loss: 7.9493\n",
      "Iteration 423/1072, Loss: 7.9189\n",
      "Iteration 424/1072, Loss: 7.8674\n",
      "Iteration 425/1072, Loss: 7.9321\n",
      "Iteration 426/1072, Loss: 7.9209\n",
      "Iteration 427/1072, Loss: 7.9331\n",
      "Iteration 428/1072, Loss: 7.8659\n",
      "Iteration 429/1072, Loss: 7.9045\n",
      "Iteration 430/1072, Loss: 7.9640\n",
      "Iteration 431/1072, Loss: 7.9262\n",
      "Iteration 432/1072, Loss: 7.9516\n",
      "Iteration 433/1072, Loss: 7.8793\n",
      "Iteration 434/1072, Loss: 7.8600\n",
      "Iteration 435/1072, Loss: 7.8700\n",
      "Iteration 436/1072, Loss: 7.9384\n",
      "Iteration 437/1072, Loss: 7.9179\n",
      "Iteration 438/1072, Loss: 7.9063\n",
      "Iteration 439/1072, Loss: 7.8968\n",
      "Iteration 440/1072, Loss: 7.9660\n",
      "Iteration 441/1072, Loss: 7.8965\n",
      "Iteration 442/1072, Loss: 7.9157\n",
      "Iteration 443/1072, Loss: 7.9699\n",
      "Iteration 444/1072, Loss: 7.9279\n",
      "Iteration 445/1072, Loss: 7.9559\n",
      "Iteration 446/1072, Loss: 7.9835\n",
      "Iteration 447/1072, Loss: 7.9297\n",
      "Iteration 448/1072, Loss: 7.9243\n",
      "Iteration 449/1072, Loss: 7.9460\n",
      "Iteration 450/1072, Loss: 7.9363\n",
      "Iteration 451/1072, Loss: 7.8958\n",
      "Iteration 452/1072, Loss: 7.8698\n",
      "Iteration 453/1072, Loss: 7.9406\n",
      "Iteration 454/1072, Loss: 7.8495\n",
      "Iteration 455/1072, Loss: 7.9412\n",
      "Iteration 456/1072, Loss: 7.8920\n",
      "Iteration 457/1072, Loss: 7.8576\n",
      "Iteration 458/1072, Loss: 7.9171\n",
      "Iteration 459/1072, Loss: 7.8973\n",
      "Iteration 460/1072, Loss: 7.9809\n",
      "Iteration 461/1072, Loss: 7.9389\n",
      "Iteration 462/1072, Loss: 7.9174\n",
      "Iteration 463/1072, Loss: 7.8959\n",
      "Iteration 464/1072, Loss: 7.9743\n",
      "Iteration 465/1072, Loss: 7.8949\n",
      "Iteration 466/1072, Loss: 7.8659\n",
      "Iteration 467/1072, Loss: 7.8924\n",
      "Iteration 468/1072, Loss: 7.8932\n",
      "Iteration 469/1072, Loss: 7.8981\n",
      "Iteration 470/1072, Loss: 7.9105\n",
      "Iteration 471/1072, Loss: 7.9242\n",
      "Iteration 472/1072, Loss: 7.9000\n",
      "Iteration 473/1072, Loss: 7.9043\n",
      "Iteration 474/1072, Loss: 7.9366\n",
      "Iteration 475/1072, Loss: 7.8929\n",
      "Iteration 476/1072, Loss: 7.8944\n",
      "Iteration 477/1072, Loss: 7.9358\n",
      "Iteration 478/1072, Loss: 7.9519\n",
      "Iteration 479/1072, Loss: 7.8829\n",
      "Iteration 480/1072, Loss: 7.9338\n",
      "Iteration 481/1072, Loss: 7.9029\n",
      "Iteration 482/1072, Loss: 7.8258\n",
      "Iteration 483/1072, Loss: 7.9261\n",
      "Iteration 484/1072, Loss: 8.0092\n",
      "Iteration 485/1072, Loss: 7.9022\n",
      "Iteration 486/1072, Loss: 7.9182\n",
      "Iteration 487/1072, Loss: 7.8739\n",
      "Iteration 488/1072, Loss: 7.8464\n",
      "Iteration 489/1072, Loss: 7.9271\n",
      "Iteration 490/1072, Loss: 7.9142\n",
      "Iteration 491/1072, Loss: 7.8686\n",
      "Iteration 492/1072, Loss: 7.9676\n",
      "Iteration 493/1072, Loss: 7.9019\n",
      "Iteration 494/1072, Loss: 7.9453\n",
      "Iteration 495/1072, Loss: 7.8703\n",
      "Iteration 496/1072, Loss: 7.8718\n",
      "Iteration 497/1072, Loss: 7.9465\n",
      "Iteration 498/1072, Loss: 7.9433\n",
      "Iteration 499/1072, Loss: 7.9101\n",
      "Iteration 500/1072, Loss: 7.9735\n",
      "Iteration 501/1072, Loss: 7.9210\n",
      "Iteration 502/1072, Loss: 7.9056\n",
      "Iteration 503/1072, Loss: 7.9191\n",
      "Iteration 504/1072, Loss: 7.9784\n",
      "Iteration 505/1072, Loss: 7.9581\n",
      "Iteration 506/1072, Loss: 7.9109\n",
      "Iteration 507/1072, Loss: 7.8458\n",
      "Iteration 508/1072, Loss: 7.8550\n",
      "Iteration 509/1072, Loss: 7.9533\n",
      "Iteration 510/1072, Loss: 7.9732\n",
      "Iteration 511/1072, Loss: 7.9106\n",
      "Iteration 512/1072, Loss: 7.8509\n",
      "Iteration 513/1072, Loss: 7.9394\n",
      "Iteration 514/1072, Loss: 7.9085\n",
      "Iteration 515/1072, Loss: 7.8950\n",
      "Iteration 516/1072, Loss: 7.8881\n",
      "Iteration 517/1072, Loss: 7.9608\n",
      "Iteration 518/1072, Loss: 7.8371\n",
      "Iteration 519/1072, Loss: 7.9224\n",
      "Iteration 520/1072, Loss: 7.9172\n",
      "Iteration 521/1072, Loss: 7.9489\n",
      "Iteration 522/1072, Loss: 7.8880\n",
      "Iteration 523/1072, Loss: 7.8805\n",
      "Iteration 524/1072, Loss: 7.9623\n",
      "Iteration 525/1072, Loss: 7.9547\n",
      "Iteration 526/1072, Loss: 7.8823\n",
      "Iteration 527/1072, Loss: 7.9292\n",
      "Iteration 528/1072, Loss: 7.9693\n",
      "Iteration 529/1072, Loss: 7.9316\n",
      "Iteration 530/1072, Loss: 7.9007\n",
      "Iteration 531/1072, Loss: 7.8949\n",
      "Iteration 532/1072, Loss: 7.8953\n",
      "Iteration 533/1072, Loss: 7.9474\n",
      "Iteration 534/1072, Loss: 7.8799\n",
      "Iteration 535/1072, Loss: 7.9230\n",
      "Iteration 536/1072, Loss: 7.9248\n",
      "Iteration 537/1072, Loss: 7.8964\n",
      "Iteration 538/1072, Loss: 7.9092\n",
      "Iteration 539/1072, Loss: 7.8939\n",
      "Iteration 540/1072, Loss: 7.8836\n",
      "Iteration 541/1072, Loss: 7.9379\n",
      "Iteration 542/1072, Loss: 7.9877\n",
      "Iteration 543/1072, Loss: 7.9254\n",
      "Iteration 544/1072, Loss: 7.8769\n",
      "Iteration 545/1072, Loss: 7.9579\n",
      "Iteration 546/1072, Loss: 7.9327\n",
      "Iteration 547/1072, Loss: 7.8838\n",
      "Iteration 548/1072, Loss: 7.9689\n",
      "Iteration 549/1072, Loss: 7.9224\n",
      "Iteration 550/1072, Loss: 7.9466\n",
      "Iteration 551/1072, Loss: 7.8865\n",
      "Iteration 552/1072, Loss: 7.9340\n",
      "Iteration 553/1072, Loss: 7.8927\n",
      "Iteration 554/1072, Loss: 7.8550\n",
      "Iteration 555/1072, Loss: 7.9196\n",
      "Iteration 556/1072, Loss: 7.9071\n",
      "Iteration 557/1072, Loss: 7.9189\n",
      "Iteration 558/1072, Loss: 7.9377\n",
      "Iteration 559/1072, Loss: 7.9197\n",
      "Iteration 560/1072, Loss: 7.9437\n",
      "Iteration 561/1072, Loss: 7.8902\n",
      "Iteration 562/1072, Loss: 7.9584\n",
      "Iteration 563/1072, Loss: 7.9072\n",
      "Iteration 564/1072, Loss: 7.8907\n",
      "Iteration 565/1072, Loss: 7.9108\n",
      "Iteration 566/1072, Loss: 7.8542\n",
      "Iteration 567/1072, Loss: 7.9142\n",
      "Iteration 568/1072, Loss: 7.9703\n",
      "Iteration 569/1072, Loss: 7.8989\n",
      "Iteration 570/1072, Loss: 7.8764\n",
      "Iteration 571/1072, Loss: 7.9186\n",
      "Iteration 572/1072, Loss: 7.8947\n",
      "Iteration 573/1072, Loss: 7.9116\n",
      "Iteration 574/1072, Loss: 7.8826\n",
      "Iteration 575/1072, Loss: 7.8987\n",
      "Iteration 576/1072, Loss: 7.8909\n",
      "Iteration 577/1072, Loss: 7.8915\n",
      "Iteration 578/1072, Loss: 7.9124\n",
      "Iteration 579/1072, Loss: 7.9409\n",
      "Iteration 580/1072, Loss: 7.9239\n",
      "Iteration 581/1072, Loss: 7.8601\n",
      "Iteration 582/1072, Loss: 7.9180\n",
      "Iteration 583/1072, Loss: 7.9192\n",
      "Iteration 584/1072, Loss: 7.8853\n",
      "Iteration 585/1072, Loss: 7.9204\n",
      "Iteration 586/1072, Loss: 7.9754\n",
      "Iteration 587/1072, Loss: 7.9927\n",
      "Iteration 588/1072, Loss: 7.9253\n",
      "Iteration 589/1072, Loss: 7.9283\n",
      "Iteration 590/1072, Loss: 7.8749\n",
      "Iteration 591/1072, Loss: 7.9407\n",
      "Iteration 592/1072, Loss: 7.8754\n",
      "Iteration 593/1072, Loss: 7.8816\n",
      "Iteration 594/1072, Loss: 7.9085\n",
      "Iteration 595/1072, Loss: 7.9722\n",
      "Iteration 596/1072, Loss: 7.9407\n",
      "Iteration 597/1072, Loss: 7.9689\n",
      "Iteration 598/1072, Loss: 7.9328\n",
      "Iteration 599/1072, Loss: 7.9480\n",
      "Iteration 600/1072, Loss: 7.9333\n",
      "Iteration 601/1072, Loss: 7.9255\n",
      "Iteration 602/1072, Loss: 7.9375\n",
      "Iteration 603/1072, Loss: 7.8866\n",
      "Iteration 604/1072, Loss: 7.8974\n",
      "Iteration 605/1072, Loss: 7.8672\n",
      "Iteration 606/1072, Loss: 7.9027\n",
      "Iteration 607/1072, Loss: 7.8907\n",
      "Iteration 608/1072, Loss: 7.9840\n",
      "Iteration 609/1072, Loss: 7.9743\n",
      "Iteration 610/1072, Loss: 7.9059\n",
      "Iteration 611/1072, Loss: 7.9133\n",
      "Iteration 612/1072, Loss: 7.9899\n",
      "Iteration 613/1072, Loss: 7.9593\n",
      "Iteration 614/1072, Loss: 7.9578\n",
      "Iteration 615/1072, Loss: 7.8830\n",
      "Iteration 616/1072, Loss: 7.8959\n",
      "Iteration 617/1072, Loss: 7.9328\n",
      "Iteration 618/1072, Loss: 7.8773\n",
      "Iteration 619/1072, Loss: 7.9408\n",
      "Iteration 620/1072, Loss: 7.9311\n",
      "Iteration 621/1072, Loss: 7.9117\n",
      "Iteration 622/1072, Loss: 7.9132\n",
      "Iteration 623/1072, Loss: 7.8930\n",
      "Iteration 624/1072, Loss: 7.9135\n",
      "Iteration 625/1072, Loss: 7.8633\n",
      "Iteration 626/1072, Loss: 7.9082\n",
      "Iteration 627/1072, Loss: 7.8887\n",
      "Iteration 628/1072, Loss: 7.8711\n",
      "Iteration 629/1072, Loss: 7.9548\n",
      "Iteration 630/1072, Loss: 7.9602\n",
      "Iteration 631/1072, Loss: 7.9562\n",
      "Iteration 632/1072, Loss: 7.9381\n",
      "Iteration 633/1072, Loss: 7.9600\n",
      "Iteration 634/1072, Loss: 7.8753\n",
      "Iteration 635/1072, Loss: 7.9270\n",
      "Iteration 636/1072, Loss: 7.9153\n",
      "Iteration 637/1072, Loss: 7.9345\n",
      "Iteration 638/1072, Loss: 7.9273\n",
      "Iteration 639/1072, Loss: 7.8853\n",
      "Iteration 640/1072, Loss: 7.9658\n",
      "Iteration 641/1072, Loss: 7.9652\n",
      "Iteration 642/1072, Loss: 7.8646\n",
      "Iteration 643/1072, Loss: 7.9625\n",
      "Iteration 644/1072, Loss: 7.9046\n",
      "Iteration 645/1072, Loss: 7.9760\n",
      "Iteration 646/1072, Loss: 7.9432\n",
      "Iteration 647/1072, Loss: 8.0256\n",
      "Iteration 648/1072, Loss: 7.9538\n",
      "Iteration 649/1072, Loss: 7.8775\n",
      "Iteration 650/1072, Loss: 7.9103\n",
      "Iteration 651/1072, Loss: 7.9252\n",
      "Iteration 652/1072, Loss: 7.8779\n",
      "Iteration 653/1072, Loss: 7.8987\n",
      "Iteration 654/1072, Loss: 7.9969\n",
      "Iteration 655/1072, Loss: 7.9402\n",
      "Iteration 656/1072, Loss: 7.8476\n",
      "Iteration 657/1072, Loss: 7.9484\n",
      "Iteration 658/1072, Loss: 7.9215\n",
      "Iteration 659/1072, Loss: 7.8748\n",
      "Iteration 660/1072, Loss: 7.9247\n",
      "Iteration 661/1072, Loss: 7.9544\n",
      "Iteration 662/1072, Loss: 8.0278\n",
      "Iteration 663/1072, Loss: 7.8971\n",
      "Iteration 664/1072, Loss: 7.8892\n",
      "Iteration 665/1072, Loss: 7.9365\n",
      "Iteration 666/1072, Loss: 7.9351\n",
      "Iteration 667/1072, Loss: 7.9065\n",
      "Iteration 668/1072, Loss: 7.9486\n",
      "Iteration 669/1072, Loss: 7.9279\n",
      "Iteration 670/1072, Loss: 7.9268\n",
      "Iteration 671/1072, Loss: 7.8840\n",
      "Iteration 672/1072, Loss: 7.9573\n",
      "Iteration 673/1072, Loss: 7.8450\n",
      "Iteration 674/1072, Loss: 7.9378\n",
      "Iteration 675/1072, Loss: 7.9160\n",
      "Iteration 676/1072, Loss: 7.9338\n",
      "Iteration 677/1072, Loss: 7.9049\n",
      "Iteration 678/1072, Loss: 7.8957\n",
      "Iteration 679/1072, Loss: 7.9222\n",
      "Iteration 680/1072, Loss: 7.8792\n",
      "Iteration 681/1072, Loss: 7.8684\n",
      "Iteration 682/1072, Loss: 7.9448\n",
      "Iteration 683/1072, Loss: 7.9392\n",
      "Iteration 684/1072, Loss: 7.8720\n",
      "Iteration 685/1072, Loss: 7.8867\n",
      "Iteration 686/1072, Loss: 7.9074\n",
      "Iteration 687/1072, Loss: 7.9211\n",
      "Iteration 688/1072, Loss: 7.9628\n",
      "Iteration 689/1072, Loss: 7.9346\n",
      "Iteration 690/1072, Loss: 7.8868\n",
      "Iteration 691/1072, Loss: 7.9137\n",
      "Iteration 692/1072, Loss: 7.9004\n",
      "Iteration 693/1072, Loss: 7.9333\n",
      "Iteration 694/1072, Loss: 7.9226\n",
      "Iteration 695/1072, Loss: 7.8540\n",
      "Iteration 696/1072, Loss: 7.8811\n",
      "Iteration 697/1072, Loss: 7.8447\n",
      "Iteration 698/1072, Loss: 7.9431\n",
      "Iteration 699/1072, Loss: 7.9378\n",
      "Iteration 700/1072, Loss: 7.9328\n",
      "Iteration 701/1072, Loss: 7.9191\n",
      "Iteration 702/1072, Loss: 7.8936\n",
      "Iteration 703/1072, Loss: 7.9009\n",
      "Iteration 704/1072, Loss: 7.9028\n",
      "Iteration 705/1072, Loss: 7.9598\n",
      "Iteration 706/1072, Loss: 7.9108\n",
      "Iteration 707/1072, Loss: 7.9448\n",
      "Iteration 708/1072, Loss: 7.8921\n",
      "Iteration 709/1072, Loss: 7.9007\n",
      "Iteration 710/1072, Loss: 7.9435\n",
      "Iteration 711/1072, Loss: 7.8764\n",
      "Iteration 712/1072, Loss: 7.8846\n",
      "Iteration 713/1072, Loss: 7.9395\n",
      "Iteration 714/1072, Loss: 7.9088\n",
      "Iteration 715/1072, Loss: 7.9405\n",
      "Iteration 716/1072, Loss: 7.8823\n",
      "Iteration 717/1072, Loss: 7.9473\n",
      "Iteration 718/1072, Loss: 7.9202\n",
      "Iteration 719/1072, Loss: 7.9430\n",
      "Iteration 720/1072, Loss: 7.8702\n",
      "Iteration 721/1072, Loss: 7.9052\n",
      "Iteration 722/1072, Loss: 7.9368\n",
      "Iteration 723/1072, Loss: 7.9450\n",
      "Iteration 724/1072, Loss: 7.9646\n",
      "Iteration 725/1072, Loss: 7.8767\n",
      "Iteration 726/1072, Loss: 7.9652\n",
      "Iteration 727/1072, Loss: 7.8441\n",
      "Iteration 728/1072, Loss: 7.8946\n",
      "Iteration 729/1072, Loss: 7.9451\n",
      "Iteration 730/1072, Loss: 7.9320\n",
      "Iteration 731/1072, Loss: 7.9387\n",
      "Iteration 732/1072, Loss: 7.9014\n",
      "Iteration 733/1072, Loss: 7.9595\n",
      "Iteration 734/1072, Loss: 7.8905\n",
      "Iteration 735/1072, Loss: 7.9105\n",
      "Iteration 736/1072, Loss: 7.8957\n",
      "Iteration 737/1072, Loss: 7.8695\n",
      "Iteration 738/1072, Loss: 7.8585\n",
      "Iteration 739/1072, Loss: 7.8928\n",
      "Iteration 740/1072, Loss: 7.8567\n",
      "Iteration 741/1072, Loss: 7.9444\n",
      "Iteration 742/1072, Loss: 7.8852\n",
      "Iteration 743/1072, Loss: 7.8998\n",
      "Iteration 744/1072, Loss: 7.9478\n",
      "Iteration 745/1072, Loss: 7.9111\n",
      "Iteration 746/1072, Loss: 7.9425\n",
      "Iteration 747/1072, Loss: 7.8976\n",
      "Iteration 748/1072, Loss: 7.9122\n",
      "Iteration 749/1072, Loss: 7.8977\n",
      "Iteration 750/1072, Loss: 7.9381\n",
      "Iteration 751/1072, Loss: 7.9159\n",
      "Iteration 752/1072, Loss: 7.8265\n",
      "Iteration 753/1072, Loss: 7.8686\n",
      "Iteration 754/1072, Loss: 7.9528\n",
      "Iteration 755/1072, Loss: 7.9978\n",
      "Iteration 756/1072, Loss: 7.8845\n",
      "Iteration 757/1072, Loss: 7.9438\n",
      "Iteration 758/1072, Loss: 7.9556\n",
      "Iteration 759/1072, Loss: 7.8891\n",
      "Iteration 760/1072, Loss: 7.8744\n",
      "Iteration 761/1072, Loss: 7.8945\n",
      "Iteration 762/1072, Loss: 7.8875\n",
      "Iteration 763/1072, Loss: 7.8999\n",
      "Iteration 764/1072, Loss: 7.9226\n",
      "Iteration 765/1072, Loss: 7.9237\n",
      "Iteration 766/1072, Loss: 8.0013\n",
      "Iteration 767/1072, Loss: 7.8767\n",
      "Iteration 768/1072, Loss: 7.9042\n",
      "Iteration 769/1072, Loss: 7.8716\n",
      "Iteration 770/1072, Loss: 7.9594\n",
      "Iteration 771/1072, Loss: 7.8291\n",
      "Iteration 772/1072, Loss: 7.9642\n",
      "Iteration 773/1072, Loss: 7.8550\n",
      "Iteration 774/1072, Loss: 7.9282\n",
      "Iteration 775/1072, Loss: 7.8576\n",
      "Iteration 776/1072, Loss: 7.8977\n",
      "Iteration 777/1072, Loss: 7.8952\n",
      "Iteration 778/1072, Loss: 7.9429\n",
      "Iteration 779/1072, Loss: 7.9076\n",
      "Iteration 780/1072, Loss: 7.9138\n",
      "Iteration 781/1072, Loss: 7.9278\n",
      "Iteration 782/1072, Loss: 7.8897\n",
      "Iteration 783/1072, Loss: 7.8522\n",
      "Iteration 784/1072, Loss: 7.8977\n",
      "Iteration 785/1072, Loss: 7.8399\n",
      "Iteration 786/1072, Loss: 7.8419\n",
      "Iteration 787/1072, Loss: 7.9298\n",
      "Iteration 788/1072, Loss: 7.8863\n",
      "Iteration 789/1072, Loss: 7.9015\n",
      "Iteration 790/1072, Loss: 7.8994\n",
      "Iteration 791/1072, Loss: 7.8841\n",
      "Iteration 792/1072, Loss: 7.8754\n",
      "Iteration 793/1072, Loss: 7.9336\n",
      "Iteration 794/1072, Loss: 7.9028\n",
      "Iteration 795/1072, Loss: 7.8593\n",
      "Iteration 796/1072, Loss: 7.8337\n",
      "Iteration 797/1072, Loss: 7.8990\n",
      "Iteration 798/1072, Loss: 7.9418\n",
      "Iteration 799/1072, Loss: 7.8920\n",
      "Iteration 800/1072, Loss: 7.8996\n",
      "Iteration 801/1072, Loss: 7.9087\n",
      "Iteration 802/1072, Loss: 7.9329\n",
      "Iteration 803/1072, Loss: 7.9095\n",
      "Iteration 804/1072, Loss: 7.8842\n",
      "Iteration 805/1072, Loss: 7.8252\n",
      "Iteration 806/1072, Loss: 7.8889\n",
      "Iteration 807/1072, Loss: 7.9297\n",
      "Iteration 808/1072, Loss: 7.8771\n",
      "Iteration 809/1072, Loss: 7.9675\n",
      "Iteration 810/1072, Loss: 7.9389\n",
      "Iteration 811/1072, Loss: 7.9032\n",
      "Iteration 812/1072, Loss: 7.9193\n",
      "Iteration 813/1072, Loss: 7.8231\n",
      "Iteration 814/1072, Loss: 7.9022\n",
      "Iteration 815/1072, Loss: 7.8554\n",
      "Iteration 816/1072, Loss: 7.9292\n",
      "Iteration 817/1072, Loss: 7.9251\n",
      "Iteration 818/1072, Loss: 7.9700\n",
      "Iteration 819/1072, Loss: 7.8815\n",
      "Iteration 820/1072, Loss: 7.9449\n",
      "Iteration 821/1072, Loss: 7.9684\n",
      "Iteration 822/1072, Loss: 7.9309\n",
      "Iteration 823/1072, Loss: 7.9321\n",
      "Iteration 824/1072, Loss: 7.8926\n",
      "Iteration 825/1072, Loss: 7.9854\n",
      "Iteration 826/1072, Loss: 7.9057\n",
      "Iteration 827/1072, Loss: 7.8835\n",
      "Iteration 828/1072, Loss: 7.9079\n",
      "Iteration 829/1072, Loss: 7.9416\n",
      "Iteration 830/1072, Loss: 7.8995\n",
      "Iteration 831/1072, Loss: 7.8954\n",
      "Iteration 832/1072, Loss: 7.8466\n",
      "Iteration 833/1072, Loss: 7.9113\n",
      "Iteration 834/1072, Loss: 7.8611\n",
      "Iteration 835/1072, Loss: 7.8839\n",
      "Iteration 836/1072, Loss: 7.9154\n",
      "Iteration 837/1072, Loss: 7.9403\n",
      "Iteration 838/1072, Loss: 7.9790\n",
      "Iteration 839/1072, Loss: 7.9285\n",
      "Iteration 840/1072, Loss: 7.9051\n",
      "Iteration 841/1072, Loss: 7.8412\n",
      "Iteration 842/1072, Loss: 7.9221\n",
      "Iteration 843/1072, Loss: 7.8848\n",
      "Iteration 844/1072, Loss: 7.9295\n",
      "Iteration 845/1072, Loss: 7.9023\n",
      "Iteration 846/1072, Loss: 7.9412\n",
      "Iteration 847/1072, Loss: 7.9237\n",
      "Iteration 848/1072, Loss: 7.9503\n",
      "Iteration 849/1072, Loss: 7.9044\n",
      "Iteration 850/1072, Loss: 7.8791\n",
      "Iteration 851/1072, Loss: 7.9746\n",
      "Iteration 852/1072, Loss: 7.9149\n",
      "Iteration 853/1072, Loss: 7.8920\n",
      "Iteration 854/1072, Loss: 7.8993\n",
      "Iteration 855/1072, Loss: 7.9035\n",
      "Iteration 856/1072, Loss: 7.8887\n",
      "Iteration 857/1072, Loss: 7.9504\n",
      "Iteration 858/1072, Loss: 7.9020\n",
      "Iteration 859/1072, Loss: 7.8808\n",
      "Iteration 860/1072, Loss: 7.8744\n",
      "Iteration 861/1072, Loss: 7.9756\n",
      "Iteration 862/1072, Loss: 7.9672\n",
      "Iteration 863/1072, Loss: 7.9394\n",
      "Iteration 864/1072, Loss: 7.9408\n",
      "Iteration 865/1072, Loss: 7.8646\n",
      "Iteration 866/1072, Loss: 8.0197\n",
      "Iteration 867/1072, Loss: 7.9208\n",
      "Iteration 868/1072, Loss: 7.8870\n",
      "Iteration 869/1072, Loss: 7.9340\n",
      "Iteration 870/1072, Loss: 7.8817\n",
      "Iteration 871/1072, Loss: 7.9049\n",
      "Iteration 872/1072, Loss: 7.9737\n",
      "Iteration 873/1072, Loss: 7.9179\n",
      "Iteration 874/1072, Loss: 7.8032\n",
      "Iteration 875/1072, Loss: 7.9187\n",
      "Iteration 876/1072, Loss: 7.8729\n",
      "Iteration 877/1072, Loss: 7.9372\n",
      "Iteration 878/1072, Loss: 7.9430\n",
      "Iteration 879/1072, Loss: 7.8631\n",
      "Iteration 880/1072, Loss: 7.9652\n",
      "Iteration 881/1072, Loss: 7.9118\n",
      "Iteration 882/1072, Loss: 7.8428\n",
      "Iteration 883/1072, Loss: 7.8526\n",
      "Iteration 884/1072, Loss: 7.8945\n",
      "Iteration 885/1072, Loss: 7.9276\n",
      "Iteration 886/1072, Loss: 7.9536\n",
      "Iteration 887/1072, Loss: 7.9122\n",
      "Iteration 888/1072, Loss: 7.9028\n",
      "Iteration 889/1072, Loss: 7.9055\n",
      "Iteration 890/1072, Loss: 7.8778\n",
      "Iteration 891/1072, Loss: 7.8578\n",
      "Iteration 892/1072, Loss: 7.8124\n",
      "Iteration 893/1072, Loss: 7.8635\n",
      "Iteration 894/1072, Loss: 7.9612\n",
      "Iteration 895/1072, Loss: 7.8877\n",
      "Iteration 896/1072, Loss: 7.9479\n",
      "Iteration 897/1072, Loss: 7.9100\n",
      "Iteration 898/1072, Loss: 7.9735\n",
      "Iteration 899/1072, Loss: 7.8960\n",
      "Iteration 900/1072, Loss: 7.9078\n",
      "Iteration 901/1072, Loss: 7.8937\n",
      "Iteration 902/1072, Loss: 7.9208\n",
      "Iteration 903/1072, Loss: 7.8937\n",
      "Iteration 904/1072, Loss: 7.8747\n",
      "Iteration 905/1072, Loss: 7.8925\n",
      "Iteration 906/1072, Loss: 7.8948\n",
      "Iteration 907/1072, Loss: 7.9477\n",
      "Iteration 908/1072, Loss: 7.9633\n",
      "Iteration 909/1072, Loss: 7.9256\n",
      "Iteration 910/1072, Loss: 7.9727\n",
      "Iteration 911/1072, Loss: 7.8691\n",
      "Iteration 912/1072, Loss: 7.9041\n",
      "Iteration 913/1072, Loss: 7.9695\n",
      "Iteration 914/1072, Loss: 7.8756\n",
      "Iteration 915/1072, Loss: 7.9348\n",
      "Iteration 916/1072, Loss: 7.9321\n",
      "Iteration 917/1072, Loss: 7.9275\n",
      "Iteration 918/1072, Loss: 7.9278\n",
      "Iteration 919/1072, Loss: 7.9577\n",
      "Iteration 920/1072, Loss: 7.8403\n",
      "Iteration 921/1072, Loss: 7.9400\n",
      "Iteration 922/1072, Loss: 7.8907\n",
      "Iteration 923/1072, Loss: 7.8775\n",
      "Iteration 924/1072, Loss: 7.9221\n",
      "Iteration 925/1072, Loss: 7.9616\n",
      "Iteration 926/1072, Loss: 7.8934\n",
      "Iteration 927/1072, Loss: 7.9320\n",
      "Iteration 928/1072, Loss: 7.9511\n",
      "Iteration 929/1072, Loss: 7.9174\n",
      "Iteration 930/1072, Loss: 7.9222\n",
      "Iteration 931/1072, Loss: 7.8505\n",
      "Iteration 932/1072, Loss: 7.9282\n",
      "Iteration 933/1072, Loss: 7.9182\n",
      "Iteration 934/1072, Loss: 7.8848\n",
      "Iteration 935/1072, Loss: 7.9102\n",
      "Iteration 936/1072, Loss: 7.8992\n",
      "Iteration 937/1072, Loss: 7.9149\n",
      "Iteration 938/1072, Loss: 7.9221\n",
      "Iteration 939/1072, Loss: 7.8533\n",
      "Iteration 940/1072, Loss: 7.9273\n",
      "Iteration 941/1072, Loss: 7.9008\n",
      "Iteration 942/1072, Loss: 7.9096\n",
      "Iteration 943/1072, Loss: 7.8985\n",
      "Iteration 944/1072, Loss: 7.9223\n",
      "Iteration 945/1072, Loss: 7.9379\n",
      "Iteration 946/1072, Loss: 7.8808\n",
      "Iteration 947/1072, Loss: 7.9466\n",
      "Iteration 948/1072, Loss: 7.8678\n",
      "Iteration 949/1072, Loss: 7.9392\n",
      "Iteration 950/1072, Loss: 7.9599\n",
      "Iteration 951/1072, Loss: 7.8715\n",
      "Iteration 952/1072, Loss: 7.9142\n",
      "Iteration 953/1072, Loss: 7.8503\n",
      "Iteration 954/1072, Loss: 7.9688\n",
      "Iteration 955/1072, Loss: 7.9067\n",
      "Iteration 956/1072, Loss: 7.8864\n",
      "Iteration 957/1072, Loss: 7.8963\n",
      "Iteration 958/1072, Loss: 7.9207\n",
      "Iteration 959/1072, Loss: 7.9379\n",
      "Iteration 960/1072, Loss: 7.9102\n",
      "Iteration 961/1072, Loss: 7.8793\n",
      "Iteration 962/1072, Loss: 7.9484\n",
      "Iteration 963/1072, Loss: 7.9194\n",
      "Iteration 964/1072, Loss: 7.9387\n",
      "Iteration 965/1072, Loss: 7.9641\n",
      "Iteration 966/1072, Loss: 7.9251\n",
      "Iteration 967/1072, Loss: 7.9036\n",
      "Iteration 968/1072, Loss: 7.8663\n",
      "Iteration 969/1072, Loss: 7.9078\n",
      "Iteration 970/1072, Loss: 7.8751\n",
      "Iteration 971/1072, Loss: 7.8371\n",
      "Iteration 972/1072, Loss: 7.8998\n",
      "Iteration 973/1072, Loss: 7.9376\n",
      "Iteration 974/1072, Loss: 7.8857\n",
      "Iteration 975/1072, Loss: 7.9247\n",
      "Iteration 976/1072, Loss: 7.9192\n",
      "Iteration 977/1072, Loss: 7.9153\n",
      "Iteration 978/1072, Loss: 7.8297\n",
      "Iteration 979/1072, Loss: 7.9215\n",
      "Iteration 980/1072, Loss: 7.9087\n",
      "Iteration 981/1072, Loss: 7.8425\n",
      "Iteration 982/1072, Loss: 7.8559\n",
      "Iteration 983/1072, Loss: 7.8794\n",
      "Iteration 984/1072, Loss: 7.9538\n",
      "Iteration 985/1072, Loss: 7.9288\n",
      "Iteration 986/1072, Loss: 7.8852\n",
      "Iteration 987/1072, Loss: 7.8924\n",
      "Iteration 988/1072, Loss: 7.8957\n",
      "Iteration 989/1072, Loss: 7.8684\n",
      "Iteration 990/1072, Loss: 7.9118\n",
      "Iteration 991/1072, Loss: 7.8885\n",
      "Iteration 992/1072, Loss: 7.9157\n",
      "Iteration 993/1072, Loss: 7.8953\n",
      "Iteration 994/1072, Loss: 7.9210\n",
      "Iteration 995/1072, Loss: 7.9319\n",
      "Iteration 996/1072, Loss: 7.9445\n",
      "Iteration 997/1072, Loss: 7.8805\n",
      "Iteration 998/1072, Loss: 7.9196\n",
      "Iteration 999/1072, Loss: 7.8537\n",
      "Iteration 1000/1072, Loss: 7.9160\n",
      "Iteration 1001/1072, Loss: 7.9207\n",
      "Iteration 1002/1072, Loss: 7.8811\n",
      "Iteration 1003/1072, Loss: 7.9124\n",
      "Iteration 1004/1072, Loss: 7.8859\n",
      "Iteration 1005/1072, Loss: 7.8579\n",
      "Iteration 1006/1072, Loss: 7.9666\n",
      "Iteration 1007/1072, Loss: 7.8323\n",
      "Iteration 1008/1072, Loss: 7.9545\n",
      "Iteration 1009/1072, Loss: 7.9412\n",
      "Iteration 1010/1072, Loss: 7.9007\n",
      "Iteration 1011/1072, Loss: 7.8437\n",
      "Iteration 1012/1072, Loss: 7.8796\n",
      "Iteration 1013/1072, Loss: 7.8380\n",
      "Iteration 1014/1072, Loss: 7.8930\n",
      "Iteration 1015/1072, Loss: 7.9174\n",
      "Iteration 1016/1072, Loss: 7.9111\n",
      "Iteration 1017/1072, Loss: 7.9164\n",
      "Iteration 1018/1072, Loss: 7.9215\n",
      "Iteration 1019/1072, Loss: 7.8652\n",
      "Iteration 1020/1072, Loss: 7.9116\n",
      "Iteration 1021/1072, Loss: 7.8818\n",
      "Iteration 1022/1072, Loss: 7.8657\n",
      "Iteration 1023/1072, Loss: 7.8594\n",
      "Iteration 1024/1072, Loss: 7.9026\n",
      "Iteration 1025/1072, Loss: 7.9006\n",
      "Iteration 1026/1072, Loss: 7.8666\n",
      "Iteration 1027/1072, Loss: 7.8849\n",
      "Iteration 1028/1072, Loss: 7.9187\n",
      "Iteration 1029/1072, Loss: 7.9372\n",
      "Iteration 1030/1072, Loss: 7.9034\n",
      "Iteration 1031/1072, Loss: 7.9433\n",
      "Iteration 1032/1072, Loss: 7.9331\n",
      "Iteration 1033/1072, Loss: 7.9375\n",
      "Iteration 1034/1072, Loss: 7.9135\n",
      "Iteration 1035/1072, Loss: 7.9141\n",
      "Iteration 1036/1072, Loss: 7.8823\n",
      "Iteration 1037/1072, Loss: 7.8758\n",
      "Iteration 1038/1072, Loss: 7.9185\n",
      "Iteration 1039/1072, Loss: 7.8848\n",
      "Iteration 1040/1072, Loss: 7.8052\n",
      "Iteration 1041/1072, Loss: 7.9204\n",
      "Iteration 1042/1072, Loss: 7.8961\n",
      "Iteration 1043/1072, Loss: 7.8905\n",
      "Iteration 1044/1072, Loss: 7.7901\n",
      "Iteration 1045/1072, Loss: 7.8628\n",
      "Iteration 1046/1072, Loss: 7.8811\n",
      "Iteration 1047/1072, Loss: 7.8660\n",
      "Iteration 1048/1072, Loss: 7.8561\n",
      "Iteration 1049/1072, Loss: 7.8769\n",
      "Iteration 1050/1072, Loss: 7.9155\n",
      "Iteration 1051/1072, Loss: 7.9043\n",
      "Iteration 1052/1072, Loss: 7.8986\n",
      "Iteration 1053/1072, Loss: 7.9027\n",
      "Iteration 1054/1072, Loss: 7.8789\n",
      "Iteration 1055/1072, Loss: 7.8704\n",
      "Iteration 1056/1072, Loss: 7.9043\n",
      "Iteration 1057/1072, Loss: 7.8413\n",
      "Iteration 1058/1072, Loss: 7.8948\n",
      "Iteration 1059/1072, Loss: 7.8847\n",
      "Iteration 1060/1072, Loss: 7.9152\n",
      "Iteration 1061/1072, Loss: 7.9270\n",
      "Iteration 1062/1072, Loss: 7.8373\n",
      "Iteration 1063/1072, Loss: 7.8659\n",
      "Iteration 1064/1072, Loss: 7.9431\n",
      "Iteration 1065/1072, Loss: 7.9207\n",
      "Iteration 1066/1072, Loss: 7.8661\n",
      "Iteration 1067/1072, Loss: 7.9003\n",
      "Iteration 1068/1072, Loss: 7.9303\n",
      "Iteration 1069/1072, Loss: 7.8971\n",
      "Iteration 1070/1072, Loss: 7.8533\n",
      "Iteration 1071/1072, Loss: 7.9612\n",
      "Iteration 1072/1072, Loss: 7.9909\n",
      "Epoch 2/10, Loss: 7.9161\n",
      "Checkpoint saved at checkpoints/efficientnet-b0_epoch_2.pth\n",
      "Validation Accuracy: 0.72%\n",
      "Iteration 1/1072, Loss: 7.9079\n",
      "Iteration 2/1072, Loss: 7.8427\n",
      "Iteration 3/1072, Loss: 7.8690\n",
      "Iteration 4/1072, Loss: 7.8993\n",
      "Iteration 5/1072, Loss: 7.9220\n",
      "Iteration 6/1072, Loss: 7.8021\n",
      "Iteration 7/1072, Loss: 7.8938\n",
      "Iteration 8/1072, Loss: 7.8167\n",
      "Iteration 9/1072, Loss: 7.8324\n",
      "Iteration 10/1072, Loss: 7.8321\n",
      "Iteration 11/1072, Loss: 7.8949\n",
      "Iteration 12/1072, Loss: 7.8120\n",
      "Iteration 13/1072, Loss: 7.8789\n",
      "Iteration 14/1072, Loss: 7.8710\n",
      "Iteration 15/1072, Loss: 7.8333\n",
      "Iteration 19/1072, Loss: 7.8376\n",
      "Iteration 20/1072, Loss: 7.8510\n",
      "Iteration 21/1072, Loss: 7.7922\n",
      "Iteration 22/1072, Loss: 7.9310\n",
      "Iteration 23/1072, Loss: 7.8726\n",
      "Iteration 24/1072, Loss: 7.8888\n",
      "Iteration 25/1072, Loss: 7.8221\n",
      "Iteration 26/1072, Loss: 7.9439\n",
      "Iteration 27/1072, Loss: 7.8855\n",
      "Iteration 28/1072, Loss: 7.8528\n",
      "Iteration 29/1072, Loss: 7.8374\n",
      "Iteration 30/1072, Loss: 7.8602\n",
      "Iteration 31/1072, Loss: 7.8368\n",
      "Iteration 32/1072, Loss: 7.8271\n",
      "Iteration 33/1072, Loss: 7.8467\n",
      "Iteration 34/1072, Loss: 7.8674\n",
      "Iteration 104/1072, Loss: 7.8807\n",
      "Iteration 105/1072, Loss: 7.8892\n",
      "Iteration 106/1072, Loss: 7.9010\n",
      "Iteration 107/1072, Loss: 7.8501\n",
      "Iteration 108/1072, Loss: 7.8384\n",
      "Iteration 109/1072, Loss: 7.9407\n",
      "Iteration 110/1072, Loss: 7.8719\n",
      "Iteration 111/1072, Loss: 7.8673\n",
      "Iteration 112/1072, Loss: 7.8515\n",
      "Iteration 113/1072, Loss: 7.9648\n",
      "Iteration 114/1072, Loss: 7.8643\n",
      "Iteration 115/1072, Loss: 7.9262\n",
      "Iteration 116/1072, Loss: 7.8566\n",
      "Iteration 117/1072, Loss: 7.9018\n",
      "Iteration 118/1072, Loss: 7.8369\n",
      "Iteration 119/1072, Loss: 7.8542\n",
      "Iteration 120/1072, Loss: 7.8837\n",
      "Iteration 121/1072, Loss: 7.9019\n",
      "Iteration 122/1072, Loss: 7.8483\n",
      "Iteration 123/1072, Loss: 7.8175\n",
      "Iteration 124/1072, Loss: 7.8666\n",
      "Iteration 125/1072, Loss: 7.8625\n",
      "Iteration 126/1072, Loss: 7.8645\n",
      "Iteration 127/1072, Loss: 7.8297\n",
      "Iteration 128/1072, Loss: 7.8481\n",
      "Iteration 129/1072, Loss: 7.8531\n",
      "Iteration 130/1072, Loss: 7.9040\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        print(f\"Iteration {i+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(epoch_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "    # Save model checkpoint after each epoch\n",
    "    checkpoint_path = os.path.join(\"checkpoints\", f\"efficientnet-b0_epoch_{epoch+1}.pth\")\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "    print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "    \n",
    "    # Validation\n",
    "    val_dataset = CustomDataset('validation.csv', 'final/validation_images', transform=data_transforms)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_running_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_loss = val_running_loss / len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Validation Accuracy: {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update best accuracy and save best model checkpoint\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_checkpoint_path = os.path.join(\"checkpoints\", \"efficientnet-b0_best_checkpoint.pth\")\n",
    "        torch.save(model.state_dict(), best_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "450ef9d9-a987-4c8e-aa87-b2332f8f3fd3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (10,) and (11,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/ipykernel_2853/1362961890.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2767\u001b[0m     return gca().plot(\n\u001b[1;32m   2768\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2769\u001b[0;31m         **({\"data\": data} if data is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1633\u001b[0m         \"\"\"\n\u001b[1;32m   1634\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1635\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1636\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1637\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    499\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    500\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (10,) and (11,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplot\n",
    "plt.plot(range(1, num_epochs+1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, num_epochs+1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77deebe8-af15-4d77-9597-c2914cecbcd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-13.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m108"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

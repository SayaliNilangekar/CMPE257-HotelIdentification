{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b328a33f-216a-43e2-8120-c2a485eed2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebb93bf3-2ea6-42e3-9c7e-4b5c77efc0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a01d67e-948d-4961-b75b-49fbae114017",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.classes = self.data['hotel_id'].unique().tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, str(self.data.iloc[idx, 1]), str(self.data.iloc[idx, 0]))\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        label = self.classes.index(self.data.iloc[idx, 1])\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f1a4454-2ab6-4e56-852f-70c188494113",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset('train.csv', 'final/train_images', transform=data_transforms)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "788a6382-6ea2-4f4e-b082-cef3f23615e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and may be removed in the future, \"\n",
      "/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet50(pretrained=True)\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, len(train_dataset.classes))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8c3a573-2dfa-4b5d-8765-30cb3b0dcfba",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=3116, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ed9fca-6d6c-4b60-a864-31f091eba6a5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/1072, Loss: 8.0329\n",
      "Iteration 2/1072, Loss: 8.1684\n",
      "Iteration 3/1072, Loss: 8.0840\n",
      "Iteration 4/1072, Loss: 8.1088\n",
      "Iteration 5/1072, Loss: 8.0804\n",
      "Iteration 6/1072, Loss: 8.1896\n",
      "Iteration 7/1072, Loss: 8.1241\n",
      "Iteration 8/1072, Loss: 8.1859\n",
      "Iteration 9/1072, Loss: 8.0456\n",
      "Iteration 10/1072, Loss: 8.0489\n",
      "Iteration 11/1072, Loss: 8.0678\n",
      "Iteration 12/1072, Loss: 8.0958\n",
      "Iteration 13/1072, Loss: 8.0902\n",
      "Iteration 14/1072, Loss: 8.1778\n",
      "Iteration 15/1072, Loss: 8.0916\n",
      "Iteration 16/1072, Loss: 8.1253\n",
      "Iteration 17/1072, Loss: 8.1654\n",
      "Iteration 18/1072, Loss: 8.0731\n",
      "Iteration 19/1072, Loss: 8.0678\n",
      "Iteration 20/1072, Loss: 8.1156\n",
      "Iteration 21/1072, Loss: 8.1596\n",
      "Iteration 22/1072, Loss: 8.0934\n",
      "Iteration 23/1072, Loss: 8.0647\n",
      "Iteration 24/1072, Loss: 8.1457\n",
      "Iteration 25/1072, Loss: 8.0605\n",
      "Iteration 26/1072, Loss: 8.1028\n",
      "Iteration 27/1072, Loss: 8.0116\n",
      "Iteration 28/1072, Loss: 8.1462\n",
      "Iteration 29/1072, Loss: 8.1201\n",
      "Iteration 30/1072, Loss: 8.0332\n",
      "Iteration 31/1072, Loss: 8.1078\n",
      "Iteration 32/1072, Loss: 8.1106\n",
      "Iteration 33/1072, Loss: 7.9436\n",
      "Iteration 34/1072, Loss: 8.0079\n",
      "Iteration 35/1072, Loss: 8.0730\n",
      "Iteration 36/1072, Loss: 8.1049\n",
      "Iteration 37/1072, Loss: 8.0212\n",
      "Iteration 38/1072, Loss: 8.0644\n",
      "Iteration 39/1072, Loss: 8.0519\n",
      "Iteration 40/1072, Loss: 8.0353\n",
      "Iteration 41/1072, Loss: 8.1292\n",
      "Iteration 42/1072, Loss: 7.9937\n",
      "Iteration 43/1072, Loss: 8.1219\n",
      "Iteration 44/1072, Loss: 8.0780\n",
      "Iteration 45/1072, Loss: 8.0431\n",
      "Iteration 46/1072, Loss: 8.0255\n",
      "Iteration 47/1072, Loss: 8.1420\n",
      "Iteration 48/1072, Loss: 8.0682\n",
      "Iteration 49/1072, Loss: 8.0772\n",
      "Iteration 50/1072, Loss: 8.2040\n",
      "Iteration 51/1072, Loss: 8.0743\n",
      "Iteration 52/1072, Loss: 8.1993\n",
      "Iteration 53/1072, Loss: 8.1131\n",
      "Iteration 54/1072, Loss: 8.0315\n",
      "Iteration 55/1072, Loss: 8.0825\n",
      "Iteration 56/1072, Loss: 8.0289\n",
      "Iteration 57/1072, Loss: 8.1411\n",
      "Iteration 58/1072, Loss: 8.0721\n",
      "Iteration 59/1072, Loss: 8.0705\n",
      "Iteration 60/1072, Loss: 8.0607\n",
      "Iteration 61/1072, Loss: 8.1054\n",
      "Iteration 62/1072, Loss: 8.1287\n",
      "Iteration 63/1072, Loss: 8.1305\n",
      "Iteration 64/1072, Loss: 8.0175\n",
      "Iteration 65/1072, Loss: 8.1620\n",
      "Iteration 66/1072, Loss: 8.0398\n",
      "Iteration 67/1072, Loss: 8.0166\n",
      "Iteration 68/1072, Loss: 8.0245\n",
      "Iteration 69/1072, Loss: 8.1303\n",
      "Iteration 70/1072, Loss: 8.1000\n",
      "Iteration 71/1072, Loss: 8.1218\n",
      "Iteration 72/1072, Loss: 8.0894\n",
      "Iteration 73/1072, Loss: 8.1626\n",
      "Iteration 74/1072, Loss: 8.0403\n",
      "Iteration 75/1072, Loss: 8.1725\n",
      "Iteration 76/1072, Loss: 8.1177\n",
      "Iteration 77/1072, Loss: 8.1036\n",
      "Iteration 78/1072, Loss: 8.0120\n",
      "Iteration 79/1072, Loss: 8.1003\n",
      "Iteration 80/1072, Loss: 8.1424\n",
      "Iteration 81/1072, Loss: 8.0010\n",
      "Iteration 82/1072, Loss: 8.1303\n",
      "Iteration 83/1072, Loss: 8.1787\n",
      "Iteration 84/1072, Loss: 8.0236\n",
      "Iteration 85/1072, Loss: 8.0876\n",
      "Iteration 86/1072, Loss: 8.0990\n",
      "Iteration 87/1072, Loss: 8.0548\n",
      "Iteration 88/1072, Loss: 8.0547\n",
      "Iteration 89/1072, Loss: 8.1517\n",
      "Iteration 90/1072, Loss: 7.9978\n",
      "Iteration 91/1072, Loss: 8.0638\n",
      "Iteration 92/1072, Loss: 8.1520\n",
      "Iteration 93/1072, Loss: 8.0733\n",
      "Iteration 94/1072, Loss: 8.0035\n",
      "Iteration 95/1072, Loss: 8.0683\n",
      "Iteration 96/1072, Loss: 8.1011\n",
      "Iteration 97/1072, Loss: 8.0815\n",
      "Iteration 98/1072, Loss: 8.1413\n",
      "Iteration 99/1072, Loss: 8.1234\n",
      "Iteration 100/1072, Loss: 7.9643\n",
      "Iteration 101/1072, Loss: 8.0505\n",
      "Iteration 102/1072, Loss: 8.0935\n",
      "Iteration 103/1072, Loss: 8.0862\n",
      "Iteration 104/1072, Loss: 8.0882\n",
      "Iteration 105/1072, Loss: 8.0537\n",
      "Iteration 106/1072, Loss: 8.1293\n",
      "Iteration 107/1072, Loss: 8.0457\n",
      "Iteration 108/1072, Loss: 8.1018\n",
      "Iteration 109/1072, Loss: 8.0808\n",
      "Iteration 110/1072, Loss: 8.0483\n",
      "Iteration 111/1072, Loss: 8.0893\n",
      "Iteration 112/1072, Loss: 8.1631\n",
      "Iteration 113/1072, Loss: 8.1315\n",
      "Iteration 114/1072, Loss: 8.0747\n",
      "Iteration 115/1072, Loss: 8.0622\n",
      "Iteration 116/1072, Loss: 8.1339\n",
      "Iteration 117/1072, Loss: 8.2117\n",
      "Iteration 118/1072, Loss: 8.0430\n",
      "Iteration 119/1072, Loss: 8.1211\n",
      "Iteration 120/1072, Loss: 8.1473\n",
      "Iteration 121/1072, Loss: 8.1031\n",
      "Iteration 122/1072, Loss: 8.0698\n",
      "Iteration 123/1072, Loss: 8.0786\n",
      "Iteration 124/1072, Loss: 8.0616\n",
      "Iteration 125/1072, Loss: 8.1027\n",
      "Iteration 126/1072, Loss: 8.0764\n",
      "Iteration 127/1072, Loss: 8.1518\n",
      "Iteration 128/1072, Loss: 8.2126\n",
      "Iteration 129/1072, Loss: 8.0694\n",
      "Iteration 130/1072, Loss: 7.9915\n",
      "Iteration 131/1072, Loss: 8.0728\n",
      "Iteration 132/1072, Loss: 8.1772\n",
      "Iteration 133/1072, Loss: 8.1380\n",
      "Iteration 134/1072, Loss: 8.1075\n",
      "Iteration 135/1072, Loss: 8.1251\n",
      "Iteration 136/1072, Loss: 8.0298\n",
      "Iteration 137/1072, Loss: 8.0235\n",
      "Iteration 138/1072, Loss: 8.0005\n",
      "Iteration 139/1072, Loss: 8.0568\n",
      "Iteration 140/1072, Loss: 8.0732\n",
      "Iteration 141/1072, Loss: 8.0764\n",
      "Iteration 142/1072, Loss: 8.1594\n",
      "Iteration 143/1072, Loss: 8.1443\n",
      "Iteration 144/1072, Loss: 8.1975\n",
      "Iteration 145/1072, Loss: 8.1412\n",
      "Iteration 146/1072, Loss: 8.1348\n",
      "Iteration 147/1072, Loss: 8.0743\n",
      "Iteration 148/1072, Loss: 8.1734\n",
      "Iteration 149/1072, Loss: 8.0699\n",
      "Iteration 150/1072, Loss: 8.1589\n",
      "Iteration 151/1072, Loss: 8.0111\n",
      "Iteration 152/1072, Loss: 8.0327\n",
      "Iteration 153/1072, Loss: 8.1626\n",
      "Iteration 154/1072, Loss: 8.0892\n",
      "Iteration 155/1072, Loss: 8.0602\n",
      "Iteration 156/1072, Loss: 8.1263\n",
      "Iteration 157/1072, Loss: 8.0148\n",
      "Iteration 158/1072, Loss: 8.0961\n",
      "Iteration 159/1072, Loss: 8.1630\n",
      "Iteration 160/1072, Loss: 8.1447\n",
      "Iteration 161/1072, Loss: 8.1324\n",
      "Iteration 162/1072, Loss: 8.1129\n",
      "Iteration 163/1072, Loss: 8.0282\n",
      "Iteration 164/1072, Loss: 8.1160\n",
      "Iteration 165/1072, Loss: 8.0282\n",
      "Iteration 166/1072, Loss: 8.2076\n",
      "Iteration 167/1072, Loss: 8.1310\n",
      "Iteration 168/1072, Loss: 8.0014\n",
      "Iteration 169/1072, Loss: 8.1214\n",
      "Iteration 170/1072, Loss: 8.1455\n",
      "Iteration 171/1072, Loss: 8.0834\n",
      "Iteration 172/1072, Loss: 8.1357\n",
      "Iteration 173/1072, Loss: 8.0744\n",
      "Iteration 174/1072, Loss: 8.0862\n",
      "Iteration 175/1072, Loss: 8.0677\n",
      "Iteration 176/1072, Loss: 8.1676\n",
      "Iteration 177/1072, Loss: 8.0085\n",
      "Iteration 178/1072, Loss: 8.0352\n",
      "Iteration 179/1072, Loss: 7.9886\n",
      "Iteration 180/1072, Loss: 8.0644\n",
      "Iteration 181/1072, Loss: 8.1632\n",
      "Iteration 182/1072, Loss: 7.9746\n",
      "Iteration 183/1072, Loss: 8.0106\n",
      "Iteration 184/1072, Loss: 8.1023\n",
      "Iteration 185/1072, Loss: 8.0892\n",
      "Iteration 186/1072, Loss: 8.0414\n",
      "Iteration 187/1072, Loss: 8.1156\n",
      "Iteration 188/1072, Loss: 8.0801\n",
      "Iteration 189/1072, Loss: 8.1190\n",
      "Iteration 190/1072, Loss: 8.0328\n",
      "Iteration 191/1072, Loss: 8.0624\n",
      "Iteration 192/1072, Loss: 8.0750\n",
      "Iteration 193/1072, Loss: 8.0886\n",
      "Iteration 194/1072, Loss: 8.0900\n",
      "Iteration 195/1072, Loss: 8.0612\n",
      "Iteration 196/1072, Loss: 8.0237\n",
      "Iteration 197/1072, Loss: 8.0679\n",
      "Iteration 198/1072, Loss: 8.1571\n",
      "Iteration 199/1072, Loss: 8.0369\n",
      "Iteration 200/1072, Loss: 8.1300\n",
      "Iteration 201/1072, Loss: 8.1080\n",
      "Iteration 202/1072, Loss: 8.1696\n",
      "Iteration 203/1072, Loss: 7.9847\n",
      "Iteration 204/1072, Loss: 8.0954\n",
      "Iteration 205/1072, Loss: 8.0262\n",
      "Iteration 206/1072, Loss: 8.2325\n",
      "Iteration 207/1072, Loss: 8.1155\n",
      "Iteration 208/1072, Loss: 8.1062\n",
      "Iteration 209/1072, Loss: 8.1407\n",
      "Iteration 210/1072, Loss: 8.0410\n",
      "Iteration 211/1072, Loss: 8.0976\n",
      "Iteration 212/1072, Loss: 8.1184\n",
      "Iteration 213/1072, Loss: 8.1664\n",
      "Iteration 214/1072, Loss: 8.0758\n",
      "Iteration 215/1072, Loss: 8.1344\n",
      "Iteration 216/1072, Loss: 8.1004\n",
      "Iteration 217/1072, Loss: 8.0913\n",
      "Iteration 218/1072, Loss: 8.0052\n",
      "Iteration 219/1072, Loss: 8.1507\n",
      "Iteration 220/1072, Loss: 8.1252\n",
      "Iteration 221/1072, Loss: 8.1329\n",
      "Iteration 222/1072, Loss: 8.1311\n",
      "Iteration 223/1072, Loss: 8.2188\n",
      "Iteration 224/1072, Loss: 8.0049\n",
      "Iteration 225/1072, Loss: 8.1839\n",
      "Iteration 226/1072, Loss: 8.1452\n",
      "Iteration 227/1072, Loss: 8.0368\n",
      "Iteration 228/1072, Loss: 8.0646\n",
      "Iteration 229/1072, Loss: 8.0834\n",
      "Iteration 230/1072, Loss: 7.9516\n",
      "Iteration 231/1072, Loss: 8.1675\n",
      "Iteration 232/1072, Loss: 8.0113\n",
      "Iteration 233/1072, Loss: 8.1679\n",
      "Iteration 234/1072, Loss: 8.1441\n",
      "Iteration 235/1072, Loss: 8.0413\n",
      "Iteration 236/1072, Loss: 8.1152\n",
      "Iteration 237/1072, Loss: 8.1178\n",
      "Iteration 238/1072, Loss: 8.1113\n",
      "Iteration 239/1072, Loss: 8.0599\n",
      "Iteration 240/1072, Loss: 8.0549\n",
      "Iteration 241/1072, Loss: 8.1006\n",
      "Iteration 242/1072, Loss: 8.1080\n",
      "Iteration 243/1072, Loss: 8.2168\n",
      "Iteration 244/1072, Loss: 8.2163\n",
      "Iteration 245/1072, Loss: 8.0776\n",
      "Iteration 246/1072, Loss: 8.0613\n",
      "Iteration 247/1072, Loss: 8.0344\n",
      "Iteration 248/1072, Loss: 8.1369\n",
      "Iteration 249/1072, Loss: 8.1265\n",
      "Iteration 250/1072, Loss: 8.0661\n",
      "Iteration 251/1072, Loss: 8.0749\n",
      "Iteration 252/1072, Loss: 8.1283\n",
      "Iteration 253/1072, Loss: 7.9904\n",
      "Iteration 254/1072, Loss: 8.1212\n",
      "Iteration 255/1072, Loss: 8.0759\n",
      "Iteration 256/1072, Loss: 8.0856\n",
      "Iteration 257/1072, Loss: 8.0146\n",
      "Iteration 258/1072, Loss: 8.1100\n",
      "Iteration 259/1072, Loss: 8.0614\n",
      "Iteration 260/1072, Loss: 8.1174\n",
      "Iteration 261/1072, Loss: 8.0909\n",
      "Iteration 262/1072, Loss: 7.9720\n",
      "Iteration 263/1072, Loss: 8.0691\n",
      "Iteration 264/1072, Loss: 8.1048\n",
      "Iteration 265/1072, Loss: 8.1309\n",
      "Iteration 266/1072, Loss: 8.0736\n",
      "Iteration 267/1072, Loss: 8.1593\n",
      "Iteration 268/1072, Loss: 8.0561\n",
      "Iteration 269/1072, Loss: 8.0594\n",
      "Iteration 270/1072, Loss: 8.1216\n",
      "Iteration 271/1072, Loss: 8.0938\n",
      "Iteration 272/1072, Loss: 8.0757\n",
      "Iteration 273/1072, Loss: 8.1071\n",
      "Iteration 274/1072, Loss: 8.1051\n",
      "Iteration 275/1072, Loss: 8.1062\n",
      "Iteration 276/1072, Loss: 8.0582\n",
      "Iteration 277/1072, Loss: 8.0697\n",
      "Iteration 278/1072, Loss: 8.0294\n",
      "Iteration 279/1072, Loss: 8.0186\n",
      "Iteration 280/1072, Loss: 8.1150\n",
      "Iteration 281/1072, Loss: 8.1324\n",
      "Iteration 282/1072, Loss: 7.9774\n",
      "Iteration 283/1072, Loss: 8.2222\n",
      "Iteration 284/1072, Loss: 8.1108\n",
      "Iteration 285/1072, Loss: 8.1168\n",
      "Iteration 286/1072, Loss: 8.1161\n",
      "Iteration 287/1072, Loss: 8.0640\n",
      "Iteration 288/1072, Loss: 8.0050\n",
      "Iteration 289/1072, Loss: 8.0971\n",
      "Iteration 290/1072, Loss: 8.1405\n",
      "Iteration 291/1072, Loss: 8.1341\n",
      "Iteration 292/1072, Loss: 8.1340\n",
      "Iteration 293/1072, Loss: 8.1468\n",
      "Iteration 294/1072, Loss: 8.0810\n",
      "Iteration 295/1072, Loss: 8.1420\n",
      "Iteration 296/1072, Loss: 7.9675\n",
      "Iteration 297/1072, Loss: 8.0709\n",
      "Iteration 298/1072, Loss: 8.0770\n",
      "Iteration 299/1072, Loss: 8.1369\n",
      "Iteration 300/1072, Loss: 8.0509\n",
      "Iteration 301/1072, Loss: 8.0392\n",
      "Iteration 302/1072, Loss: 8.1613\n",
      "Iteration 303/1072, Loss: 8.1527\n",
      "Iteration 304/1072, Loss: 8.0482\n",
      "Iteration 305/1072, Loss: 8.1484\n",
      "Iteration 306/1072, Loss: 8.1442\n",
      "Iteration 307/1072, Loss: 8.0859\n",
      "Iteration 308/1072, Loss: 8.1050\n",
      "Iteration 309/1072, Loss: 8.0921\n",
      "Iteration 310/1072, Loss: 8.0841\n",
      "Iteration 311/1072, Loss: 8.0513\n",
      "Iteration 312/1072, Loss: 8.0164\n",
      "Iteration 313/1072, Loss: 8.1224\n",
      "Iteration 314/1072, Loss: 8.1027\n",
      "Iteration 315/1072, Loss: 8.1053\n",
      "Iteration 316/1072, Loss: 8.1252\n",
      "Iteration 317/1072, Loss: 8.1197\n",
      "Iteration 318/1072, Loss: 8.0451\n",
      "Iteration 319/1072, Loss: 8.2042\n",
      "Iteration 320/1072, Loss: 8.0225\n",
      "Iteration 321/1072, Loss: 8.0568\n",
      "Iteration 322/1072, Loss: 8.0073\n",
      "Iteration 323/1072, Loss: 8.0381\n",
      "Iteration 324/1072, Loss: 8.0601\n",
      "Iteration 325/1072, Loss: 8.0991\n",
      "Iteration 326/1072, Loss: 8.1216\n",
      "Iteration 327/1072, Loss: 8.1254\n",
      "Iteration 328/1072, Loss: 8.0839\n",
      "Iteration 329/1072, Loss: 7.9613\n",
      "Iteration 330/1072, Loss: 8.0590\n",
      "Iteration 331/1072, Loss: 8.0108\n",
      "Iteration 332/1072, Loss: 8.0910\n",
      "Iteration 333/1072, Loss: 8.1101\n",
      "Iteration 334/1072, Loss: 8.0593\n",
      "Iteration 335/1072, Loss: 8.0636\n",
      "Iteration 336/1072, Loss: 8.0342\n",
      "Iteration 337/1072, Loss: 8.1585\n",
      "Iteration 338/1072, Loss: 8.0725\n",
      "Iteration 339/1072, Loss: 8.0608\n",
      "Iteration 340/1072, Loss: 8.0561\n",
      "Iteration 341/1072, Loss: 8.1151\n",
      "Iteration 342/1072, Loss: 8.1469\n",
      "Iteration 343/1072, Loss: 8.1044\n",
      "Iteration 344/1072, Loss: 8.0297\n",
      "Iteration 345/1072, Loss: 8.0045\n",
      "Iteration 346/1072, Loss: 8.1332\n",
      "Iteration 347/1072, Loss: 8.0233\n",
      "Iteration 348/1072, Loss: 8.0320\n",
      "Iteration 349/1072, Loss: 8.0753\n",
      "Iteration 350/1072, Loss: 8.0965\n",
      "Iteration 351/1072, Loss: 8.0760\n",
      "Iteration 352/1072, Loss: 8.0219\n",
      "Iteration 353/1072, Loss: 8.0571\n",
      "Iteration 354/1072, Loss: 8.0677\n",
      "Iteration 355/1072, Loss: 8.0332\n",
      "Iteration 356/1072, Loss: 8.0701\n",
      "Iteration 357/1072, Loss: 8.0518\n",
      "Iteration 358/1072, Loss: 8.0823\n",
      "Iteration 359/1072, Loss: 8.1772\n",
      "Iteration 360/1072, Loss: 8.0547\n",
      "Iteration 361/1072, Loss: 8.1380\n",
      "Iteration 362/1072, Loss: 8.0462\n",
      "Iteration 363/1072, Loss: 8.1288\n",
      "Iteration 364/1072, Loss: 8.1867\n",
      "Iteration 365/1072, Loss: 7.9563\n",
      "Iteration 366/1072, Loss: 8.0657\n",
      "Iteration 367/1072, Loss: 7.9780\n",
      "Iteration 368/1072, Loss: 8.1292\n",
      "Iteration 369/1072, Loss: 8.0840\n",
      "Iteration 370/1072, Loss: 8.0199\n",
      "Iteration 371/1072, Loss: 8.0769\n",
      "Iteration 372/1072, Loss: 8.1455\n",
      "Iteration 373/1072, Loss: 8.0694\n",
      "Iteration 374/1072, Loss: 8.1312\n",
      "Iteration 375/1072, Loss: 8.1195\n",
      "Iteration 376/1072, Loss: 8.0542\n",
      "Iteration 377/1072, Loss: 8.0077\n",
      "Iteration 378/1072, Loss: 8.0793\n",
      "Iteration 379/1072, Loss: 8.0297\n",
      "Iteration 380/1072, Loss: 8.0625\n",
      "Iteration 381/1072, Loss: 8.1289\n",
      "Iteration 382/1072, Loss: 8.0583\n",
      "Iteration 383/1072, Loss: 8.1252\n",
      "Iteration 384/1072, Loss: 8.0695\n",
      "Iteration 385/1072, Loss: 8.0828\n",
      "Iteration 386/1072, Loss: 8.1362\n",
      "Iteration 387/1072, Loss: 8.0410\n",
      "Iteration 388/1072, Loss: 8.1772\n",
      "Iteration 389/1072, Loss: 8.0305\n",
      "Iteration 390/1072, Loss: 8.0870\n",
      "Iteration 391/1072, Loss: 8.1812\n",
      "Iteration 392/1072, Loss: 8.1152\n",
      "Iteration 393/1072, Loss: 8.0769\n",
      "Iteration 394/1072, Loss: 8.0570\n",
      "Iteration 395/1072, Loss: 8.0801\n",
      "Iteration 396/1072, Loss: 8.0786\n",
      "Iteration 397/1072, Loss: 8.1369\n",
      "Iteration 398/1072, Loss: 8.1118\n",
      "Iteration 399/1072, Loss: 7.9604\n",
      "Iteration 400/1072, Loss: 8.0979\n",
      "Iteration 401/1072, Loss: 8.0786\n",
      "Iteration 402/1072, Loss: 7.9944\n",
      "Iteration 403/1072, Loss: 8.1300\n",
      "Iteration 404/1072, Loss: 8.2150\n",
      "Iteration 405/1072, Loss: 8.1042\n",
      "Iteration 406/1072, Loss: 8.0574\n",
      "Iteration 407/1072, Loss: 7.9753\n",
      "Iteration 408/1072, Loss: 8.1099\n",
      "Iteration 409/1072, Loss: 8.0841\n",
      "Iteration 410/1072, Loss: 8.0861\n",
      "Iteration 411/1072, Loss: 8.0840\n",
      "Iteration 412/1072, Loss: 8.0307\n",
      "Iteration 413/1072, Loss: 8.0402\n",
      "Iteration 414/1072, Loss: 8.0709\n",
      "Iteration 415/1072, Loss: 8.1233\n",
      "Iteration 416/1072, Loss: 8.0789\n",
      "Iteration 417/1072, Loss: 8.1438\n",
      "Iteration 418/1072, Loss: 8.1010\n",
      "Iteration 419/1072, Loss: 8.1724\n",
      "Iteration 420/1072, Loss: 7.9923\n",
      "Iteration 421/1072, Loss: 8.0286\n",
      "Iteration 422/1072, Loss: 8.0782\n",
      "Iteration 423/1072, Loss: 7.9556\n",
      "Iteration 424/1072, Loss: 8.0170\n",
      "Iteration 425/1072, Loss: 8.0815\n",
      "Iteration 426/1072, Loss: 8.0244\n",
      "Iteration 427/1072, Loss: 8.1007\n",
      "Iteration 428/1072, Loss: 8.0964\n",
      "Iteration 429/1072, Loss: 7.9374\n",
      "Iteration 430/1072, Loss: 8.0899\n",
      "Iteration 431/1072, Loss: 8.0367\n",
      "Iteration 432/1072, Loss: 8.0384\n",
      "Iteration 433/1072, Loss: 8.0822\n",
      "Iteration 434/1072, Loss: 8.0207\n",
      "Iteration 435/1072, Loss: 8.0062\n",
      "Iteration 436/1072, Loss: 8.0360\n",
      "Iteration 437/1072, Loss: 8.0002\n",
      "Iteration 438/1072, Loss: 8.1395\n",
      "Iteration 439/1072, Loss: 8.1784\n",
      "Iteration 440/1072, Loss: 8.0830\n",
      "Iteration 441/1072, Loss: 8.0611\n",
      "Iteration 442/1072, Loss: 8.1227\n",
      "Iteration 443/1072, Loss: 8.0041\n",
      "Iteration 444/1072, Loss: 8.0727\n",
      "Iteration 445/1072, Loss: 8.1564\n",
      "Iteration 446/1072, Loss: 8.0851\n",
      "Iteration 447/1072, Loss: 8.1065\n",
      "Iteration 448/1072, Loss: 8.1176\n",
      "Iteration 449/1072, Loss: 8.1111\n",
      "Iteration 450/1072, Loss: 8.1694\n",
      "Iteration 451/1072, Loss: 8.1087\n",
      "Iteration 452/1072, Loss: 8.0348\n",
      "Iteration 453/1072, Loss: 8.1111\n",
      "Iteration 454/1072, Loss: 8.1531\n",
      "Iteration 455/1072, Loss: 8.0822\n",
      "Iteration 456/1072, Loss: 8.1166\n",
      "Iteration 457/1072, Loss: 8.0808\n",
      "Iteration 458/1072, Loss: 8.0108\n",
      "Iteration 459/1072, Loss: 8.1268\n",
      "Iteration 460/1072, Loss: 7.9916\n",
      "Iteration 461/1072, Loss: 7.9778\n",
      "Iteration 462/1072, Loss: 8.0988\n",
      "Iteration 463/1072, Loss: 8.0744\n",
      "Iteration 464/1072, Loss: 8.0092\n",
      "Iteration 465/1072, Loss: 8.0779\n",
      "Iteration 466/1072, Loss: 7.9684\n",
      "Iteration 467/1072, Loss: 8.0745\n",
      "Iteration 468/1072, Loss: 8.1009\n",
      "Iteration 469/1072, Loss: 8.1024\n",
      "Iteration 470/1072, Loss: 8.1429\n",
      "Iteration 471/1072, Loss: 8.1376\n",
      "Iteration 472/1072, Loss: 8.1165\n",
      "Iteration 473/1072, Loss: 8.0283\n",
      "Iteration 474/1072, Loss: 8.1879\n",
      "Iteration 475/1072, Loss: 8.0305\n",
      "Iteration 476/1072, Loss: 8.1518\n",
      "Iteration 477/1072, Loss: 8.0627\n",
      "Iteration 478/1072, Loss: 8.0590\n",
      "Iteration 479/1072, Loss: 8.0826\n",
      "Iteration 480/1072, Loss: 8.1058\n",
      "Iteration 481/1072, Loss: 8.1164\n",
      "Iteration 482/1072, Loss: 8.0750\n",
      "Iteration 483/1072, Loss: 8.0519\n",
      "Iteration 484/1072, Loss: 8.1117\n",
      "Iteration 485/1072, Loss: 8.0101\n",
      "Iteration 486/1072, Loss: 8.0354\n",
      "Iteration 487/1072, Loss: 8.0613\n",
      "Iteration 488/1072, Loss: 8.1316\n",
      "Iteration 489/1072, Loss: 8.1119\n",
      "Iteration 490/1072, Loss: 7.9593\n",
      "Iteration 491/1072, Loss: 8.0778\n",
      "Iteration 492/1072, Loss: 8.0359\n",
      "Iteration 493/1072, Loss: 8.0437\n",
      "Iteration 494/1072, Loss: 8.0910\n",
      "Iteration 495/1072, Loss: 8.0813\n",
      "Iteration 496/1072, Loss: 8.0313\n",
      "Iteration 497/1072, Loss: 8.1706\n",
      "Iteration 498/1072, Loss: 8.0576\n",
      "Iteration 499/1072, Loss: 8.0497\n",
      "Iteration 500/1072, Loss: 8.0304\n",
      "Iteration 501/1072, Loss: 8.0975\n",
      "Iteration 502/1072, Loss: 8.1287\n",
      "Iteration 503/1072, Loss: 8.0412\n",
      "Iteration 504/1072, Loss: 8.0567\n",
      "Iteration 505/1072, Loss: 8.1609\n",
      "Iteration 506/1072, Loss: 8.0212\n",
      "Iteration 507/1072, Loss: 8.0551\n",
      "Iteration 508/1072, Loss: 8.0846\n",
      "Iteration 509/1072, Loss: 8.0580\n",
      "Iteration 510/1072, Loss: 8.1610\n",
      "Iteration 511/1072, Loss: 8.0734\n",
      "Iteration 512/1072, Loss: 7.9904\n",
      "Iteration 513/1072, Loss: 8.0553\n",
      "Iteration 514/1072, Loss: 8.1163\n",
      "Iteration 515/1072, Loss: 8.0206\n",
      "Iteration 516/1072, Loss: 8.0364\n",
      "Iteration 517/1072, Loss: 8.0911\n",
      "Iteration 518/1072, Loss: 8.0811\n",
      "Iteration 519/1072, Loss: 8.0860\n",
      "Iteration 520/1072, Loss: 8.0292\n",
      "Iteration 521/1072, Loss: 8.0922\n",
      "Iteration 522/1072, Loss: 8.1343\n",
      "Iteration 523/1072, Loss: 8.0778\n",
      "Iteration 524/1072, Loss: 8.0683\n",
      "Iteration 525/1072, Loss: 8.0814\n",
      "Iteration 526/1072, Loss: 8.0950\n",
      "Iteration 527/1072, Loss: 8.1284\n",
      "Iteration 528/1072, Loss: 8.0128\n",
      "Iteration 529/1072, Loss: 7.9872\n",
      "Iteration 530/1072, Loss: 8.0841\n",
      "Iteration 531/1072, Loss: 8.0812\n",
      "Iteration 532/1072, Loss: 7.9831\n",
      "Iteration 533/1072, Loss: 8.0403\n",
      "Iteration 534/1072, Loss: 8.0879\n",
      "Iteration 535/1072, Loss: 8.0275\n",
      "Iteration 536/1072, Loss: 8.0925\n",
      "Iteration 537/1072, Loss: 8.1298\n",
      "Iteration 538/1072, Loss: 8.0174\n",
      "Iteration 539/1072, Loss: 8.1113\n",
      "Iteration 540/1072, Loss: 8.0878\n",
      "Iteration 541/1072, Loss: 8.0121\n",
      "Iteration 542/1072, Loss: 8.0601\n",
      "Iteration 543/1072, Loss: 8.0704\n",
      "Iteration 544/1072, Loss: 8.0927\n",
      "Iteration 545/1072, Loss: 7.9962\n",
      "Iteration 546/1072, Loss: 8.0589\n",
      "Iteration 547/1072, Loss: 8.1543\n",
      "Iteration 548/1072, Loss: 8.0899\n",
      "Iteration 549/1072, Loss: 8.0143\n",
      "Iteration 550/1072, Loss: 8.0950\n",
      "Iteration 551/1072, Loss: 8.0990\n",
      "Iteration 552/1072, Loss: 8.0716\n",
      "Iteration 553/1072, Loss: 7.9980\n",
      "Iteration 554/1072, Loss: 8.0647\n",
      "Iteration 555/1072, Loss: 8.1532\n",
      "Iteration 556/1072, Loss: 8.0309\n",
      "Iteration 557/1072, Loss: 8.1096\n",
      "Iteration 558/1072, Loss: 8.0219\n",
      "Iteration 559/1072, Loss: 8.0802\n",
      "Iteration 560/1072, Loss: 8.1034\n",
      "Iteration 561/1072, Loss: 8.1218\n",
      "Iteration 562/1072, Loss: 8.0970\n",
      "Iteration 563/1072, Loss: 8.0747\n",
      "Iteration 564/1072, Loss: 8.0664\n",
      "Iteration 565/1072, Loss: 7.9932\n",
      "Iteration 566/1072, Loss: 8.0146\n",
      "Iteration 567/1072, Loss: 7.9433\n",
      "Iteration 568/1072, Loss: 8.0286\n",
      "Iteration 569/1072, Loss: 8.0934\n",
      "Iteration 570/1072, Loss: 7.9683\n",
      "Iteration 571/1072, Loss: 8.0544\n",
      "Iteration 572/1072, Loss: 8.0450\n",
      "Iteration 573/1072, Loss: 8.1213\n",
      "Iteration 574/1072, Loss: 8.1256\n",
      "Iteration 575/1072, Loss: 8.0368\n",
      "Iteration 576/1072, Loss: 7.9588\n",
      "Iteration 577/1072, Loss: 8.0655\n",
      "Iteration 578/1072, Loss: 8.1036\n",
      "Iteration 579/1072, Loss: 8.0854\n",
      "Iteration 580/1072, Loss: 8.0406\n",
      "Iteration 581/1072, Loss: 8.0893\n",
      "Iteration 582/1072, Loss: 8.0504\n",
      "Iteration 583/1072, Loss: 8.0813\n",
      "Iteration 584/1072, Loss: 7.9753\n",
      "Iteration 585/1072, Loss: 8.0469\n",
      "Iteration 586/1072, Loss: 8.1057\n",
      "Iteration 587/1072, Loss: 7.9817\n",
      "Iteration 588/1072, Loss: 8.0847\n",
      "Iteration 589/1072, Loss: 8.0732\n",
      "Iteration 590/1072, Loss: 8.1747\n",
      "Iteration 591/1072, Loss: 8.0894\n",
      "Iteration 592/1072, Loss: 8.1378\n",
      "Iteration 593/1072, Loss: 8.0172\n",
      "Iteration 594/1072, Loss: 8.0570\n",
      "Iteration 595/1072, Loss: 8.0707\n",
      "Iteration 596/1072, Loss: 7.9923\n",
      "Iteration 597/1072, Loss: 8.1169\n",
      "Iteration 598/1072, Loss: 8.0325\n",
      "Iteration 599/1072, Loss: 8.0129\n",
      "Iteration 600/1072, Loss: 8.1381\n",
      "Iteration 601/1072, Loss: 8.0495\n",
      "Iteration 602/1072, Loss: 8.0789\n",
      "Iteration 603/1072, Loss: 8.0073\n",
      "Iteration 604/1072, Loss: 8.0108\n",
      "Iteration 605/1072, Loss: 8.0728\n",
      "Iteration 606/1072, Loss: 8.0500\n",
      "Iteration 607/1072, Loss: 8.0775\n",
      "Iteration 608/1072, Loss: 8.0641\n",
      "Iteration 609/1072, Loss: 8.0575\n",
      "Iteration 610/1072, Loss: 8.0129\n",
      "Iteration 611/1072, Loss: 8.0224\n",
      "Iteration 612/1072, Loss: 8.1079\n",
      "Iteration 613/1072, Loss: 8.1386\n",
      "Iteration 614/1072, Loss: 8.0308\n",
      "Iteration 615/1072, Loss: 8.0893\n",
      "Iteration 616/1072, Loss: 8.1251\n",
      "Iteration 617/1072, Loss: 8.0714\n",
      "Iteration 618/1072, Loss: 8.0914\n",
      "Iteration 619/1072, Loss: 8.0315\n",
      "Iteration 620/1072, Loss: 8.1622\n",
      "Iteration 621/1072, Loss: 8.0507\n",
      "Iteration 622/1072, Loss: 8.0504\n",
      "Iteration 623/1072, Loss: 8.1524\n",
      "Iteration 624/1072, Loss: 8.0728\n",
      "Iteration 625/1072, Loss: 8.0125\n",
      "Iteration 626/1072, Loss: 8.0032\n",
      "Iteration 627/1072, Loss: 8.0680\n",
      "Iteration 628/1072, Loss: 8.0751\n",
      "Iteration 629/1072, Loss: 8.0577\n",
      "Iteration 630/1072, Loss: 8.1747\n",
      "Iteration 631/1072, Loss: 8.0660\n",
      "Iteration 632/1072, Loss: 8.0607\n",
      "Iteration 633/1072, Loss: 8.0126\n",
      "Iteration 634/1072, Loss: 8.1298\n",
      "Iteration 635/1072, Loss: 8.1103\n",
      "Iteration 636/1072, Loss: 8.1392\n",
      "Iteration 637/1072, Loss: 8.1201\n",
      "Iteration 638/1072, Loss: 8.0322\n",
      "Iteration 639/1072, Loss: 8.1335\n",
      "Iteration 640/1072, Loss: 8.0673\n",
      "Iteration 641/1072, Loss: 8.0633\n",
      "Iteration 642/1072, Loss: 8.0156\n",
      "Iteration 643/1072, Loss: 8.0617\n",
      "Iteration 644/1072, Loss: 8.1260\n",
      "Iteration 645/1072, Loss: 8.0343\n",
      "Iteration 646/1072, Loss: 8.0825\n",
      "Iteration 647/1072, Loss: 8.0952\n",
      "Iteration 648/1072, Loss: 8.0418\n",
      "Iteration 649/1072, Loss: 7.9864\n",
      "Iteration 650/1072, Loss: 8.0348\n",
      "Iteration 651/1072, Loss: 8.1025\n",
      "Iteration 652/1072, Loss: 7.9808\n",
      "Iteration 653/1072, Loss: 8.1245\n",
      "Iteration 654/1072, Loss: 8.0766\n",
      "Iteration 655/1072, Loss: 8.0341\n",
      "Iteration 656/1072, Loss: 8.0054\n",
      "Iteration 657/1072, Loss: 8.0904\n",
      "Iteration 658/1072, Loss: 8.1066\n",
      "Iteration 659/1072, Loss: 8.0675\n",
      "Iteration 660/1072, Loss: 8.0684\n",
      "Iteration 661/1072, Loss: 8.0384\n",
      "Iteration 662/1072, Loss: 8.0461\n",
      "Iteration 663/1072, Loss: 8.0268\n",
      "Iteration 664/1072, Loss: 8.0527\n",
      "Iteration 665/1072, Loss: 8.0593\n",
      "Iteration 666/1072, Loss: 8.1285\n",
      "Iteration 667/1072, Loss: 8.0772\n",
      "Iteration 711/1072, Loss: 8.0375\n",
      "Iteration 712/1072, Loss: 8.0628\n",
      "Iteration 713/1072, Loss: 8.0966\n",
      "Iteration 714/1072, Loss: 8.1244\n",
      "Iteration 715/1072, Loss: 8.0251\n",
      "Iteration 716/1072, Loss: 8.0660\n",
      "Iteration 717/1072, Loss: 8.0236\n",
      "Iteration 718/1072, Loss: 8.0178\n",
      "Iteration 719/1072, Loss: 8.0413\n",
      "Iteration 720/1072, Loss: 8.0241\n",
      "Iteration 721/1072, Loss: 8.0251\n",
      "Iteration 722/1072, Loss: 8.0411\n",
      "Iteration 723/1072, Loss: 8.0526\n",
      "Iteration 724/1072, Loss: 8.0745\n",
      "Iteration 725/1072, Loss: 8.0492\n",
      "Iteration 726/1072, Loss: 8.0289\n",
      "Iteration 727/1072, Loss: 7.9809\n",
      "Iteration 728/1072, Loss: 8.0972\n",
      "Iteration 729/1072, Loss: 8.1560\n",
      "Iteration 730/1072, Loss: 8.1606\n",
      "Iteration 731/1072, Loss: 8.0778\n",
      "Iteration 732/1072, Loss: 8.1193\n",
      "Iteration 733/1072, Loss: 8.0621\n",
      "Iteration 734/1072, Loss: 7.9311\n",
      "Iteration 735/1072, Loss: 8.0749\n",
      "Iteration 736/1072, Loss: 8.0754\n",
      "Iteration 737/1072, Loss: 8.0175\n",
      "Iteration 738/1072, Loss: 7.9906\n",
      "Iteration 739/1072, Loss: 8.0234\n",
      "Iteration 740/1072, Loss: 8.0054\n",
      "Iteration 741/1072, Loss: 8.0168\n",
      "Iteration 742/1072, Loss: 8.0577\n",
      "Iteration 743/1072, Loss: 8.1740\n",
      "Iteration 744/1072, Loss: 8.0926\n",
      "Iteration 745/1072, Loss: 8.1132\n",
      "Iteration 746/1072, Loss: 8.0515\n",
      "Iteration 747/1072, Loss: 7.9228\n",
      "Iteration 748/1072, Loss: 8.1069\n",
      "Iteration 749/1072, Loss: 8.0590\n",
      "Iteration 750/1072, Loss: 8.1731\n",
      "Iteration 751/1072, Loss: 8.0626\n",
      "Iteration 752/1072, Loss: 8.1384\n",
      "Iteration 753/1072, Loss: 8.0957\n",
      "Iteration 754/1072, Loss: 7.9906\n",
      "Iteration 755/1072, Loss: 8.0647\n",
      "Iteration 756/1072, Loss: 8.0415\n",
      "Iteration 757/1072, Loss: 8.0844\n",
      "Iteration 758/1072, Loss: 8.0530\n",
      "Iteration 759/1072, Loss: 8.0574\n",
      "Iteration 760/1072, Loss: 8.0529\n",
      "Iteration 761/1072, Loss: 8.0054\n",
      "Iteration 762/1072, Loss: 8.0953\n",
      "Iteration 763/1072, Loss: 8.0896\n",
      "Iteration 764/1072, Loss: 8.0573\n",
      "Iteration 765/1072, Loss: 8.0510\n",
      "Iteration 766/1072, Loss: 8.0073\n",
      "Iteration 767/1072, Loss: 8.0316\n",
      "Iteration 768/1072, Loss: 8.0350\n",
      "Iteration 769/1072, Loss: 8.0639\n",
      "Iteration 770/1072, Loss: 8.0758\n",
      "Iteration 771/1072, Loss: 8.0067\n",
      "Iteration 772/1072, Loss: 8.0340\n",
      "Iteration 773/1072, Loss: 8.0753\n",
      "Iteration 774/1072, Loss: 8.0521\n",
      "Iteration 775/1072, Loss: 7.9772\n",
      "Iteration 776/1072, Loss: 8.0263\n",
      "Iteration 777/1072, Loss: 8.0630\n",
      "Iteration 778/1072, Loss: 8.0796\n",
      "Iteration 779/1072, Loss: 8.0317\n",
      "Iteration 780/1072, Loss: 8.0473\n",
      "Iteration 781/1072, Loss: 8.0680\n",
      "Iteration 782/1072, Loss: 8.0459\n",
      "Iteration 783/1072, Loss: 8.0269\n",
      "Iteration 784/1072, Loss: 8.1346\n",
      "Iteration 785/1072, Loss: 8.0005\n",
      "Iteration 786/1072, Loss: 8.0113\n",
      "Iteration 787/1072, Loss: 8.1122\n",
      "Iteration 788/1072, Loss: 8.0180\n",
      "Iteration 789/1072, Loss: 8.1039\n",
      "Iteration 790/1072, Loss: 7.9959\n",
      "Iteration 791/1072, Loss: 8.0617\n",
      "Iteration 792/1072, Loss: 8.0712\n",
      "Iteration 793/1072, Loss: 8.1771\n",
      "Iteration 794/1072, Loss: 8.0133\n",
      "Iteration 795/1072, Loss: 7.9864\n",
      "Iteration 796/1072, Loss: 7.9927\n",
      "Iteration 797/1072, Loss: 8.0090\n",
      "Iteration 798/1072, Loss: 8.0236\n",
      "Iteration 799/1072, Loss: 8.0058\n",
      "Iteration 800/1072, Loss: 7.9950\n",
      "Iteration 801/1072, Loss: 8.0648\n",
      "Iteration 802/1072, Loss: 8.0814\n",
      "Iteration 803/1072, Loss: 8.0601\n",
      "Iteration 804/1072, Loss: 8.0680\n",
      "Iteration 805/1072, Loss: 8.0692\n",
      "Iteration 806/1072, Loss: 8.0072\n",
      "Iteration 807/1072, Loss: 8.0473\n",
      "Iteration 808/1072, Loss: 8.1310\n",
      "Iteration 809/1072, Loss: 7.9887\n",
      "Iteration 810/1072, Loss: 8.0185\n",
      "Iteration 811/1072, Loss: 8.0729\n",
      "Iteration 812/1072, Loss: 8.0601\n",
      "Iteration 813/1072, Loss: 8.0184\n",
      "Iteration 814/1072, Loss: 7.9999\n",
      "Iteration 815/1072, Loss: 8.0579\n",
      "Iteration 816/1072, Loss: 8.0697\n",
      "Iteration 817/1072, Loss: 7.9809\n",
      "Iteration 818/1072, Loss: 7.9597\n",
      "Iteration 819/1072, Loss: 8.0365\n",
      "Iteration 820/1072, Loss: 8.0274\n",
      "Iteration 821/1072, Loss: 8.0209\n",
      "Iteration 822/1072, Loss: 8.0144\n",
      "Iteration 823/1072, Loss: 8.0846\n",
      "Iteration 824/1072, Loss: 8.0608\n",
      "Iteration 825/1072, Loss: 8.0906\n",
      "Iteration 826/1072, Loss: 8.0844\n",
      "Iteration 827/1072, Loss: 8.0581\n",
      "Iteration 828/1072, Loss: 8.0580\n",
      "Iteration 829/1072, Loss: 8.0271\n",
      "Iteration 830/1072, Loss: 8.0099\n",
      "Iteration 831/1072, Loss: 8.0205\n",
      "Iteration 832/1072, Loss: 8.0658\n",
      "Iteration 833/1072, Loss: 8.0182\n",
      "Iteration 834/1072, Loss: 8.0738\n",
      "Iteration 835/1072, Loss: 8.0961\n",
      "Iteration 836/1072, Loss: 7.9734\n",
      "Iteration 837/1072, Loss: 7.9865\n",
      "Iteration 838/1072, Loss: 8.0756\n",
      "Iteration 839/1072, Loss: 8.0714\n",
      "Iteration 840/1072, Loss: 7.9958\n",
      "Iteration 841/1072, Loss: 8.1358\n",
      "Iteration 842/1072, Loss: 8.0979\n",
      "Iteration 843/1072, Loss: 7.9781\n",
      "Iteration 844/1072, Loss: 8.0228\n",
      "Iteration 845/1072, Loss: 8.0231\n",
      "Iteration 846/1072, Loss: 8.0330\n",
      "Iteration 847/1072, Loss: 8.0656\n",
      "Iteration 848/1072, Loss: 8.0160\n",
      "Iteration 849/1072, Loss: 8.0385\n",
      "Iteration 850/1072, Loss: 8.0314\n",
      "Iteration 851/1072, Loss: 8.0440\n",
      "Iteration 852/1072, Loss: 8.0842\n",
      "Iteration 853/1072, Loss: 8.0232\n",
      "Iteration 854/1072, Loss: 7.9891\n",
      "Iteration 855/1072, Loss: 8.0021\n",
      "Iteration 856/1072, Loss: 8.0396\n",
      "Iteration 857/1072, Loss: 7.9700\n",
      "Iteration 858/1072, Loss: 8.0738\n",
      "Iteration 859/1072, Loss: 8.0813\n",
      "Iteration 860/1072, Loss: 8.1288\n",
      "Iteration 861/1072, Loss: 8.0160\n",
      "Iteration 862/1072, Loss: 8.0592\n",
      "Iteration 863/1072, Loss: 8.0966\n",
      "Iteration 864/1072, Loss: 8.1022\n",
      "Iteration 865/1072, Loss: 8.1321\n",
      "Iteration 866/1072, Loss: 7.9988\n",
      "Iteration 867/1072, Loss: 8.0266\n",
      "Iteration 868/1072, Loss: 7.9318\n",
      "Iteration 869/1072, Loss: 7.9294\n",
      "Iteration 870/1072, Loss: 8.1116\n",
      "Iteration 871/1072, Loss: 8.0123\n",
      "Iteration 872/1072, Loss: 8.0693\n",
      "Iteration 873/1072, Loss: 7.9894\n",
      "Iteration 874/1072, Loss: 7.9915\n",
      "Iteration 875/1072, Loss: 8.0371\n",
      "Iteration 876/1072, Loss: 7.9349\n",
      "Iteration 877/1072, Loss: 8.0322\n",
      "Iteration 878/1072, Loss: 8.0233\n",
      "Iteration 879/1072, Loss: 8.0409\n",
      "Iteration 880/1072, Loss: 8.0128\n",
      "Iteration 881/1072, Loss: 7.9580\n",
      "Iteration 882/1072, Loss: 8.0585\n",
      "Iteration 883/1072, Loss: 8.1013\n",
      "Iteration 884/1072, Loss: 8.0577\n",
      "Iteration 885/1072, Loss: 8.0215\n",
      "Iteration 886/1072, Loss: 8.0496\n",
      "Iteration 887/1072, Loss: 8.0538\n",
      "Iteration 888/1072, Loss: 7.9963\n",
      "Iteration 889/1072, Loss: 8.0084\n",
      "Iteration 890/1072, Loss: 8.0208\n",
      "Iteration 891/1072, Loss: 8.0456\n",
      "Iteration 892/1072, Loss: 8.0202\n",
      "Iteration 893/1072, Loss: 8.0873\n",
      "Iteration 894/1072, Loss: 8.0446\n",
      "Iteration 895/1072, Loss: 8.1007\n",
      "Iteration 896/1072, Loss: 8.0570\n",
      "Iteration 897/1072, Loss: 8.0580\n",
      "Iteration 898/1072, Loss: 8.0529\n",
      "Iteration 899/1072, Loss: 8.1119\n",
      "Iteration 900/1072, Loss: 8.0650\n",
      "Iteration 901/1072, Loss: 8.0065\n",
      "Iteration 902/1072, Loss: 8.0638\n",
      "Iteration 903/1072, Loss: 8.0477\n",
      "Iteration 904/1072, Loss: 7.9970\n",
      "Iteration 905/1072, Loss: 8.0515\n",
      "Iteration 906/1072, Loss: 8.0144\n",
      "Iteration 907/1072, Loss: 8.0117\n",
      "Iteration 908/1072, Loss: 7.9254\n",
      "Iteration 909/1072, Loss: 8.0260\n",
      "Iteration 910/1072, Loss: 8.0277\n",
      "Iteration 911/1072, Loss: 7.9537\n",
      "Iteration 912/1072, Loss: 8.0172\n",
      "Iteration 913/1072, Loss: 8.0058\n",
      "Iteration 914/1072, Loss: 8.0502\n",
      "Iteration 915/1072, Loss: 8.0448\n",
      "Iteration 916/1072, Loss: 8.0249\n",
      "Iteration 917/1072, Loss: 8.1018\n",
      "Iteration 918/1072, Loss: 8.0695\n",
      "Iteration 919/1072, Loss: 7.9195\n",
      "Iteration 920/1072, Loss: 8.0802\n",
      "Iteration 921/1072, Loss: 8.0500\n",
      "Iteration 922/1072, Loss: 7.9803\n",
      "Iteration 923/1072, Loss: 7.9605\n",
      "Iteration 924/1072, Loss: 8.0390\n",
      "Iteration 925/1072, Loss: 8.0569\n",
      "Iteration 926/1072, Loss: 8.0648\n",
      "Iteration 927/1072, Loss: 8.0856\n",
      "Iteration 928/1072, Loss: 7.9968\n",
      "Iteration 929/1072, Loss: 8.1510\n",
      "Iteration 930/1072, Loss: 8.0752\n",
      "Iteration 931/1072, Loss: 8.0496\n",
      "Iteration 932/1072, Loss: 8.0288\n",
      "Iteration 933/1072, Loss: 8.0391\n",
      "Iteration 934/1072, Loss: 8.0204\n",
      "Iteration 935/1072, Loss: 7.9947\n",
      "Iteration 936/1072, Loss: 8.0542\n",
      "Iteration 937/1072, Loss: 7.9524\n",
      "Iteration 938/1072, Loss: 8.0811\n",
      "Iteration 939/1072, Loss: 8.0482\n",
      "Iteration 940/1072, Loss: 8.0221\n",
      "Iteration 941/1072, Loss: 8.0006\n",
      "Iteration 942/1072, Loss: 8.0423\n",
      "Iteration 943/1072, Loss: 8.0286\n",
      "Iteration 944/1072, Loss: 8.0106\n",
      "Iteration 945/1072, Loss: 8.0843\n",
      "Iteration 946/1072, Loss: 8.0619\n",
      "Iteration 947/1072, Loss: 7.9526\n",
      "Iteration 948/1072, Loss: 8.0281\n",
      "Iteration 949/1072, Loss: 8.0761\n",
      "Iteration 950/1072, Loss: 8.0241\n",
      "Iteration 951/1072, Loss: 8.0054\n",
      "Iteration 952/1072, Loss: 7.9868\n",
      "Iteration 953/1072, Loss: 7.9717\n",
      "Iteration 954/1072, Loss: 8.0292\n",
      "Iteration 955/1072, Loss: 8.0683\n",
      "Iteration 956/1072, Loss: 8.0431\n",
      "Iteration 957/1072, Loss: 7.9966\n",
      "Iteration 958/1072, Loss: 8.0964\n",
      "Iteration 959/1072, Loss: 8.0141\n",
      "Iteration 960/1072, Loss: 8.0695\n",
      "Iteration 961/1072, Loss: 7.9887\n",
      "Iteration 962/1072, Loss: 8.0608\n",
      "Iteration 963/1072, Loss: 7.9564\n",
      "Iteration 964/1072, Loss: 8.0195\n",
      "Iteration 965/1072, Loss: 8.0199\n",
      "Iteration 966/1072, Loss: 8.0301\n",
      "Iteration 967/1072, Loss: 8.0579\n",
      "Iteration 968/1072, Loss: 8.0432\n",
      "Iteration 969/1072, Loss: 8.0809\n",
      "Iteration 970/1072, Loss: 8.0310\n",
      "Iteration 971/1072, Loss: 7.9737\n",
      "Iteration 972/1072, Loss: 8.0209\n",
      "Iteration 973/1072, Loss: 7.9673\n",
      "Iteration 974/1072, Loss: 8.0275\n",
      "Iteration 975/1072, Loss: 8.0224\n",
      "Iteration 976/1072, Loss: 8.0535\n",
      "Iteration 977/1072, Loss: 8.0617\n",
      "Iteration 978/1072, Loss: 8.0086\n",
      "Iteration 979/1072, Loss: 8.1267\n",
      "Iteration 980/1072, Loss: 8.0382\n",
      "Iteration 981/1072, Loss: 7.9736\n",
      "Iteration 982/1072, Loss: 8.0278\n",
      "Iteration 983/1072, Loss: 7.9632\n",
      "Iteration 984/1072, Loss: 8.0241\n",
      "Iteration 985/1072, Loss: 7.9350\n",
      "Iteration 986/1072, Loss: 7.9146\n",
      "Iteration 987/1072, Loss: 7.9568\n",
      "Iteration 988/1072, Loss: 7.9798\n",
      "Iteration 989/1072, Loss: 8.0049\n",
      "Iteration 990/1072, Loss: 8.0230\n",
      "Iteration 991/1072, Loss: 8.0609\n",
      "Iteration 992/1072, Loss: 8.0303\n",
      "Iteration 993/1072, Loss: 8.0383\n",
      "Iteration 994/1072, Loss: 8.0368\n",
      "Iteration 995/1072, Loss: 8.0379\n",
      "Iteration 996/1072, Loss: 8.0166\n",
      "Iteration 997/1072, Loss: 8.0469\n",
      "Iteration 998/1072, Loss: 7.9590\n",
      "Iteration 999/1072, Loss: 7.9946\n",
      "Iteration 1000/1072, Loss: 8.0951\n",
      "Iteration 1001/1072, Loss: 7.9809\n",
      "Iteration 1002/1072, Loss: 8.0006\n",
      "Iteration 1003/1072, Loss: 8.0148\n",
      "Iteration 1004/1072, Loss: 7.9705\n",
      "Iteration 1005/1072, Loss: 8.0602\n",
      "Iteration 1006/1072, Loss: 7.9536\n",
      "Iteration 1007/1072, Loss: 8.0421\n",
      "Iteration 1008/1072, Loss: 8.0444\n",
      "Iteration 1009/1072, Loss: 7.9824\n",
      "Iteration 1010/1072, Loss: 8.0347\n",
      "Iteration 1011/1072, Loss: 8.0402\n",
      "Iteration 1012/1072, Loss: 8.1131\n",
      "Iteration 1013/1072, Loss: 8.0528\n",
      "Iteration 1014/1072, Loss: 7.9828\n",
      "Iteration 1015/1072, Loss: 8.0763\n",
      "Iteration 1016/1072, Loss: 8.0270\n",
      "Iteration 1017/1072, Loss: 7.9872\n",
      "Iteration 1018/1072, Loss: 7.9507\n",
      "Iteration 1019/1072, Loss: 8.0697\n",
      "Iteration 1020/1072, Loss: 7.9769\n",
      "Iteration 1021/1072, Loss: 8.0331\n",
      "Iteration 1022/1072, Loss: 7.9526\n",
      "Iteration 1023/1072, Loss: 7.9834\n",
      "Iteration 1024/1072, Loss: 8.0052\n",
      "Iteration 1025/1072, Loss: 8.0653\n",
      "Iteration 1026/1072, Loss: 7.8952\n",
      "Iteration 1027/1072, Loss: 8.0132\n",
      "Iteration 1028/1072, Loss: 8.0189\n",
      "Iteration 1029/1072, Loss: 8.0383\n",
      "Iteration 1030/1072, Loss: 7.9990\n",
      "Iteration 1031/1072, Loss: 7.9693\n",
      "Iteration 1032/1072, Loss: 8.0285\n",
      "Iteration 1033/1072, Loss: 8.0005\n",
      "Iteration 1034/1072, Loss: 7.9501\n",
      "Iteration 1035/1072, Loss: 7.9453\n",
      "Iteration 1036/1072, Loss: 8.0449\n",
      "Iteration 1037/1072, Loss: 8.0331\n",
      "Iteration 1038/1072, Loss: 8.0228\n",
      "Iteration 1039/1072, Loss: 8.0211\n",
      "Iteration 1040/1072, Loss: 8.0178\n",
      "Iteration 1041/1072, Loss: 7.9731\n",
      "Iteration 1042/1072, Loss: 7.9795\n",
      "Iteration 1043/1072, Loss: 8.0167\n",
      "Iteration 1044/1072, Loss: 7.9409\n",
      "Iteration 1045/1072, Loss: 7.9605\n",
      "Iteration 1046/1072, Loss: 7.9962\n",
      "Iteration 1047/1072, Loss: 8.0145\n",
      "Iteration 1048/1072, Loss: 8.0578\n",
      "Iteration 1049/1072, Loss: 8.0358\n",
      "Iteration 1050/1072, Loss: 8.0191\n",
      "Iteration 1051/1072, Loss: 8.0593\n",
      "Iteration 1052/1072, Loss: 8.0012\n",
      "Iteration 1053/1072, Loss: 8.0637\n",
      "Iteration 1054/1072, Loss: 8.0312\n",
      "Iteration 1055/1072, Loss: 7.9371\n",
      "Iteration 1056/1072, Loss: 8.0211\n",
      "Iteration 1057/1072, Loss: 7.9840\n",
      "Iteration 1058/1072, Loss: 7.9973\n",
      "Iteration 1059/1072, Loss: 8.0414\n",
      "Iteration 1060/1072, Loss: 8.1140\n",
      "Iteration 1061/1072, Loss: 7.9987\n",
      "Iteration 1062/1072, Loss: 8.0166\n",
      "Iteration 1063/1072, Loss: 8.0070\n",
      "Iteration 1064/1072, Loss: 8.0408\n",
      "Iteration 1065/1072, Loss: 8.0506\n",
      "Iteration 1066/1072, Loss: 7.9924\n",
      "Iteration 1067/1072, Loss: 8.0458\n",
      "Iteration 1068/1072, Loss: 8.0427\n",
      "Iteration 1069/1072, Loss: 7.9873\n",
      "Iteration 1070/1072, Loss: 8.0651\n",
      "Iteration 1071/1072, Loss: 8.0247\n",
      "Iteration 1072/1072, Loss: 8.0049\n",
      "Epoch 1/10, Loss: 8.0644\n",
      "Validation Accuracy: 0.93%\n",
      "Model checkpoint saved!\n",
      "Iteration 1/1072, Loss: 7.8658\n",
      "Iteration 2/1072, Loss: 7.8130\n",
      "Iteration 3/1072, Loss: 7.8418\n",
      "Iteration 4/1072, Loss: 7.8475\n",
      "Iteration 5/1072, Loss: 7.8088\n",
      "Iteration 6/1072, Loss: 7.8885\n",
      "Iteration 7/1072, Loss: 7.8830\n",
      "Iteration 8/1072, Loss: 7.7495\n",
      "Iteration 9/1072, Loss: 7.8233\n",
      "Iteration 10/1072, Loss: 7.7917\n",
      "Iteration 11/1072, Loss: 7.8302\n",
      "Iteration 12/1072, Loss: 7.8099\n",
      "Iteration 13/1072, Loss: 7.8432\n",
      "Iteration 14/1072, Loss: 7.8488\n",
      "Iteration 15/1072, Loss: 7.8677\n",
      "Iteration 16/1072, Loss: 7.8928\n",
      "Iteration 17/1072, Loss: 7.8905\n",
      "Iteration 18/1072, Loss: 7.8311\n",
      "Iteration 19/1072, Loss: 7.7976\n",
      "Iteration 20/1072, Loss: 7.8150\n",
      "Iteration 21/1072, Loss: 7.8550\n",
      "Iteration 22/1072, Loss: 7.8283\n",
      "Iteration 23/1072, Loss: 7.7455\n",
      "Iteration 24/1072, Loss: 7.8280\n",
      "Iteration 25/1072, Loss: 7.8509\n",
      "Iteration 26/1072, Loss: 7.8320\n",
      "Iteration 27/1072, Loss: 7.8175\n",
      "Iteration 28/1072, Loss: 7.9339\n",
      "Iteration 29/1072, Loss: 7.8311\n",
      "Iteration 30/1072, Loss: 7.8333\n",
      "Iteration 31/1072, Loss: 7.8619\n",
      "Iteration 32/1072, Loss: 7.8436\n",
      "Iteration 33/1072, Loss: 7.8410\n",
      "Iteration 34/1072, Loss: 7.9031\n",
      "Iteration 35/1072, Loss: 7.8014\n",
      "Iteration 36/1072, Loss: 7.9046\n",
      "Iteration 37/1072, Loss: 7.8751\n",
      "Iteration 38/1072, Loss: 7.9143\n",
      "Iteration 39/1072, Loss: 7.8471\n",
      "Iteration 40/1072, Loss: 7.8592\n",
      "Iteration 41/1072, Loss: 7.8539\n",
      "Iteration 42/1072, Loss: 7.8611\n",
      "Iteration 43/1072, Loss: 7.8431\n",
      "Iteration 44/1072, Loss: 7.8749\n",
      "Iteration 45/1072, Loss: 7.8551\n",
      "Iteration 46/1072, Loss: 7.9315\n",
      "Iteration 47/1072, Loss: 7.8588\n",
      "Iteration 48/1072, Loss: 7.7749\n",
      "Iteration 49/1072, Loss: 7.8030\n",
      "Iteration 50/1072, Loss: 7.8284\n",
      "Iteration 51/1072, Loss: 7.8349\n",
      "Iteration 52/1072, Loss: 7.8649\n",
      "Iteration 53/1072, Loss: 7.8336\n",
      "Iteration 54/1072, Loss: 7.8446\n",
      "Iteration 55/1072, Loss: 7.7893\n",
      "Iteration 56/1072, Loss: 7.9012\n",
      "Iteration 57/1072, Loss: 7.8301\n",
      "Iteration 58/1072, Loss: 7.8311\n",
      "Iteration 59/1072, Loss: 7.8040\n",
      "Iteration 60/1072, Loss: 7.8585\n",
      "Iteration 61/1072, Loss: 7.8380\n",
      "Iteration 62/1072, Loss: 7.8831\n",
      "Iteration 63/1072, Loss: 7.8625\n",
      "Iteration 64/1072, Loss: 7.8667\n",
      "Iteration 65/1072, Loss: 7.8317\n",
      "Iteration 66/1072, Loss: 7.8410\n",
      "Iteration 67/1072, Loss: 7.8085\n",
      "Iteration 68/1072, Loss: 7.9149\n",
      "Iteration 69/1072, Loss: 7.8320\n",
      "Iteration 70/1072, Loss: 7.7962\n",
      "Iteration 71/1072, Loss: 7.8290\n",
      "Iteration 72/1072, Loss: 7.7892\n",
      "Iteration 73/1072, Loss: 7.7816\n",
      "Iteration 74/1072, Loss: 7.7927\n",
      "Iteration 75/1072, Loss: 7.7516\n",
      "Iteration 76/1072, Loss: 7.8063\n",
      "Iteration 77/1072, Loss: 7.8091\n",
      "Iteration 78/1072, Loss: 7.8655\n",
      "Iteration 79/1072, Loss: 7.8254\n",
      "Iteration 80/1072, Loss: 7.8368\n",
      "Iteration 81/1072, Loss: 7.8461\n",
      "Iteration 82/1072, Loss: 7.9044\n",
      "Iteration 83/1072, Loss: 7.8440\n",
      "Iteration 84/1072, Loss: 7.8453\n",
      "Iteration 85/1072, Loss: 7.8104\n",
      "Iteration 86/1072, Loss: 7.8621\n",
      "Iteration 87/1072, Loss: 7.8648\n",
      "Iteration 88/1072, Loss: 7.8859\n",
      "Iteration 89/1072, Loss: 7.8495\n",
      "Iteration 90/1072, Loss: 7.8915\n",
      "Iteration 91/1072, Loss: 7.8981\n",
      "Iteration 92/1072, Loss: 7.8057\n",
      "Iteration 93/1072, Loss: 7.7438\n",
      "Iteration 94/1072, Loss: 7.8505\n",
      "Iteration 95/1072, Loss: 7.7854\n",
      "Iteration 96/1072, Loss: 7.7930\n",
      "Iteration 97/1072, Loss: 7.8268\n",
      "Iteration 98/1072, Loss: 7.8638\n",
      "Iteration 99/1072, Loss: 7.8499\n",
      "Iteration 100/1072, Loss: 7.8804\n",
      "Iteration 101/1072, Loss: 7.8377\n",
      "Iteration 102/1072, Loss: 7.8657\n",
      "Iteration 103/1072, Loss: 7.8720\n",
      "Iteration 104/1072, Loss: 7.8743\n",
      "Iteration 105/1072, Loss: 7.8711\n",
      "Iteration 106/1072, Loss: 7.7907\n",
      "Iteration 107/1072, Loss: 7.8469\n",
      "Iteration 108/1072, Loss: 7.9479\n",
      "Iteration 109/1072, Loss: 7.7927\n",
      "Iteration 110/1072, Loss: 7.8344\n",
      "Iteration 111/1072, Loss: 7.8083\n",
      "Iteration 112/1072, Loss: 7.7742\n",
      "Iteration 113/1072, Loss: 7.8074\n",
      "Iteration 114/1072, Loss: 7.8596\n",
      "Iteration 115/1072, Loss: 7.7520\n",
      "Iteration 116/1072, Loss: 7.8200\n",
      "Iteration 117/1072, Loss: 7.8714\n",
      "Iteration 118/1072, Loss: 7.8598\n",
      "Iteration 119/1072, Loss: 7.7393\n",
      "Iteration 120/1072, Loss: 7.8613\n",
      "Iteration 121/1072, Loss: 7.8292\n",
      "Iteration 122/1072, Loss: 7.8682\n",
      "Iteration 123/1072, Loss: 7.7554\n",
      "Iteration 124/1072, Loss: 7.8737\n",
      "Iteration 125/1072, Loss: 7.8524\n",
      "Iteration 126/1072, Loss: 7.8031\n",
      "Iteration 127/1072, Loss: 7.8968\n",
      "Iteration 128/1072, Loss: 7.8450\n",
      "Iteration 129/1072, Loss: 7.8480\n",
      "Iteration 130/1072, Loss: 7.7733\n",
      "Iteration 131/1072, Loss: 7.8609\n",
      "Iteration 132/1072, Loss: 7.8946\n",
      "Iteration 133/1072, Loss: 7.9117\n",
      "Iteration 134/1072, Loss: 7.8348\n",
      "Iteration 135/1072, Loss: 7.8924\n",
      "Iteration 136/1072, Loss: 7.7983\n",
      "Iteration 137/1072, Loss: 7.8120\n",
      "Iteration 138/1072, Loss: 7.9659\n",
      "Iteration 139/1072, Loss: 7.8427\n",
      "Iteration 140/1072, Loss: 7.8913\n",
      "Iteration 141/1072, Loss: 7.8141\n",
      "Iteration 142/1072, Loss: 7.8654\n",
      "Iteration 143/1072, Loss: 7.8034\n",
      "Iteration 144/1072, Loss: 7.8512\n",
      "Iteration 145/1072, Loss: 7.8771\n",
      "Iteration 146/1072, Loss: 7.8535\n",
      "Iteration 147/1072, Loss: 7.8478\n",
      "Iteration 148/1072, Loss: 7.8738\n",
      "Iteration 149/1072, Loss: 7.8974\n",
      "Iteration 150/1072, Loss: 7.8510\n",
      "Iteration 151/1072, Loss: 7.8990\n",
      "Iteration 152/1072, Loss: 7.8431\n",
      "Iteration 153/1072, Loss: 7.8056\n",
      "Iteration 154/1072, Loss: 7.7871\n",
      "Iteration 155/1072, Loss: 7.8324\n",
      "Iteration 156/1072, Loss: 7.7198\n",
      "Iteration 157/1072, Loss: 7.7981\n",
      "Iteration 158/1072, Loss: 7.8531\n",
      "Iteration 159/1072, Loss: 7.8390\n",
      "Iteration 160/1072, Loss: 7.8605\n",
      "Iteration 161/1072, Loss: 7.8597\n",
      "Iteration 162/1072, Loss: 7.8859\n",
      "Iteration 163/1072, Loss: 7.8609\n",
      "Iteration 164/1072, Loss: 7.9124\n",
      "Iteration 165/1072, Loss: 7.8784\n",
      "Iteration 166/1072, Loss: 7.7583\n",
      "Iteration 167/1072, Loss: 7.7157\n",
      "Iteration 168/1072, Loss: 7.9382\n",
      "Iteration 169/1072, Loss: 7.8041\n",
      "Iteration 170/1072, Loss: 7.8400\n",
      "Iteration 171/1072, Loss: 7.7287\n",
      "Iteration 172/1072, Loss: 7.8576\n",
      "Iteration 173/1072, Loss: 7.8439\n",
      "Iteration 174/1072, Loss: 7.8862\n",
      "Iteration 175/1072, Loss: 7.8362\n",
      "Iteration 176/1072, Loss: 7.7480\n",
      "Iteration 177/1072, Loss: 7.8351\n",
      "Iteration 178/1072, Loss: 7.8225\n",
      "Iteration 179/1072, Loss: 7.9035\n",
      "Iteration 180/1072, Loss: 7.8624\n",
      "Iteration 181/1072, Loss: 7.8414\n",
      "Iteration 182/1072, Loss: 7.8521\n",
      "Iteration 183/1072, Loss: 7.8176\n",
      "Iteration 184/1072, Loss: 7.8160\n",
      "Iteration 185/1072, Loss: 7.8429\n",
      "Iteration 186/1072, Loss: 7.8500\n",
      "Iteration 187/1072, Loss: 7.8210\n",
      "Iteration 188/1072, Loss: 7.7887\n",
      "Iteration 189/1072, Loss: 7.7665\n",
      "Iteration 190/1072, Loss: 7.8136\n",
      "Iteration 191/1072, Loss: 7.7722\n",
      "Iteration 192/1072, Loss: 7.8821\n",
      "Iteration 193/1072, Loss: 7.8925\n",
      "Iteration 194/1072, Loss: 7.7891\n",
      "Iteration 195/1072, Loss: 7.7529\n",
      "Iteration 196/1072, Loss: 7.9262\n",
      "Iteration 197/1072, Loss: 7.7923\n",
      "Iteration 198/1072, Loss: 7.8776\n",
      "Iteration 199/1072, Loss: 7.8172\n",
      "Iteration 200/1072, Loss: 7.8008\n",
      "Iteration 201/1072, Loss: 7.8861\n",
      "Iteration 202/1072, Loss: 7.8591\n",
      "Iteration 203/1072, Loss: 7.8210\n",
      "Iteration 204/1072, Loss: 7.7888\n",
      "Iteration 205/1072, Loss: 7.8771\n",
      "Iteration 206/1072, Loss: 7.8876\n",
      "Iteration 207/1072, Loss: 7.8099\n",
      "Iteration 208/1072, Loss: 7.8592\n",
      "Iteration 209/1072, Loss: 7.8513\n",
      "Iteration 210/1072, Loss: 7.8975\n",
      "Iteration 211/1072, Loss: 7.8366\n",
      "Iteration 212/1072, Loss: 7.7739\n",
      "Iteration 213/1072, Loss: 7.8563\n",
      "Iteration 214/1072, Loss: 7.8343\n",
      "Iteration 215/1072, Loss: 7.8796\n",
      "Iteration 216/1072, Loss: 7.6976\n",
      "Iteration 217/1072, Loss: 7.8570\n",
      "Iteration 218/1072, Loss: 7.8850\n",
      "Iteration 219/1072, Loss: 7.6997\n",
      "Iteration 220/1072, Loss: 7.8224\n",
      "Iteration 221/1072, Loss: 7.8933\n",
      "Iteration 222/1072, Loss: 7.8979\n",
      "Iteration 223/1072, Loss: 7.9184\n",
      "Iteration 224/1072, Loss: 7.8325\n",
      "Iteration 225/1072, Loss: 7.9059\n",
      "Iteration 226/1072, Loss: 7.8347\n",
      "Iteration 227/1072, Loss: 7.9101\n",
      "Iteration 228/1072, Loss: 7.8286\n",
      "Iteration 229/1072, Loss: 7.7877\n",
      "Iteration 230/1072, Loss: 7.8492\n",
      "Iteration 231/1072, Loss: 7.8989\n",
      "Iteration 232/1072, Loss: 7.9588\n",
      "Iteration 233/1072, Loss: 7.8844\n",
      "Iteration 234/1072, Loss: 7.8831\n",
      "Iteration 235/1072, Loss: 7.8446\n",
      "Iteration 236/1072, Loss: 7.8381\n",
      "Iteration 237/1072, Loss: 7.8429\n",
      "Iteration 238/1072, Loss: 7.8246\n",
      "Iteration 239/1072, Loss: 7.8147\n",
      "Iteration 240/1072, Loss: 7.8287\n",
      "Iteration 241/1072, Loss: 7.8266\n",
      "Iteration 242/1072, Loss: 7.7580\n",
      "Iteration 243/1072, Loss: 7.7772\n",
      "Iteration 244/1072, Loss: 7.7578\n",
      "Iteration 245/1072, Loss: 7.8920\n",
      "Iteration 246/1072, Loss: 7.8611\n",
      "Iteration 247/1072, Loss: 7.9045\n",
      "Iteration 248/1072, Loss: 7.7948\n",
      "Iteration 249/1072, Loss: 7.8223\n",
      "Iteration 250/1072, Loss: 7.8104\n",
      "Iteration 251/1072, Loss: 7.8206\n",
      "Iteration 252/1072, Loss: 7.8453\n",
      "Iteration 253/1072, Loss: 7.8116\n",
      "Iteration 254/1072, Loss: 7.9085\n",
      "Iteration 255/1072, Loss: 7.7758\n",
      "Iteration 256/1072, Loss: 7.8317\n",
      "Iteration 257/1072, Loss: 7.8392\n",
      "Iteration 258/1072, Loss: 7.8291\n",
      "Iteration 259/1072, Loss: 7.8841\n",
      "Iteration 260/1072, Loss: 7.8957\n",
      "Iteration 261/1072, Loss: 7.8427\n",
      "Iteration 262/1072, Loss: 7.8375\n",
      "Iteration 263/1072, Loss: 7.8900\n",
      "Iteration 264/1072, Loss: 7.8797\n",
      "Iteration 265/1072, Loss: 7.8670\n",
      "Iteration 266/1072, Loss: 7.9110\n",
      "Iteration 267/1072, Loss: 7.8911\n",
      "Iteration 268/1072, Loss: 7.7714\n",
      "Iteration 269/1072, Loss: 7.9094\n",
      "Iteration 270/1072, Loss: 7.8814\n",
      "Iteration 271/1072, Loss: 7.7518\n",
      "Iteration 272/1072, Loss: 7.8209\n",
      "Iteration 273/1072, Loss: 7.8954\n",
      "Iteration 274/1072, Loss: 7.8223\n",
      "Iteration 275/1072, Loss: 7.8350\n",
      "Iteration 276/1072, Loss: 7.7946\n",
      "Iteration 277/1072, Loss: 7.9046\n",
      "Iteration 278/1072, Loss: 7.8481\n",
      "Iteration 279/1072, Loss: 7.8502\n",
      "Iteration 280/1072, Loss: 7.7719\n",
      "Iteration 281/1072, Loss: 7.8585\n",
      "Iteration 282/1072, Loss: 7.7511\n",
      "Iteration 283/1072, Loss: 7.8338\n",
      "Iteration 284/1072, Loss: 7.8812\n",
      "Iteration 285/1072, Loss: 7.7507\n",
      "Iteration 286/1072, Loss: 7.7965\n",
      "Iteration 287/1072, Loss: 7.7863\n",
      "Iteration 288/1072, Loss: 7.8642\n",
      "Iteration 289/1072, Loss: 7.7726\n",
      "Iteration 290/1072, Loss: 7.8281\n",
      "Iteration 291/1072, Loss: 7.8404\n",
      "Iteration 292/1072, Loss: 7.8612\n",
      "Iteration 293/1072, Loss: 7.9084\n",
      "Iteration 294/1072, Loss: 7.8398\n",
      "Iteration 295/1072, Loss: 7.7938\n",
      "Iteration 296/1072, Loss: 7.8704\n",
      "Iteration 297/1072, Loss: 7.8555\n",
      "Iteration 298/1072, Loss: 7.8477\n",
      "Iteration 299/1072, Loss: 7.8204\n",
      "Iteration 300/1072, Loss: 7.8659\n",
      "Iteration 301/1072, Loss: 7.8063\n",
      "Iteration 302/1072, Loss: 7.8156\n",
      "Iteration 303/1072, Loss: 7.9106\n",
      "Iteration 304/1072, Loss: 7.9059\n",
      "Iteration 305/1072, Loss: 7.7839\n",
      "Iteration 306/1072, Loss: 7.7222\n",
      "Iteration 307/1072, Loss: 7.9110\n",
      "Iteration 308/1072, Loss: 7.7747\n",
      "Iteration 309/1072, Loss: 7.8546\n",
      "Iteration 310/1072, Loss: 7.7696\n",
      "Iteration 311/1072, Loss: 7.9381\n",
      "Iteration 312/1072, Loss: 7.8067\n",
      "Iteration 313/1072, Loss: 7.7410\n",
      "Iteration 314/1072, Loss: 7.8430\n",
      "Iteration 315/1072, Loss: 7.8153\n",
      "Iteration 316/1072, Loss: 7.8085\n",
      "Iteration 317/1072, Loss: 7.8550\n",
      "Iteration 318/1072, Loss: 7.8404\n",
      "Iteration 319/1072, Loss: 7.7867\n",
      "Iteration 320/1072, Loss: 7.9109\n",
      "Iteration 321/1072, Loss: 7.7979\n",
      "Iteration 322/1072, Loss: 7.9345\n",
      "Iteration 323/1072, Loss: 7.8184\n",
      "Iteration 324/1072, Loss: 7.9396\n",
      "Iteration 325/1072, Loss: 7.7847\n",
      "Iteration 326/1072, Loss: 7.8269\n",
      "Iteration 327/1072, Loss: 7.8287\n",
      "Iteration 328/1072, Loss: 7.8332\n",
      "Iteration 329/1072, Loss: 7.7810\n",
      "Iteration 330/1072, Loss: 7.8431\n",
      "Iteration 331/1072, Loss: 7.9050\n",
      "Iteration 332/1072, Loss: 7.7873\n",
      "Iteration 333/1072, Loss: 7.7827\n",
      "Iteration 334/1072, Loss: 7.7587\n",
      "Iteration 335/1072, Loss: 7.8944\n",
      "Iteration 336/1072, Loss: 7.8982\n",
      "Iteration 337/1072, Loss: 7.7683\n",
      "Iteration 338/1072, Loss: 7.8499\n",
      "Iteration 339/1072, Loss: 7.9156\n",
      "Iteration 340/1072, Loss: 7.7661\n",
      "Iteration 341/1072, Loss: 7.8321\n",
      "Iteration 342/1072, Loss: 7.8552\n",
      "Iteration 343/1072, Loss: 7.8561\n",
      "Iteration 344/1072, Loss: 7.9238\n",
      "Iteration 345/1072, Loss: 7.7816\n",
      "Iteration 346/1072, Loss: 7.8225\n",
      "Iteration 347/1072, Loss: 7.8014\n",
      "Iteration 348/1072, Loss: 7.8083\n",
      "Iteration 349/1072, Loss: 7.8736\n",
      "Iteration 350/1072, Loss: 7.7612\n",
      "Iteration 351/1072, Loss: 7.8320\n",
      "Iteration 352/1072, Loss: 7.7928\n",
      "Iteration 353/1072, Loss: 7.8662\n",
      "Iteration 354/1072, Loss: 7.7572\n",
      "Iteration 355/1072, Loss: 7.8643\n",
      "Iteration 356/1072, Loss: 7.9054\n",
      "Iteration 357/1072, Loss: 7.7955\n",
      "Iteration 358/1072, Loss: 7.8295\n",
      "Iteration 359/1072, Loss: 7.7795\n",
      "Iteration 360/1072, Loss: 7.8024\n",
      "Iteration 361/1072, Loss: 7.8799\n",
      "Iteration 362/1072, Loss: 7.8512\n",
      "Iteration 363/1072, Loss: 7.8089\n",
      "Iteration 364/1072, Loss: 7.8123\n",
      "Iteration 365/1072, Loss: 7.8979\n",
      "Iteration 366/1072, Loss: 7.8419\n",
      "Iteration 367/1072, Loss: 7.7469\n",
      "Iteration 368/1072, Loss: 7.8880\n",
      "Iteration 369/1072, Loss: 7.7919\n",
      "Iteration 370/1072, Loss: 7.8941\n",
      "Iteration 371/1072, Loss: 7.9131\n",
      "Iteration 372/1072, Loss: 7.8399\n",
      "Iteration 373/1072, Loss: 7.9005\n",
      "Iteration 374/1072, Loss: 7.9074\n",
      "Iteration 375/1072, Loss: 7.9070\n",
      "Iteration 376/1072, Loss: 7.8006\n",
      "Iteration 377/1072, Loss: 7.7886\n",
      "Iteration 378/1072, Loss: 7.8179\n",
      "Iteration 379/1072, Loss: 7.7914\n",
      "Iteration 380/1072, Loss: 7.9639\n",
      "Iteration 381/1072, Loss: 7.8500\n",
      "Iteration 382/1072, Loss: 7.8605\n",
      "Iteration 383/1072, Loss: 7.7115\n",
      "Iteration 384/1072, Loss: 7.8358\n",
      "Iteration 385/1072, Loss: 7.7574\n",
      "Iteration 386/1072, Loss: 7.8599\n",
      "Iteration 387/1072, Loss: 7.8679\n",
      "Iteration 388/1072, Loss: 7.8358\n",
      "Iteration 389/1072, Loss: 7.9273\n",
      "Iteration 390/1072, Loss: 7.8829\n",
      "Iteration 391/1072, Loss: 7.7434\n",
      "Iteration 392/1072, Loss: 7.8345\n",
      "Iteration 393/1072, Loss: 7.9382\n",
      "Iteration 394/1072, Loss: 7.8533\n",
      "Iteration 395/1072, Loss: 7.8061\n",
      "Iteration 396/1072, Loss: 7.8357\n",
      "Iteration 397/1072, Loss: 7.9287\n",
      "Iteration 398/1072, Loss: 7.8752\n",
      "Iteration 399/1072, Loss: 7.8577\n",
      "Iteration 400/1072, Loss: 7.9302\n",
      "Iteration 401/1072, Loss: 7.8302\n",
      "Iteration 402/1072, Loss: 7.8446\n",
      "Iteration 403/1072, Loss: 7.8155\n",
      "Iteration 404/1072, Loss: 7.8840\n",
      "Iteration 405/1072, Loss: 7.8384\n",
      "Iteration 406/1072, Loss: 7.8034\n",
      "Iteration 407/1072, Loss: 7.8326\n",
      "Iteration 408/1072, Loss: 7.7952\n",
      "Iteration 409/1072, Loss: 7.8413\n",
      "Iteration 410/1072, Loss: 7.7052\n",
      "Iteration 411/1072, Loss: 7.8053\n",
      "Iteration 412/1072, Loss: 7.8230\n",
      "Iteration 413/1072, Loss: 7.7757\n",
      "Iteration 414/1072, Loss: 7.8404\n",
      "Iteration 415/1072, Loss: 7.8392\n",
      "Iteration 416/1072, Loss: 7.8252\n",
      "Iteration 417/1072, Loss: 7.8541\n",
      "Iteration 418/1072, Loss: 7.8710\n",
      "Iteration 419/1072, Loss: 7.7733\n",
      "Iteration 420/1072, Loss: 7.8471\n",
      "Iteration 421/1072, Loss: 7.9147\n",
      "Iteration 422/1072, Loss: 7.8625\n",
      "Iteration 423/1072, Loss: 7.8286\n",
      "Iteration 424/1072, Loss: 7.7058\n",
      "Iteration 425/1072, Loss: 7.8579\n",
      "Iteration 426/1072, Loss: 7.8220\n",
      "Iteration 427/1072, Loss: 7.9617\n",
      "Iteration 428/1072, Loss: 7.7453\n",
      "Iteration 429/1072, Loss: 7.9520\n",
      "Iteration 430/1072, Loss: 7.9119\n",
      "Iteration 431/1072, Loss: 7.8128\n",
      "Iteration 432/1072, Loss: 7.8637\n",
      "Iteration 433/1072, Loss: 7.8177\n",
      "Iteration 434/1072, Loss: 7.8246\n",
      "Iteration 435/1072, Loss: 7.8144\n",
      "Iteration 436/1072, Loss: 7.7958\n",
      "Iteration 437/1072, Loss: 7.7648\n",
      "Iteration 438/1072, Loss: 7.7815\n",
      "Iteration 439/1072, Loss: 7.8900\n",
      "Iteration 440/1072, Loss: 7.8818\n",
      "Iteration 441/1072, Loss: 7.8812\n",
      "Iteration 442/1072, Loss: 7.9422\n",
      "Iteration 443/1072, Loss: 7.7661\n",
      "Iteration 444/1072, Loss: 7.7458\n",
      "Iteration 445/1072, Loss: 7.8632\n",
      "Iteration 446/1072, Loss: 7.7936\n",
      "Iteration 447/1072, Loss: 7.8363\n",
      "Iteration 448/1072, Loss: 7.8533\n",
      "Iteration 449/1072, Loss: 7.8060\n",
      "Iteration 450/1072, Loss: 7.8848\n",
      "Iteration 451/1072, Loss: 7.7889\n",
      "Iteration 452/1072, Loss: 7.7896\n",
      "Iteration 453/1072, Loss: 7.7603\n",
      "Iteration 454/1072, Loss: 7.7960\n",
      "Iteration 455/1072, Loss: 7.8565\n",
      "Iteration 456/1072, Loss: 7.6349\n",
      "Iteration 457/1072, Loss: 7.8627\n",
      "Iteration 458/1072, Loss: 7.8985\n",
      "Iteration 459/1072, Loss: 7.8575\n",
      "Iteration 460/1072, Loss: 7.8657\n",
      "Iteration 461/1072, Loss: 7.7749\n",
      "Iteration 462/1072, Loss: 7.8252\n",
      "Iteration 463/1072, Loss: 7.8597\n",
      "Iteration 464/1072, Loss: 7.7996\n",
      "Iteration 465/1072, Loss: 7.8484\n",
      "Iteration 466/1072, Loss: 7.7791\n",
      "Iteration 467/1072, Loss: 7.9521\n",
      "Iteration 468/1072, Loss: 7.8614\n",
      "Iteration 469/1072, Loss: 7.7326\n",
      "Iteration 470/1072, Loss: 7.8724\n",
      "Iteration 471/1072, Loss: 7.7412\n",
      "Iteration 472/1072, Loss: 7.8500\n",
      "Iteration 473/1072, Loss: 7.7448\n",
      "Iteration 474/1072, Loss: 7.9385\n",
      "Iteration 475/1072, Loss: 7.7945\n",
      "Iteration 476/1072, Loss: 7.8507\n",
      "Iteration 477/1072, Loss: 7.8743\n",
      "Iteration 478/1072, Loss: 7.8677\n",
      "Iteration 479/1072, Loss: 7.7846\n",
      "Iteration 480/1072, Loss: 7.8883\n",
      "Iteration 481/1072, Loss: 7.9445\n",
      "Iteration 482/1072, Loss: 7.7917\n",
      "Iteration 483/1072, Loss: 7.7708\n",
      "Iteration 484/1072, Loss: 7.8248\n",
      "Iteration 485/1072, Loss: 7.7912\n",
      "Iteration 486/1072, Loss: 7.8699\n",
      "Iteration 487/1072, Loss: 7.8257\n",
      "Iteration 488/1072, Loss: 7.8311\n",
      "Iteration 489/1072, Loss: 7.7122\n",
      "Iteration 490/1072, Loss: 7.6561\n",
      "Iteration 491/1072, Loss: 7.8022\n",
      "Iteration 492/1072, Loss: 7.7801\n",
      "Iteration 493/1072, Loss: 7.9428\n",
      "Iteration 494/1072, Loss: 7.8352\n",
      "Iteration 495/1072, Loss: 7.8039\n",
      "Iteration 496/1072, Loss: 7.8277\n",
      "Iteration 497/1072, Loss: 7.9160\n",
      "Iteration 498/1072, Loss: 7.8070\n",
      "Iteration 499/1072, Loss: 7.8205\n",
      "Iteration 500/1072, Loss: 7.7307\n",
      "Iteration 501/1072, Loss: 7.7630\n",
      "Iteration 502/1072, Loss: 7.8017\n",
      "Iteration 503/1072, Loss: 7.8493\n",
      "Iteration 504/1072, Loss: 7.7929\n",
      "Iteration 505/1072, Loss: 7.7981\n",
      "Iteration 506/1072, Loss: 7.8709\n",
      "Iteration 507/1072, Loss: 7.8127\n",
      "Iteration 508/1072, Loss: 7.7980\n",
      "Iteration 509/1072, Loss: 7.7873\n",
      "Iteration 510/1072, Loss: 7.8675\n",
      "Iteration 511/1072, Loss: 7.9151\n",
      "Iteration 512/1072, Loss: 7.8195\n",
      "Iteration 513/1072, Loss: 7.8068\n",
      "Iteration 514/1072, Loss: 7.7604\n",
      "Iteration 515/1072, Loss: 7.8194\n",
      "Iteration 516/1072, Loss: 7.7794\n",
      "Iteration 517/1072, Loss: 7.8755\n",
      "Iteration 518/1072, Loss: 7.9163\n",
      "Iteration 519/1072, Loss: 7.7522\n",
      "Iteration 520/1072, Loss: 7.9212\n",
      "Iteration 521/1072, Loss: 7.7737\n",
      "Iteration 522/1072, Loss: 7.7777\n",
      "Iteration 523/1072, Loss: 7.8129\n",
      "Iteration 524/1072, Loss: 7.7822\n",
      "Iteration 525/1072, Loss: 7.8503\n",
      "Iteration 526/1072, Loss: 7.8632\n",
      "Iteration 527/1072, Loss: 7.8562\n",
      "Iteration 528/1072, Loss: 7.8717\n",
      "Iteration 529/1072, Loss: 7.8842\n",
      "Iteration 530/1072, Loss: 7.8270\n",
      "Iteration 531/1072, Loss: 7.7911\n",
      "Iteration 532/1072, Loss: 7.8936\n",
      "Iteration 533/1072, Loss: 7.7544\n",
      "Iteration 534/1072, Loss: 7.7752\n",
      "Iteration 535/1072, Loss: 7.8465\n",
      "Iteration 536/1072, Loss: 7.8003\n",
      "Iteration 537/1072, Loss: 7.8231\n",
      "Iteration 538/1072, Loss: 7.7753\n",
      "Iteration 539/1072, Loss: 7.7122\n",
      "Iteration 540/1072, Loss: 7.8414\n",
      "Iteration 541/1072, Loss: 7.8589\n",
      "Iteration 542/1072, Loss: 7.8164\n",
      "Iteration 543/1072, Loss: 7.8808\n",
      "Iteration 544/1072, Loss: 7.7454\n",
      "Iteration 545/1072, Loss: 7.8732\n",
      "Iteration 546/1072, Loss: 7.8810\n",
      "Iteration 547/1072, Loss: 7.8859\n",
      "Iteration 548/1072, Loss: 7.8656\n",
      "Iteration 549/1072, Loss: 7.8491\n",
      "Iteration 550/1072, Loss: 7.8191\n",
      "Iteration 551/1072, Loss: 7.8205\n",
      "Iteration 552/1072, Loss: 7.8185\n",
      "Iteration 553/1072, Loss: 7.8038\n",
      "Iteration 554/1072, Loss: 7.8945\n",
      "Iteration 555/1072, Loss: 7.8148\n",
      "Iteration 556/1072, Loss: 7.7905\n",
      "Iteration 557/1072, Loss: 7.7525\n",
      "Iteration 558/1072, Loss: 7.8267\n",
      "Iteration 559/1072, Loss: 7.7789\n",
      "Iteration 560/1072, Loss: 7.8713\n",
      "Iteration 561/1072, Loss: 7.8004\n",
      "Iteration 562/1072, Loss: 7.7993\n",
      "Iteration 563/1072, Loss: 7.7876\n",
      "Iteration 564/1072, Loss: 7.8612\n",
      "Iteration 565/1072, Loss: 7.8382\n",
      "Iteration 566/1072, Loss: 7.7947\n",
      "Iteration 567/1072, Loss: 7.7686\n",
      "Iteration 568/1072, Loss: 7.8354\n",
      "Iteration 569/1072, Loss: 7.7801\n",
      "Iteration 570/1072, Loss: 7.7336\n",
      "Iteration 571/1072, Loss: 7.9188\n",
      "Iteration 572/1072, Loss: 7.7975\n",
      "Iteration 573/1072, Loss: 7.7458\n",
      "Iteration 574/1072, Loss: 7.8278\n",
      "Iteration 575/1072, Loss: 7.7945\n",
      "Iteration 576/1072, Loss: 7.8663\n",
      "Iteration 577/1072, Loss: 7.8754\n",
      "Iteration 578/1072, Loss: 7.9848\n",
      "Iteration 579/1072, Loss: 7.8699\n",
      "Iteration 580/1072, Loss: 7.8008\n",
      "Iteration 581/1072, Loss: 7.7482\n",
      "Iteration 582/1072, Loss: 7.9048\n",
      "Iteration 583/1072, Loss: 7.7488\n",
      "Iteration 584/1072, Loss: 7.8263\n",
      "Iteration 585/1072, Loss: 7.8194\n",
      "Iteration 586/1072, Loss: 7.7928\n",
      "Iteration 587/1072, Loss: 7.9062\n",
      "Iteration 588/1072, Loss: 7.7665\n",
      "Iteration 589/1072, Loss: 7.8098\n",
      "Iteration 590/1072, Loss: 7.8366\n",
      "Iteration 591/1072, Loss: 7.7546\n",
      "Iteration 592/1072, Loss: 7.7581\n",
      "Iteration 593/1072, Loss: 7.7819\n",
      "Iteration 594/1072, Loss: 7.7494\n",
      "Iteration 595/1072, Loss: 7.6399\n",
      "Iteration 596/1072, Loss: 7.8488\n",
      "Iteration 597/1072, Loss: 7.8220\n",
      "Iteration 598/1072, Loss: 7.8186\n",
      "Iteration 599/1072, Loss: 7.7978\n",
      "Iteration 600/1072, Loss: 7.8935\n",
      "Iteration 601/1072, Loss: 7.9102\n",
      "Iteration 602/1072, Loss: 7.7439\n",
      "Iteration 603/1072, Loss: 7.7390\n",
      "Iteration 604/1072, Loss: 7.6962\n",
      "Iteration 605/1072, Loss: 7.6867\n",
      "Iteration 606/1072, Loss: 7.8548\n",
      "Iteration 607/1072, Loss: 7.9004\n",
      "Iteration 608/1072, Loss: 7.6862\n",
      "Iteration 609/1072, Loss: 7.7913\n",
      "Iteration 610/1072, Loss: 7.8522\n",
      "Iteration 611/1072, Loss: 7.7627\n",
      "Iteration 612/1072, Loss: 7.7938\n",
      "Iteration 613/1072, Loss: 7.8104\n",
      "Iteration 614/1072, Loss: 7.7331\n",
      "Iteration 615/1072, Loss: 7.8353\n",
      "Iteration 616/1072, Loss: 7.7879\n",
      "Iteration 617/1072, Loss: 7.7110\n",
      "Iteration 618/1072, Loss: 7.8379\n",
      "Iteration 619/1072, Loss: 7.8513\n",
      "Iteration 620/1072, Loss: 7.8782\n",
      "Iteration 621/1072, Loss: 7.7830\n",
      "Iteration 622/1072, Loss: 7.7904\n",
      "Iteration 623/1072, Loss: 7.8861\n",
      "Iteration 624/1072, Loss: 7.8307\n",
      "Iteration 625/1072, Loss: 7.6702\n",
      "Iteration 626/1072, Loss: 7.7838\n",
      "Iteration 627/1072, Loss: 7.8414\n",
      "Iteration 628/1072, Loss: 7.8529\n",
      "Iteration 629/1072, Loss: 7.8540\n",
      "Iteration 630/1072, Loss: 7.7618\n",
      "Iteration 631/1072, Loss: 7.8596\n",
      "Iteration 632/1072, Loss: 7.7660\n",
      "Iteration 633/1072, Loss: 7.7987\n",
      "Iteration 634/1072, Loss: 7.8273\n",
      "Iteration 635/1072, Loss: 7.8297\n",
      "Iteration 636/1072, Loss: 7.8125\n",
      "Iteration 637/1072, Loss: 7.8125\n",
      "Iteration 638/1072, Loss: 7.8523\n",
      "Iteration 639/1072, Loss: 7.7598\n",
      "Iteration 640/1072, Loss: 7.7854\n",
      "Iteration 641/1072, Loss: 7.9084\n",
      "Iteration 642/1072, Loss: 7.8526\n",
      "Iteration 643/1072, Loss: 7.7815\n",
      "Iteration 644/1072, Loss: 7.8803\n",
      "Iteration 645/1072, Loss: 7.8235\n",
      "Iteration 646/1072, Loss: 7.7374\n",
      "Iteration 647/1072, Loss: 7.8164\n",
      "Iteration 648/1072, Loss: 7.7940\n",
      "Iteration 649/1072, Loss: 7.7652\n",
      "Iteration 650/1072, Loss: 7.7794\n",
      "Iteration 651/1072, Loss: 7.7485\n",
      "Iteration 652/1072, Loss: 7.8027\n",
      "Iteration 653/1072, Loss: 7.8022\n",
      "Iteration 654/1072, Loss: 7.8900\n",
      "Iteration 655/1072, Loss: 7.8283\n",
      "Iteration 656/1072, Loss: 7.8206\n",
      "Iteration 657/1072, Loss: 7.7391\n",
      "Iteration 658/1072, Loss: 7.7497\n",
      "Iteration 659/1072, Loss: 7.7921\n",
      "Iteration 660/1072, Loss: 7.8064\n",
      "Iteration 661/1072, Loss: 7.8499\n",
      "Iteration 662/1072, Loss: 7.8792\n",
      "Iteration 663/1072, Loss: 7.8935\n",
      "Iteration 664/1072, Loss: 7.7902\n",
      "Iteration 665/1072, Loss: 7.8149\n",
      "Iteration 666/1072, Loss: 7.7474\n",
      "Iteration 667/1072, Loss: 7.8475\n",
      "Iteration 668/1072, Loss: 7.9062\n",
      "Iteration 669/1072, Loss: 7.7365\n",
      "Iteration 670/1072, Loss: 7.6952\n",
      "Iteration 671/1072, Loss: 7.8452\n",
      "Iteration 672/1072, Loss: 7.8397\n",
      "Iteration 673/1072, Loss: 7.8316\n",
      "Iteration 674/1072, Loss: 7.9307\n",
      "Iteration 675/1072, Loss: 7.8170\n",
      "Iteration 676/1072, Loss: 7.6850\n",
      "Iteration 677/1072, Loss: 7.8363\n",
      "Iteration 678/1072, Loss: 7.7687\n",
      "Iteration 679/1072, Loss: 7.6696\n",
      "Iteration 680/1072, Loss: 7.7304\n",
      "Iteration 681/1072, Loss: 7.8638\n",
      "Iteration 682/1072, Loss: 7.7813\n",
      "Iteration 683/1072, Loss: 7.8866\n",
      "Iteration 684/1072, Loss: 7.8034\n",
      "Iteration 685/1072, Loss: 7.8098\n",
      "Iteration 686/1072, Loss: 7.8807\n",
      "Iteration 687/1072, Loss: 7.8282\n",
      "Iteration 688/1072, Loss: 7.8545\n",
      "Iteration 689/1072, Loss: 7.7362\n",
      "Iteration 690/1072, Loss: 7.8265\n",
      "Iteration 691/1072, Loss: 7.8552\n",
      "Iteration 692/1072, Loss: 7.8209\n",
      "Iteration 693/1072, Loss: 7.7280\n",
      "Iteration 694/1072, Loss: 7.7455\n",
      "Iteration 695/1072, Loss: 7.8216\n",
      "Iteration 696/1072, Loss: 7.7331\n",
      "Iteration 697/1072, Loss: 7.7308\n",
      "Iteration 698/1072, Loss: 7.8355\n",
      "Iteration 699/1072, Loss: 7.7345\n",
      "Iteration 700/1072, Loss: 7.7279\n",
      "Iteration 701/1072, Loss: 7.8108\n",
      "Iteration 702/1072, Loss: 7.7658\n",
      "Iteration 703/1072, Loss: 7.7991\n",
      "Iteration 704/1072, Loss: 7.8342\n",
      "Iteration 705/1072, Loss: 7.7020\n",
      "Iteration 706/1072, Loss: 7.7781\n",
      "Iteration 707/1072, Loss: 7.7386\n",
      "Iteration 708/1072, Loss: 7.8070\n",
      "Iteration 709/1072, Loss: 7.7896\n",
      "Iteration 710/1072, Loss: 7.8120\n",
      "Iteration 711/1072, Loss: 7.8151\n",
      "Iteration 712/1072, Loss: 7.7564\n",
      "Iteration 713/1072, Loss: 7.7685\n",
      "Iteration 714/1072, Loss: 7.6885\n",
      "Iteration 715/1072, Loss: 7.8116\n",
      "Iteration 716/1072, Loss: 7.8012\n",
      "Iteration 717/1072, Loss: 7.8250\n",
      "Iteration 718/1072, Loss: 7.8385\n",
      "Iteration 719/1072, Loss: 7.7545\n",
      "Iteration 720/1072, Loss: 7.7226\n",
      "Iteration 721/1072, Loss: 7.8442\n",
      "Iteration 722/1072, Loss: 7.8411\n",
      "Iteration 723/1072, Loss: 7.8201\n",
      "Iteration 724/1072, Loss: 7.8329\n",
      "Iteration 725/1072, Loss: 7.8040\n",
      "Iteration 726/1072, Loss: 7.7545\n",
      "Iteration 727/1072, Loss: 7.8380\n",
      "Iteration 728/1072, Loss: 7.8819\n",
      "Iteration 729/1072, Loss: 7.8256\n",
      "Iteration 730/1072, Loss: 7.9041\n",
      "Iteration 731/1072, Loss: 7.6914\n",
      "Iteration 732/1072, Loss: 7.8609\n",
      "Iteration 733/1072, Loss: 7.7027\n",
      "Iteration 734/1072, Loss: 7.7892\n",
      "Iteration 735/1072, Loss: 7.7015\n",
      "Iteration 736/1072, Loss: 7.7572\n",
      "Iteration 737/1072, Loss: 7.8528\n",
      "Iteration 738/1072, Loss: 7.8451\n",
      "Iteration 739/1072, Loss: 7.9164\n",
      "Iteration 740/1072, Loss: 7.8544\n",
      "Iteration 741/1072, Loss: 7.7177\n",
      "Iteration 742/1072, Loss: 7.8131\n",
      "Iteration 743/1072, Loss: 7.7832\n",
      "Iteration 744/1072, Loss: 7.8399\n",
      "Iteration 745/1072, Loss: 7.7707\n",
      "Iteration 746/1072, Loss: 7.6994\n",
      "Iteration 747/1072, Loss: 7.8057\n",
      "Iteration 748/1072, Loss: 7.7917\n",
      "Iteration 749/1072, Loss: 7.8626\n",
      "Iteration 750/1072, Loss: 7.7084\n",
      "Iteration 751/1072, Loss: 7.7495\n",
      "Iteration 752/1072, Loss: 7.8502\n",
      "Iteration 753/1072, Loss: 7.7417\n",
      "Iteration 754/1072, Loss: 7.7562\n",
      "Iteration 755/1072, Loss: 7.7801\n",
      "Iteration 756/1072, Loss: 7.8649\n",
      "Iteration 757/1072, Loss: 7.7939\n",
      "Iteration 758/1072, Loss: 7.8226\n",
      "Iteration 759/1072, Loss: 7.7908\n",
      "Iteration 760/1072, Loss: 7.8055\n",
      "Iteration 761/1072, Loss: 7.8300\n",
      "Iteration 762/1072, Loss: 7.7972\n",
      "Iteration 763/1072, Loss: 7.7907\n",
      "Iteration 764/1072, Loss: 7.7618\n",
      "Iteration 765/1072, Loss: 7.8747\n",
      "Iteration 766/1072, Loss: 7.7956\n",
      "Iteration 767/1072, Loss: 7.9180\n",
      "Iteration 768/1072, Loss: 7.7596\n",
      "Iteration 769/1072, Loss: 7.6703\n",
      "Iteration 770/1072, Loss: 7.7365\n",
      "Iteration 771/1072, Loss: 7.7728\n",
      "Iteration 772/1072, Loss: 7.7584\n",
      "Iteration 773/1072, Loss: 7.8187\n",
      "Iteration 774/1072, Loss: 7.7488\n",
      "Iteration 775/1072, Loss: 7.6809\n",
      "Iteration 776/1072, Loss: 7.8401\n",
      "Iteration 777/1072, Loss: 7.9061\n",
      "Iteration 778/1072, Loss: 7.7654\n",
      "Iteration 779/1072, Loss: 7.6171\n",
      "Iteration 780/1072, Loss: 7.7683\n",
      "Iteration 781/1072, Loss: 7.7990\n",
      "Iteration 782/1072, Loss: 7.7927\n",
      "Iteration 783/1072, Loss: 7.8574\n",
      "Iteration 784/1072, Loss: 7.7542\n",
      "Iteration 785/1072, Loss: 7.8232\n",
      "Iteration 786/1072, Loss: 7.8528\n",
      "Iteration 787/1072, Loss: 7.8452\n",
      "Iteration 788/1072, Loss: 7.7982\n",
      "Iteration 789/1072, Loss: 7.7840\n",
      "Iteration 790/1072, Loss: 7.7869\n",
      "Iteration 791/1072, Loss: 7.7087\n",
      "Iteration 792/1072, Loss: 7.7943\n",
      "Iteration 793/1072, Loss: 7.8103\n",
      "Iteration 794/1072, Loss: 7.8140\n",
      "Iteration 795/1072, Loss: 7.8568\n",
      "Iteration 796/1072, Loss: 7.6971\n",
      "Iteration 797/1072, Loss: 7.7534\n",
      "Iteration 798/1072, Loss: 7.6122\n",
      "Iteration 799/1072, Loss: 7.7069\n",
      "Iteration 800/1072, Loss: 7.6291\n",
      "Iteration 801/1072, Loss: 7.9230\n",
      "Iteration 802/1072, Loss: 7.7894\n",
      "Iteration 803/1072, Loss: 7.6841\n",
      "Iteration 804/1072, Loss: 7.8240\n",
      "Iteration 805/1072, Loss: 7.8601\n",
      "Iteration 806/1072, Loss: 7.7432\n",
      "Iteration 807/1072, Loss: 7.7111\n",
      "Iteration 808/1072, Loss: 7.8007\n",
      "Iteration 809/1072, Loss: 7.7958\n",
      "Iteration 810/1072, Loss: 7.7531\n",
      "Iteration 811/1072, Loss: 7.7527\n",
      "Iteration 812/1072, Loss: 7.8401\n",
      "Iteration 813/1072, Loss: 7.7497\n",
      "Iteration 814/1072, Loss: 7.7822\n",
      "Iteration 815/1072, Loss: 7.8302\n",
      "Iteration 816/1072, Loss: 7.7787\n",
      "Iteration 817/1072, Loss: 7.7576\n",
      "Iteration 818/1072, Loss: 7.7829\n",
      "Iteration 819/1072, Loss: 7.7505\n",
      "Iteration 820/1072, Loss: 7.8342\n",
      "Iteration 821/1072, Loss: 7.8852\n",
      "Iteration 822/1072, Loss: 7.8310\n",
      "Iteration 823/1072, Loss: 7.6667\n",
      "Iteration 824/1072, Loss: 7.7947\n",
      "Iteration 825/1072, Loss: 7.7996\n",
      "Iteration 826/1072, Loss: 7.7746\n",
      "Iteration 827/1072, Loss: 7.8348\n",
      "Iteration 828/1072, Loss: 7.8376\n",
      "Iteration 829/1072, Loss: 7.7903\n",
      "Iteration 830/1072, Loss: 7.8432\n",
      "Iteration 831/1072, Loss: 7.7930\n",
      "Iteration 832/1072, Loss: 7.6896\n",
      "Iteration 833/1072, Loss: 7.8323\n",
      "Iteration 834/1072, Loss: 7.7565\n",
      "Iteration 835/1072, Loss: 7.7602\n",
      "Iteration 836/1072, Loss: 7.8260\n",
      "Iteration 837/1072, Loss: 7.8749\n",
      "Iteration 838/1072, Loss: 7.6809\n",
      "Iteration 839/1072, Loss: 7.7625\n",
      "Iteration 840/1072, Loss: 7.7356\n",
      "Iteration 841/1072, Loss: 7.8347\n",
      "Iteration 842/1072, Loss: 7.8106\n",
      "Iteration 843/1072, Loss: 7.8451\n",
      "Iteration 844/1072, Loss: 7.7448\n",
      "Iteration 845/1072, Loss: 7.8303\n",
      "Iteration 846/1072, Loss: 7.7959\n",
      "Iteration 847/1072, Loss: 7.7831\n",
      "Iteration 848/1072, Loss: 7.8899\n",
      "Iteration 849/1072, Loss: 7.8299\n",
      "Iteration 850/1072, Loss: 7.8337\n",
      "Iteration 851/1072, Loss: 7.8209\n",
      "Iteration 852/1072, Loss: 7.7121\n",
      "Iteration 853/1072, Loss: 7.7206\n",
      "Iteration 854/1072, Loss: 7.7713\n",
      "Iteration 855/1072, Loss: 7.8240\n",
      "Iteration 856/1072, Loss: 7.7182\n",
      "Iteration 857/1072, Loss: 7.8114\n",
      "Iteration 858/1072, Loss: 7.8035\n",
      "Iteration 859/1072, Loss: 7.7207\n",
      "Iteration 860/1072, Loss: 7.8598\n",
      "Iteration 861/1072, Loss: 7.8724\n",
      "Iteration 862/1072, Loss: 7.7261\n",
      "Iteration 863/1072, Loss: 7.7810\n",
      "Iteration 864/1072, Loss: 7.7588\n",
      "Iteration 865/1072, Loss: 7.7332\n",
      "Iteration 866/1072, Loss: 7.9109\n",
      "Iteration 867/1072, Loss: 7.8078\n",
      "Iteration 868/1072, Loss: 7.7550\n",
      "Iteration 869/1072, Loss: 7.6200\n",
      "Iteration 870/1072, Loss: 7.7663\n",
      "Iteration 871/1072, Loss: 7.7385\n",
      "Iteration 872/1072, Loss: 7.7261\n",
      "Iteration 873/1072, Loss: 7.7760\n",
      "Iteration 874/1072, Loss: 7.8279\n",
      "Iteration 875/1072, Loss: 7.8581\n",
      "Iteration 876/1072, Loss: 7.7211\n",
      "Iteration 877/1072, Loss: 7.8436\n",
      "Iteration 878/1072, Loss: 7.7798\n",
      "Iteration 879/1072, Loss: 7.8469\n",
      "Iteration 880/1072, Loss: 7.7857\n",
      "Iteration 881/1072, Loss: 7.8061\n",
      "Iteration 882/1072, Loss: 7.8055\n",
      "Iteration 883/1072, Loss: 7.6457\n",
      "Iteration 884/1072, Loss: 7.8266\n",
      "Iteration 885/1072, Loss: 7.8587\n",
      "Iteration 886/1072, Loss: 7.7848\n",
      "Iteration 887/1072, Loss: 7.7100\n",
      "Iteration 888/1072, Loss: 7.7630\n",
      "Iteration 889/1072, Loss: 7.7224\n",
      "Iteration 890/1072, Loss: 7.7425\n",
      "Iteration 891/1072, Loss: 7.6963\n",
      "Iteration 892/1072, Loss: 7.7357\n",
      "Iteration 893/1072, Loss: 7.6889\n",
      "Iteration 894/1072, Loss: 7.7732\n",
      "Iteration 895/1072, Loss: 7.8142\n",
      "Iteration 896/1072, Loss: 7.6513\n",
      "Iteration 897/1072, Loss: 7.7219\n",
      "Iteration 898/1072, Loss: 7.8798\n",
      "Iteration 899/1072, Loss: 7.6759\n",
      "Iteration 900/1072, Loss: 7.7802\n",
      "Iteration 901/1072, Loss: 7.7954\n",
      "Iteration 902/1072, Loss: 7.7201\n",
      "Iteration 903/1072, Loss: 7.6917\n",
      "Iteration 904/1072, Loss: 7.7071\n",
      "Iteration 905/1072, Loss: 7.8215\n",
      "Iteration 906/1072, Loss: 7.7951\n",
      "Iteration 907/1072, Loss: 7.8485\n",
      "Iteration 908/1072, Loss: 7.7705\n",
      "Iteration 909/1072, Loss: 7.7883\n",
      "Iteration 910/1072, Loss: 7.7294\n",
      "Iteration 911/1072, Loss: 7.7248\n",
      "Iteration 912/1072, Loss: 7.7977\n",
      "Iteration 913/1072, Loss: 7.8837\n",
      "Iteration 914/1072, Loss: 7.7816\n",
      "Iteration 915/1072, Loss: 7.6680\n",
      "Iteration 916/1072, Loss: 7.6543\n",
      "Iteration 917/1072, Loss: 7.6488\n",
      "Iteration 918/1072, Loss: 7.6767\n",
      "Iteration 919/1072, Loss: 7.8502\n",
      "Iteration 920/1072, Loss: 7.8325\n",
      "Iteration 921/1072, Loss: 7.7588\n",
      "Iteration 922/1072, Loss: 7.8124\n",
      "Iteration 923/1072, Loss: 7.6487\n",
      "Iteration 924/1072, Loss: 7.8292\n",
      "Iteration 925/1072, Loss: 7.7664\n",
      "Iteration 926/1072, Loss: 7.6860\n",
      "Iteration 927/1072, Loss: 7.8095\n",
      "Iteration 928/1072, Loss: 7.7414\n",
      "Iteration 929/1072, Loss: 7.7959\n",
      "Iteration 930/1072, Loss: 7.7498\n",
      "Iteration 931/1072, Loss: 7.6275\n",
      "Iteration 932/1072, Loss: 7.9038\n",
      "Iteration 933/1072, Loss: 7.6834\n",
      "Iteration 934/1072, Loss: 7.7850\n",
      "Iteration 935/1072, Loss: 7.8576\n",
      "Iteration 936/1072, Loss: 7.7738\n",
      "Iteration 937/1072, Loss: 7.9375\n",
      "Iteration 938/1072, Loss: 7.8678\n",
      "Iteration 939/1072, Loss: 7.7759\n",
      "Iteration 940/1072, Loss: 7.6973\n",
      "Iteration 941/1072, Loss: 7.8218\n",
      "Iteration 942/1072, Loss: 7.7093\n",
      "Iteration 943/1072, Loss: 7.7851\n",
      "Iteration 944/1072, Loss: 7.8219\n",
      "Iteration 945/1072, Loss: 7.7137\n",
      "Iteration 946/1072, Loss: 7.7439\n",
      "Iteration 947/1072, Loss: 7.7095\n",
      "Iteration 948/1072, Loss: 7.8777\n",
      "Iteration 949/1072, Loss: 7.8200\n",
      "Iteration 950/1072, Loss: 7.7878\n",
      "Iteration 951/1072, Loss: 7.8087\n",
      "Iteration 952/1072, Loss: 7.6243\n",
      "Iteration 953/1072, Loss: 7.8365\n",
      "Iteration 954/1072, Loss: 7.5943\n",
      "Iteration 955/1072, Loss: 7.7483\n",
      "Iteration 956/1072, Loss: 7.7207\n",
      "Iteration 957/1072, Loss: 7.8015\n",
      "Iteration 958/1072, Loss: 7.6948\n",
      "Iteration 959/1072, Loss: 7.8293\n",
      "Iteration 960/1072, Loss: 7.8015\n",
      "Iteration 961/1072, Loss: 7.7967\n",
      "Iteration 962/1072, Loss: 7.6639\n",
      "Iteration 963/1072, Loss: 7.7327\n",
      "Iteration 964/1072, Loss: 7.8558\n",
      "Iteration 965/1072, Loss: 7.8317\n",
      "Iteration 966/1072, Loss: 7.7443\n",
      "Iteration 967/1072, Loss: 7.7401\n",
      "Iteration 968/1072, Loss: 7.8356\n",
      "Iteration 969/1072, Loss: 7.6312\n",
      "Iteration 970/1072, Loss: 7.7023\n",
      "Iteration 971/1072, Loss: 7.7496\n",
      "Iteration 972/1072, Loss: 7.7512\n",
      "Iteration 973/1072, Loss: 7.8014\n",
      "Iteration 974/1072, Loss: 7.6824\n",
      "Iteration 975/1072, Loss: 7.7200\n",
      "Iteration 976/1072, Loss: 7.7661\n",
      "Iteration 977/1072, Loss: 7.6961\n",
      "Iteration 978/1072, Loss: 7.7884\n",
      "Iteration 979/1072, Loss: 7.8500\n",
      "Iteration 980/1072, Loss: 7.5857\n",
      "Iteration 981/1072, Loss: 7.6818\n",
      "Iteration 982/1072, Loss: 7.7674\n",
      "Iteration 983/1072, Loss: 7.8030\n",
      "Iteration 984/1072, Loss: 7.8136\n",
      "Iteration 985/1072, Loss: 7.6705\n",
      "Iteration 986/1072, Loss: 7.8330\n",
      "Iteration 987/1072, Loss: 7.8024\n",
      "Iteration 988/1072, Loss: 7.7322\n",
      "Iteration 989/1072, Loss: 7.8154\n",
      "Iteration 990/1072, Loss: 7.8167\n",
      "Iteration 991/1072, Loss: 7.8061\n",
      "Iteration 992/1072, Loss: 7.8027\n",
      "Iteration 993/1072, Loss: 7.7749\n",
      "Iteration 994/1072, Loss: 7.6769\n",
      "Iteration 995/1072, Loss: 7.7858\n",
      "Iteration 996/1072, Loss: 7.7870\n",
      "Iteration 997/1072, Loss: 7.7523\n",
      "Iteration 998/1072, Loss: 7.7212\n",
      "Iteration 999/1072, Loss: 7.7116\n",
      "Iteration 1000/1072, Loss: 7.7353\n",
      "Iteration 1001/1072, Loss: 7.7676\n",
      "Iteration 1002/1072, Loss: 7.7503\n",
      "Iteration 1003/1072, Loss: 7.7120\n",
      "Iteration 1004/1072, Loss: 7.8137\n",
      "Iteration 1005/1072, Loss: 7.7014\n",
      "Iteration 1006/1072, Loss: 7.8460\n",
      "Iteration 1007/1072, Loss: 7.7431\n",
      "Iteration 1008/1072, Loss: 7.7170\n",
      "Iteration 1009/1072, Loss: 7.6368\n",
      "Iteration 1010/1072, Loss: 7.7338\n",
      "Iteration 1011/1072, Loss: 7.6820\n",
      "Iteration 1012/1072, Loss: 7.6236\n",
      "Iteration 1013/1072, Loss: 7.6292\n",
      "Iteration 1014/1072, Loss: 7.7912\n",
      "Iteration 1015/1072, Loss: 7.7610\n",
      "Iteration 1016/1072, Loss: 7.6970\n",
      "Iteration 1017/1072, Loss: 7.8452\n",
      "Iteration 1018/1072, Loss: 7.7525\n",
      "Iteration 1019/1072, Loss: 7.8487\n",
      "Iteration 1020/1072, Loss: 7.7355\n",
      "Iteration 1021/1072, Loss: 7.7184\n",
      "Iteration 1022/1072, Loss: 7.8067\n",
      "Iteration 1023/1072, Loss: 7.9144\n",
      "Iteration 1024/1072, Loss: 7.7956\n",
      "Iteration 1025/1072, Loss: 7.7982\n",
      "Iteration 1026/1072, Loss: 7.8642\n",
      "Iteration 1027/1072, Loss: 7.7475\n",
      "Iteration 1028/1072, Loss: 7.7870\n",
      "Iteration 1029/1072, Loss: 7.7278\n",
      "Iteration 1030/1072, Loss: 7.7618\n",
      "Iteration 1031/1072, Loss: 7.7998\n",
      "Iteration 1032/1072, Loss: 7.8718\n",
      "Iteration 1033/1072, Loss: 7.7994\n",
      "Iteration 1034/1072, Loss: 7.8049\n",
      "Iteration 1035/1072, Loss: 7.7204\n",
      "Iteration 1036/1072, Loss: 7.8358\n",
      "Iteration 1037/1072, Loss: 7.7599\n",
      "Iteration 1038/1072, Loss: 7.7718\n",
      "Iteration 1039/1072, Loss: 7.6461\n",
      "Iteration 1040/1072, Loss: 7.6737\n",
      "Iteration 1041/1072, Loss: 7.7397\n",
      "Iteration 1042/1072, Loss: 7.7891\n",
      "Iteration 1043/1072, Loss: 7.7413\n",
      "Iteration 1044/1072, Loss: 7.8403\n",
      "Iteration 1045/1072, Loss: 7.7071\n",
      "Iteration 1046/1072, Loss: 7.7625\n",
      "Iteration 1047/1072, Loss: 7.6678\n",
      "Iteration 1048/1072, Loss: 7.6368\n",
      "Iteration 1049/1072, Loss: 7.6962\n",
      "Iteration 1050/1072, Loss: 7.7506\n",
      "Iteration 1051/1072, Loss: 7.7319\n",
      "Iteration 1052/1072, Loss: 7.7802\n",
      "Iteration 1053/1072, Loss: 7.5349\n",
      "Iteration 1054/1072, Loss: 7.6866\n",
      "Iteration 1055/1072, Loss: 7.7127\n",
      "Iteration 1056/1072, Loss: 7.7543\n",
      "Iteration 1057/1072, Loss: 7.8248\n",
      "Iteration 1058/1072, Loss: 7.6783\n",
      "Iteration 1059/1072, Loss: 7.7302\n",
      "Iteration 1060/1072, Loss: 7.8183\n",
      "Iteration 1061/1072, Loss: 7.8170\n",
      "Iteration 1062/1072, Loss: 7.7624\n",
      "Iteration 1063/1072, Loss: 7.7670\n",
      "Iteration 1064/1072, Loss: 7.7966\n",
      "Iteration 1065/1072, Loss: 7.8597\n",
      "Iteration 1066/1072, Loss: 7.7602\n",
      "Iteration 1067/1072, Loss: 7.8135\n",
      "Iteration 1068/1072, Loss: 7.8361\n",
      "Iteration 1069/1072, Loss: 7.6551\n",
      "Iteration 1070/1072, Loss: 7.7423\n",
      "Iteration 1071/1072, Loss: 7.7391\n",
      "Iteration 1072/1072, Loss: 7.9073\n",
      "Epoch 2/10, Loss: 7.8101\n",
      "Validation Accuracy: 3.35%\n",
      "Model checkpoint saved!\n",
      "Iteration 1/1072, Loss: 7.6555\n",
      "Iteration 2/1072, Loss: 7.5573\n",
      "Iteration 3/1072, Loss: 7.6079\n",
      "Iteration 4/1072, Loss: 7.5471\n",
      "Iteration 5/1072, Loss: 7.5817\n",
      "Iteration 6/1072, Loss: 7.5349\n",
      "Iteration 7/1072, Loss: 7.5759\n",
      "Iteration 8/1072, Loss: 7.5660\n",
      "Iteration 9/1072, Loss: 7.6233\n",
      "Iteration 10/1072, Loss: 7.6205\n",
      "Iteration 11/1072, Loss: 7.5854\n",
      "Iteration 12/1072, Loss: 7.5608\n",
      "Iteration 13/1072, Loss: 7.4507\n",
      "Iteration 14/1072, Loss: 7.5280\n",
      "Iteration 15/1072, Loss: 7.5441\n",
      "Iteration 16/1072, Loss: 7.4397\n",
      "Iteration 17/1072, Loss: 7.4809\n",
      "Iteration 18/1072, Loss: 7.5818\n",
      "Iteration 19/1072, Loss: 7.4859\n",
      "Iteration 20/1072, Loss: 7.4842\n",
      "Iteration 21/1072, Loss: 7.6120\n",
      "Iteration 22/1072, Loss: 7.5563\n",
      "Iteration 23/1072, Loss: 7.6217\n",
      "Iteration 24/1072, Loss: 7.4116\n",
      "Iteration 25/1072, Loss: 7.5157\n",
      "Iteration 26/1072, Loss: 7.6553\n",
      "Iteration 27/1072, Loss: 7.5174\n",
      "Iteration 28/1072, Loss: 7.5278\n",
      "Iteration 29/1072, Loss: 7.4790\n",
      "Iteration 30/1072, Loss: 7.4792\n",
      "Iteration 31/1072, Loss: 7.5294\n",
      "Iteration 32/1072, Loss: 7.5762\n",
      "Iteration 33/1072, Loss: 7.5803\n",
      "Iteration 34/1072, Loss: 7.5377\n",
      "Iteration 35/1072, Loss: 7.5350\n",
      "Iteration 36/1072, Loss: 7.6434\n",
      "Iteration 37/1072, Loss: 7.5048\n",
      "Iteration 38/1072, Loss: 7.5440\n",
      "Iteration 39/1072, Loss: 7.6423\n",
      "Iteration 40/1072, Loss: 7.4979\n",
      "Iteration 41/1072, Loss: 7.6388\n",
      "Iteration 42/1072, Loss: 7.3750\n",
      "Iteration 43/1072, Loss: 7.5674\n",
      "Iteration 44/1072, Loss: 7.5224\n",
      "Iteration 45/1072, Loss: 7.5600\n",
      "Iteration 46/1072, Loss: 7.5935\n",
      "Iteration 47/1072, Loss: 7.5390\n",
      "Iteration 48/1072, Loss: 7.6222\n",
      "Iteration 49/1072, Loss: 7.5662\n",
      "Iteration 50/1072, Loss: 7.4439\n",
      "Iteration 51/1072, Loss: 7.5559\n",
      "Iteration 52/1072, Loss: 7.5803\n",
      "Iteration 53/1072, Loss: 7.5506\n",
      "Iteration 54/1072, Loss: 7.6215\n",
      "Iteration 55/1072, Loss: 7.5739\n",
      "Iteration 56/1072, Loss: 7.3507\n",
      "Iteration 57/1072, Loss: 7.4967\n",
      "Iteration 58/1072, Loss: 7.6523\n",
      "Iteration 59/1072, Loss: 7.5406\n",
      "Iteration 60/1072, Loss: 7.6258\n",
      "Iteration 61/1072, Loss: 7.6700\n",
      "Iteration 62/1072, Loss: 7.5513\n",
      "Iteration 63/1072, Loss: 7.5020\n",
      "Iteration 64/1072, Loss: 7.5757\n",
      "Iteration 65/1072, Loss: 7.5162\n",
      "Iteration 66/1072, Loss: 7.4562\n",
      "Iteration 67/1072, Loss: 7.5103\n",
      "Iteration 68/1072, Loss: 7.5161\n",
      "Iteration 69/1072, Loss: 7.4951\n",
      "Iteration 70/1072, Loss: 7.5529\n",
      "Iteration 71/1072, Loss: 7.5402\n",
      "Iteration 72/1072, Loss: 7.5191\n",
      "Iteration 73/1072, Loss: 7.4585\n",
      "Iteration 74/1072, Loss: 7.6022\n",
      "Iteration 75/1072, Loss: 7.5114\n",
      "Iteration 76/1072, Loss: 7.5568\n",
      "Iteration 77/1072, Loss: 7.5528\n",
      "Iteration 78/1072, Loss: 7.5824\n",
      "Iteration 79/1072, Loss: 7.4956\n",
      "Iteration 80/1072, Loss: 7.5203\n",
      "Iteration 81/1072, Loss: 7.5267\n",
      "Iteration 82/1072, Loss: 7.5832\n",
      "Iteration 83/1072, Loss: 7.4856\n",
      "Iteration 84/1072, Loss: 7.5532\n",
      "Iteration 85/1072, Loss: 7.5045\n",
      "Iteration 86/1072, Loss: 7.6740\n",
      "Iteration 87/1072, Loss: 7.6477\n",
      "Iteration 88/1072, Loss: 7.4370\n",
      "Iteration 89/1072, Loss: 7.5718\n",
      "Iteration 90/1072, Loss: 7.5892\n",
      "Iteration 91/1072, Loss: 7.4460\n",
      "Iteration 92/1072, Loss: 7.4928\n",
      "Iteration 93/1072, Loss: 7.6009\n",
      "Iteration 94/1072, Loss: 7.5284\n",
      "Iteration 95/1072, Loss: 7.4146\n",
      "Iteration 96/1072, Loss: 7.5925\n",
      "Iteration 97/1072, Loss: 7.5275\n",
      "Iteration 98/1072, Loss: 7.6131\n",
      "Iteration 99/1072, Loss: 7.4907\n",
      "Iteration 366/1072, Loss: 7.6021\n",
      "Iteration 367/1072, Loss: 7.3773\n",
      "Iteration 368/1072, Loss: 7.4297\n",
      "Iteration 369/1072, Loss: 7.6418\n",
      "Iteration 370/1072, Loss: 7.4164\n",
      "Iteration 371/1072, Loss: 7.4118\n",
      "Iteration 372/1072, Loss: 7.4341\n",
      "Iteration 373/1072, Loss: 7.5638\n",
      "Iteration 374/1072, Loss: 7.6258\n",
      "Iteration 375/1072, Loss: 7.5690\n",
      "Iteration 376/1072, Loss: 7.3859\n",
      "Iteration 377/1072, Loss: 7.4879\n",
      "Iteration 378/1072, Loss: 7.4885\n",
      "Iteration 379/1072, Loss: 7.5103\n",
      "Iteration 380/1072, Loss: 7.6812\n",
      "Iteration 381/1072, Loss: 7.4789\n",
      "Iteration 382/1072, Loss: 7.5615\n",
      "Iteration 383/1072, Loss: 7.4266\n",
      "Iteration 384/1072, Loss: 7.3253\n",
      "Iteration 385/1072, Loss: 7.2153\n",
      "Iteration 386/1072, Loss: 7.4705\n",
      "Iteration 387/1072, Loss: 7.4957\n",
      "Iteration 388/1072, Loss: 7.5893\n",
      "Iteration 389/1072, Loss: 7.3225\n",
      "Iteration 390/1072, Loss: 7.5684\n",
      "Iteration 391/1072, Loss: 7.4684\n",
      "Iteration 392/1072, Loss: 7.6115\n",
      "Iteration 393/1072, Loss: 7.5197\n",
      "Iteration 394/1072, Loss: 7.5499\n",
      "Iteration 395/1072, Loss: 7.3900\n",
      "Iteration 396/1072, Loss: 7.3804\n",
      "Iteration 397/1072, Loss: 7.4334\n",
      "Iteration 398/1072, Loss: 7.5902\n",
      "Iteration 399/1072, Loss: 7.4586\n",
      "Iteration 400/1072, Loss: 7.4818\n",
      "Iteration 401/1072, Loss: 7.4458\n",
      "Iteration 402/1072, Loss: 7.4708\n",
      "Iteration 403/1072, Loss: 7.5294\n",
      "Iteration 404/1072, Loss: 7.5453\n",
      "Iteration 405/1072, Loss: 7.6118\n",
      "Iteration 406/1072, Loss: 7.4593\n",
      "Iteration 407/1072, Loss: 7.6779\n",
      "Iteration 408/1072, Loss: 7.5422\n",
      "Iteration 409/1072, Loss: 7.5638\n",
      "Iteration 410/1072, Loss: 7.5957\n",
      "Iteration 411/1072, Loss: 7.5706\n",
      "Iteration 412/1072, Loss: 7.5447\n",
      "Iteration 413/1072, Loss: 7.6739\n",
      "Iteration 414/1072, Loss: 7.5213\n",
      "Iteration 415/1072, Loss: 7.5382\n",
      "Iteration 416/1072, Loss: 7.4731\n",
      "Iteration 417/1072, Loss: 7.4578\n",
      "Iteration 418/1072, Loss: 7.5899\n",
      "Iteration 419/1072, Loss: 7.5472\n",
      "Iteration 420/1072, Loss: 7.6287\n",
      "Iteration 421/1072, Loss: 7.4136\n",
      "Iteration 422/1072, Loss: 7.6817\n",
      "Iteration 423/1072, Loss: 7.6354\n",
      "Iteration 424/1072, Loss: 7.6183\n",
      "Iteration 425/1072, Loss: 7.5002\n",
      "Iteration 426/1072, Loss: 7.5681\n",
      "Iteration 427/1072, Loss: 7.5973\n",
      "Iteration 428/1072, Loss: 7.5819\n",
      "Iteration 429/1072, Loss: 7.3107\n",
      "Iteration 430/1072, Loss: 7.3695\n",
      "Iteration 431/1072, Loss: 7.5619\n",
      "Iteration 432/1072, Loss: 7.6886\n",
      "Iteration 433/1072, Loss: 7.4220\n",
      "Iteration 434/1072, Loss: 7.5077\n",
      "Iteration 435/1072, Loss: 7.5858\n",
      "Iteration 436/1072, Loss: 7.5527\n",
      "Iteration 437/1072, Loss: 7.5443\n",
      "Iteration 438/1072, Loss: 7.2120\n",
      "Iteration 439/1072, Loss: 7.5327\n",
      "Iteration 440/1072, Loss: 7.6428\n",
      "Iteration 441/1072, Loss: 7.5350\n",
      "Iteration 442/1072, Loss: 7.3363\n",
      "Iteration 443/1072, Loss: 7.2651\n",
      "Iteration 444/1072, Loss: 7.5627\n",
      "Iteration 445/1072, Loss: 7.6211\n",
      "Iteration 446/1072, Loss: 7.5360\n",
      "Iteration 447/1072, Loss: 7.6843\n",
      "Iteration 448/1072, Loss: 7.5798\n",
      "Iteration 449/1072, Loss: 7.5375\n",
      "Iteration 450/1072, Loss: 7.5005\n",
      "Iteration 451/1072, Loss: 7.5760\n",
      "Iteration 452/1072, Loss: 7.7154\n",
      "Iteration 453/1072, Loss: 7.4289\n",
      "Iteration 454/1072, Loss: 7.5657\n",
      "Iteration 455/1072, Loss: 7.3507\n",
      "Iteration 456/1072, Loss: 7.4981\n",
      "Iteration 457/1072, Loss: 7.6060\n",
      "Iteration 458/1072, Loss: 7.3085\n",
      "Iteration 459/1072, Loss: 7.5093\n",
      "Iteration 460/1072, Loss: 7.3561\n",
      "Iteration 461/1072, Loss: 7.5421\n",
      "Iteration 462/1072, Loss: 7.4416\n",
      "Iteration 463/1072, Loss: 7.5286\n",
      "Iteration 464/1072, Loss: 7.4587\n",
      "Iteration 465/1072, Loss: 7.5230\n",
      "Iteration 466/1072, Loss: 7.5802\n",
      "Iteration 467/1072, Loss: 7.6930\n",
      "Iteration 468/1072, Loss: 7.5530\n",
      "Iteration 469/1072, Loss: 7.5237\n",
      "Iteration 470/1072, Loss: 7.5728\n",
      "Iteration 471/1072, Loss: 7.5777\n",
      "Iteration 472/1072, Loss: 7.2703\n",
      "Iteration 473/1072, Loss: 7.4488\n",
      "Iteration 474/1072, Loss: 7.4121\n",
      "Iteration 475/1072, Loss: 7.4967\n",
      "Iteration 476/1072, Loss: 7.5852\n",
      "Iteration 477/1072, Loss: 7.6108\n",
      "Iteration 478/1072, Loss: 7.6528\n",
      "Iteration 479/1072, Loss: 7.5955\n",
      "Iteration 480/1072, Loss: 7.5911\n",
      "Iteration 481/1072, Loss: 7.4509\n",
      "Iteration 482/1072, Loss: 7.4253\n",
      "Iteration 483/1072, Loss: 7.5822\n",
      "Iteration 484/1072, Loss: 7.6293\n",
      "Iteration 485/1072, Loss: 7.5324\n",
      "Iteration 486/1072, Loss: 7.5155\n",
      "Iteration 487/1072, Loss: 7.3730\n",
      "Iteration 488/1072, Loss: 7.6311\n",
      "Iteration 489/1072, Loss: 7.4357\n",
      "Iteration 490/1072, Loss: 7.4185\n",
      "Iteration 491/1072, Loss: 7.4529\n",
      "Iteration 492/1072, Loss: 7.2783\n",
      "Iteration 493/1072, Loss: 7.3941\n",
      "Iteration 494/1072, Loss: 7.6194\n",
      "Iteration 495/1072, Loss: 7.5596\n",
      "Iteration 496/1072, Loss: 7.4824\n",
      "Iteration 497/1072, Loss: 7.6051\n",
      "Iteration 498/1072, Loss: 7.4620\n",
      "Iteration 499/1072, Loss: 7.5307\n",
      "Iteration 500/1072, Loss: 7.5577\n",
      "Iteration 501/1072, Loss: 7.5428\n",
      "Iteration 502/1072, Loss: 7.5022\n",
      "Iteration 503/1072, Loss: 7.6218\n",
      "Iteration 504/1072, Loss: 7.6775\n",
      "Iteration 505/1072, Loss: 7.4637\n",
      "Iteration 506/1072, Loss: 7.4966\n",
      "Iteration 507/1072, Loss: 7.5111\n",
      "Iteration 508/1072, Loss: 7.6181\n",
      "Iteration 509/1072, Loss: 7.3892\n",
      "Iteration 510/1072, Loss: 7.4406\n",
      "Iteration 511/1072, Loss: 7.4826\n",
      "Iteration 512/1072, Loss: 7.4033\n",
      "Iteration 513/1072, Loss: 7.5041\n",
      "Iteration 514/1072, Loss: 7.3262\n",
      "Iteration 515/1072, Loss: 7.6218\n",
      "Iteration 516/1072, Loss: 7.4810\n",
      "Iteration 517/1072, Loss: 7.4691\n",
      "Iteration 518/1072, Loss: 7.4698\n",
      "Iteration 519/1072, Loss: 7.5712\n",
      "Iteration 520/1072, Loss: 7.4370\n",
      "Iteration 521/1072, Loss: 7.5494\n",
      "Iteration 522/1072, Loss: 7.5337\n",
      "Iteration 523/1072, Loss: 7.5611\n",
      "Iteration 524/1072, Loss: 7.4192\n",
      "Iteration 525/1072, Loss: 7.5313\n",
      "Iteration 526/1072, Loss: 7.5540\n",
      "Iteration 527/1072, Loss: 7.4359\n",
      "Iteration 528/1072, Loss: 7.4837\n",
      "Iteration 529/1072, Loss: 7.6726\n",
      "Iteration 530/1072, Loss: 7.3311\n",
      "Iteration 531/1072, Loss: 7.2763\n",
      "Iteration 532/1072, Loss: 7.6153\n",
      "Iteration 533/1072, Loss: 7.7042\n",
      "Iteration 534/1072, Loss: 7.5955\n",
      "Iteration 535/1072, Loss: 7.4853\n",
      "Iteration 536/1072, Loss: 7.4557\n",
      "Iteration 537/1072, Loss: 7.6094\n",
      "Iteration 538/1072, Loss: 7.4149\n",
      "Iteration 539/1072, Loss: 7.6798\n",
      "Iteration 540/1072, Loss: 7.4288\n",
      "Iteration 541/1072, Loss: 7.5014\n",
      "Iteration 542/1072, Loss: 7.4450\n",
      "Iteration 543/1072, Loss: 7.5172\n",
      "Iteration 544/1072, Loss: 7.5559\n",
      "Iteration 545/1072, Loss: 7.4807\n",
      "Iteration 546/1072, Loss: 7.5408\n",
      "Iteration 547/1072, Loss: 7.5849\n",
      "Iteration 548/1072, Loss: 7.5428\n",
      "Iteration 549/1072, Loss: 7.5573\n",
      "Iteration 550/1072, Loss: 7.3888\n",
      "Iteration 551/1072, Loss: 7.5679\n",
      "Iteration 552/1072, Loss: 7.5787\n",
      "Iteration 553/1072, Loss: 7.5265\n",
      "Iteration 554/1072, Loss: 7.5143\n",
      "Iteration 555/1072, Loss: 7.4369\n",
      "Iteration 556/1072, Loss: 7.3277\n",
      "Iteration 557/1072, Loss: 7.5311\n",
      "Iteration 558/1072, Loss: 7.4336\n",
      "Iteration 559/1072, Loss: 7.4970\n",
      "Iteration 560/1072, Loss: 7.4180\n",
      "Iteration 561/1072, Loss: 7.4605\n",
      "Iteration 562/1072, Loss: 7.5334\n",
      "Iteration 563/1072, Loss: 7.3706\n",
      "Iteration 564/1072, Loss: 7.5806\n",
      "Iteration 565/1072, Loss: 7.5788\n",
      "Iteration 566/1072, Loss: 7.2986\n",
      "Iteration 567/1072, Loss: 7.5449\n",
      "Iteration 568/1072, Loss: 7.4800\n",
      "Iteration 569/1072, Loss: 7.2994\n",
      "Iteration 570/1072, Loss: 7.3190\n",
      "Iteration 571/1072, Loss: 7.5366\n",
      "Iteration 572/1072, Loss: 7.5630\n",
      "Iteration 573/1072, Loss: 7.4399\n",
      "Iteration 574/1072, Loss: 7.3852\n",
      "Iteration 575/1072, Loss: 7.4866\n",
      "Iteration 576/1072, Loss: 7.4917\n",
      "Iteration 577/1072, Loss: 7.5213\n",
      "Iteration 578/1072, Loss: 7.3375\n",
      "Iteration 579/1072, Loss: 7.5902\n",
      "Iteration 580/1072, Loss: 7.5690\n",
      "Iteration 581/1072, Loss: 7.5919\n",
      "Iteration 582/1072, Loss: 7.5726\n",
      "Iteration 583/1072, Loss: 7.4744\n",
      "Iteration 584/1072, Loss: 7.6462\n",
      "Iteration 585/1072, Loss: 7.6461\n",
      "Iteration 586/1072, Loss: 7.5463\n",
      "Iteration 587/1072, Loss: 7.5730\n",
      "Iteration 588/1072, Loss: 7.3631\n",
      "Iteration 589/1072, Loss: 7.6390\n",
      "Iteration 590/1072, Loss: 7.2677\n",
      "Iteration 591/1072, Loss: 7.5811\n",
      "Iteration 592/1072, Loss: 7.5454\n",
      "Iteration 593/1072, Loss: 7.5684\n",
      "Iteration 594/1072, Loss: 7.4495\n",
      "Iteration 595/1072, Loss: 7.7443\n",
      "Iteration 596/1072, Loss: 7.5251\n",
      "Iteration 597/1072, Loss: 7.5074\n",
      "Iteration 598/1072, Loss: 7.5931\n",
      "Iteration 599/1072, Loss: 7.4834\n",
      "Iteration 600/1072, Loss: 7.2854\n",
      "Iteration 601/1072, Loss: 7.5146\n",
      "Iteration 602/1072, Loss: 7.3893\n",
      "Iteration 603/1072, Loss: 7.6458\n",
      "Iteration 604/1072, Loss: 7.5134\n",
      "Iteration 605/1072, Loss: 7.2624\n",
      "Iteration 606/1072, Loss: 7.4993\n",
      "Iteration 607/1072, Loss: 7.6030\n",
      "Iteration 608/1072, Loss: 7.3411\n",
      "Iteration 609/1072, Loss: 7.5005\n",
      "Iteration 610/1072, Loss: 7.4555\n",
      "Iteration 611/1072, Loss: 7.3576\n",
      "Iteration 612/1072, Loss: 7.5117\n",
      "Iteration 613/1072, Loss: 7.5458\n",
      "Iteration 614/1072, Loss: 7.5699\n",
      "Iteration 615/1072, Loss: 7.6388\n",
      "Iteration 616/1072, Loss: 7.2609\n",
      "Iteration 617/1072, Loss: 7.4638\n",
      "Iteration 618/1072, Loss: 7.4476\n",
      "Iteration 619/1072, Loss: 7.3282\n",
      "Iteration 620/1072, Loss: 7.5347\n",
      "Iteration 621/1072, Loss: 7.3715\n",
      "Iteration 622/1072, Loss: 7.6857\n",
      "Iteration 623/1072, Loss: 7.4268\n",
      "Iteration 624/1072, Loss: 7.5322\n",
      "Iteration 625/1072, Loss: 7.4530\n",
      "Iteration 626/1072, Loss: 7.6438\n",
      "Iteration 627/1072, Loss: 7.5776\n",
      "Iteration 628/1072, Loss: 7.5984\n",
      "Iteration 629/1072, Loss: 7.4906\n",
      "Iteration 630/1072, Loss: 7.5694\n",
      "Iteration 631/1072, Loss: 7.3942\n",
      "Iteration 632/1072, Loss: 7.4945\n",
      "Iteration 633/1072, Loss: 7.5960\n",
      "Iteration 634/1072, Loss: 7.5393\n",
      "Iteration 635/1072, Loss: 7.5472\n",
      "Iteration 636/1072, Loss: 7.4874\n",
      "Iteration 637/1072, Loss: 7.3931\n",
      "Iteration 638/1072, Loss: 7.4822\n",
      "Iteration 639/1072, Loss: 7.4151\n",
      "Iteration 640/1072, Loss: 7.4230\n",
      "Iteration 641/1072, Loss: 7.4736\n",
      "Iteration 642/1072, Loss: 7.6286\n",
      "Iteration 643/1072, Loss: 7.5055\n",
      "Iteration 644/1072, Loss: 7.3750\n",
      "Iteration 645/1072, Loss: 7.4077\n",
      "Iteration 646/1072, Loss: 7.4160\n",
      "Iteration 647/1072, Loss: 7.5392\n",
      "Iteration 648/1072, Loss: 7.5008\n",
      "Iteration 649/1072, Loss: 7.5407\n",
      "Iteration 650/1072, Loss: 7.4429\n",
      "Iteration 651/1072, Loss: 7.5293\n",
      "Iteration 652/1072, Loss: 7.5279\n",
      "Iteration 653/1072, Loss: 7.4406\n",
      "Iteration 654/1072, Loss: 7.4113\n",
      "Iteration 655/1072, Loss: 7.6886\n",
      "Iteration 656/1072, Loss: 7.5212\n",
      "Iteration 657/1072, Loss: 7.5082\n",
      "Iteration 658/1072, Loss: 7.5710\n",
      "Iteration 659/1072, Loss: 7.5603\n",
      "Iteration 660/1072, Loss: 7.3171\n",
      "Iteration 661/1072, Loss: 7.5471\n",
      "Iteration 662/1072, Loss: 7.3979\n",
      "Iteration 663/1072, Loss: 7.3825\n",
      "Iteration 664/1072, Loss: 7.5797\n",
      "Iteration 665/1072, Loss: 7.4407\n",
      "Iteration 666/1072, Loss: 7.4661\n",
      "Iteration 667/1072, Loss: 7.4904\n",
      "Iteration 668/1072, Loss: 7.4705\n",
      "Iteration 669/1072, Loss: 7.4108\n",
      "Iteration 670/1072, Loss: 7.4374\n",
      "Iteration 671/1072, Loss: 7.4425\n",
      "Iteration 672/1072, Loss: 7.4951\n",
      "Iteration 673/1072, Loss: 7.5326\n",
      "Iteration 674/1072, Loss: 7.2694\n",
      "Iteration 675/1072, Loss: 7.5754\n",
      "Iteration 676/1072, Loss: 7.5547\n",
      "Iteration 677/1072, Loss: 7.4572\n",
      "Iteration 678/1072, Loss: 7.6029\n",
      "Iteration 679/1072, Loss: 7.5326\n",
      "Iteration 680/1072, Loss: 7.6065\n",
      "Iteration 681/1072, Loss: 7.4978\n",
      "Iteration 682/1072, Loss: 7.5458\n",
      "Iteration 683/1072, Loss: 7.2796\n",
      "Iteration 684/1072, Loss: 7.4761\n",
      "Iteration 685/1072, Loss: 7.4823\n",
      "Iteration 686/1072, Loss: 7.4457\n",
      "Iteration 687/1072, Loss: 7.5680\n",
      "Iteration 688/1072, Loss: 7.7152\n",
      "Iteration 689/1072, Loss: 7.4912\n",
      "Iteration 690/1072, Loss: 7.4647\n",
      "Iteration 691/1072, Loss: 7.5322\n",
      "Iteration 692/1072, Loss: 7.4421\n",
      "Iteration 693/1072, Loss: 7.5601\n",
      "Iteration 694/1072, Loss: 7.6182\n",
      "Iteration 695/1072, Loss: 7.5116\n",
      "Iteration 696/1072, Loss: 7.4447\n",
      "Iteration 697/1072, Loss: 7.5993\n",
      "Iteration 698/1072, Loss: 7.3029\n",
      "Iteration 699/1072, Loss: 7.5675\n",
      "Iteration 700/1072, Loss: 7.5290\n",
      "Iteration 701/1072, Loss: 7.6450\n",
      "Iteration 702/1072, Loss: 7.5265\n",
      "Iteration 703/1072, Loss: 7.6263\n",
      "Iteration 704/1072, Loss: 7.3199\n",
      "Iteration 705/1072, Loss: 7.5811\n",
      "Iteration 706/1072, Loss: 7.4683\n",
      "Iteration 707/1072, Loss: 7.5421\n",
      "Iteration 708/1072, Loss: 7.6416\n",
      "Iteration 709/1072, Loss: 7.2684\n",
      "Iteration 710/1072, Loss: 7.0765\n",
      "Iteration 711/1072, Loss: 7.5019\n",
      "Iteration 712/1072, Loss: 7.3643\n",
      "Iteration 713/1072, Loss: 7.6095\n",
      "Iteration 714/1072, Loss: 7.4943\n",
      "Iteration 715/1072, Loss: 7.6136\n",
      "Iteration 716/1072, Loss: 7.5364\n",
      "Iteration 717/1072, Loss: 7.5198\n",
      "Iteration 718/1072, Loss: 7.4665\n",
      "Iteration 719/1072, Loss: 7.5458\n",
      "Iteration 720/1072, Loss: 7.6296\n",
      "Iteration 721/1072, Loss: 7.6370\n",
      "Iteration 722/1072, Loss: 7.4750\n",
      "Iteration 723/1072, Loss: 7.6443\n",
      "Iteration 724/1072, Loss: 7.3869\n",
      "Iteration 725/1072, Loss: 7.4247\n",
      "Iteration 726/1072, Loss: 7.5777\n",
      "Iteration 727/1072, Loss: 7.4759\n",
      "Iteration 728/1072, Loss: 7.3918\n",
      "Iteration 729/1072, Loss: 7.1325\n",
      "Iteration 730/1072, Loss: 7.3355\n",
      "Iteration 731/1072, Loss: 7.4897\n",
      "Iteration 732/1072, Loss: 7.6140\n",
      "Iteration 733/1072, Loss: 7.4491\n",
      "Iteration 734/1072, Loss: 7.4347\n",
      "Iteration 735/1072, Loss: 7.4363\n",
      "Iteration 736/1072, Loss: 7.5121\n",
      "Iteration 737/1072, Loss: 7.3515\n",
      "Iteration 738/1072, Loss: 7.4175\n",
      "Iteration 739/1072, Loss: 7.4963\n",
      "Iteration 740/1072, Loss: 7.5188\n",
      "Iteration 741/1072, Loss: 7.4677\n",
      "Iteration 742/1072, Loss: 7.4753\n",
      "Iteration 743/1072, Loss: 7.4574\n",
      "Iteration 744/1072, Loss: 7.3051\n",
      "Iteration 745/1072, Loss: 7.5143\n",
      "Iteration 746/1072, Loss: 7.5533\n",
      "Iteration 747/1072, Loss: 7.4712\n",
      "Iteration 748/1072, Loss: 7.4235\n",
      "Iteration 749/1072, Loss: 7.5024\n",
      "Iteration 750/1072, Loss: 7.4276\n",
      "Iteration 751/1072, Loss: 7.4830\n",
      "Iteration 752/1072, Loss: 7.6016\n",
      "Iteration 753/1072, Loss: 7.6138\n",
      "Iteration 754/1072, Loss: 7.3057\n",
      "Iteration 755/1072, Loss: 7.6203\n",
      "Iteration 756/1072, Loss: 7.4678\n",
      "Iteration 757/1072, Loss: 7.4446\n",
      "Iteration 758/1072, Loss: 7.5404\n",
      "Iteration 759/1072, Loss: 7.5555\n",
      "Iteration 760/1072, Loss: 7.5628\n",
      "Iteration 761/1072, Loss: 7.5123\n",
      "Iteration 762/1072, Loss: 7.5193\n",
      "Iteration 763/1072, Loss: 7.6742\n",
      "Iteration 764/1072, Loss: 7.4946\n",
      "Iteration 765/1072, Loss: 7.2127\n",
      "Iteration 766/1072, Loss: 7.4071\n",
      "Iteration 767/1072, Loss: 7.3844\n",
      "Iteration 768/1072, Loss: 7.4962\n",
      "Iteration 769/1072, Loss: 7.3585\n",
      "Iteration 770/1072, Loss: 7.4399\n",
      "Iteration 771/1072, Loss: 7.4248\n",
      "Iteration 772/1072, Loss: 7.4652\n",
      "Iteration 773/1072, Loss: 7.5736\n",
      "Iteration 774/1072, Loss: 7.4874\n",
      "Iteration 775/1072, Loss: 7.3494\n",
      "Iteration 776/1072, Loss: 7.2743\n",
      "Iteration 777/1072, Loss: 7.4936\n",
      "Iteration 778/1072, Loss: 7.5212\n",
      "Iteration 779/1072, Loss: 7.3751\n",
      "Iteration 780/1072, Loss: 7.4018\n",
      "Iteration 781/1072, Loss: 7.4872\n",
      "Iteration 782/1072, Loss: 7.5700\n",
      "Iteration 783/1072, Loss: 7.5284\n",
      "Iteration 784/1072, Loss: 7.4814\n",
      "Iteration 785/1072, Loss: 7.4971\n",
      "Iteration 786/1072, Loss: 7.4087\n",
      "Iteration 787/1072, Loss: 7.4869\n",
      "Iteration 788/1072, Loss: 7.4944\n",
      "Iteration 789/1072, Loss: 7.4467\n",
      "Iteration 790/1072, Loss: 7.5458\n",
      "Iteration 791/1072, Loss: 7.5706\n",
      "Iteration 792/1072, Loss: 7.3825\n",
      "Iteration 793/1072, Loss: 7.5362\n",
      "Iteration 794/1072, Loss: 7.5818\n",
      "Iteration 795/1072, Loss: 7.5020\n",
      "Iteration 796/1072, Loss: 7.4941\n",
      "Iteration 797/1072, Loss: 7.3564\n",
      "Iteration 798/1072, Loss: 7.2184\n",
      "Iteration 799/1072, Loss: 7.5727\n",
      "Iteration 800/1072, Loss: 7.4817\n",
      "Iteration 801/1072, Loss: 7.5234\n",
      "Iteration 802/1072, Loss: 7.2439\n",
      "Iteration 803/1072, Loss: 7.4431\n",
      "Iteration 804/1072, Loss: 7.3623\n",
      "Iteration 805/1072, Loss: 7.3683\n",
      "Iteration 806/1072, Loss: 7.3040\n",
      "Iteration 807/1072, Loss: 7.4651\n",
      "Iteration 808/1072, Loss: 7.6808\n",
      "Iteration 809/1072, Loss: 7.5381\n",
      "Iteration 810/1072, Loss: 7.4395\n",
      "Iteration 811/1072, Loss: 7.5941\n",
      "Iteration 812/1072, Loss: 7.4720\n",
      "Iteration 813/1072, Loss: 7.4031\n",
      "Iteration 814/1072, Loss: 7.4642\n",
      "Iteration 815/1072, Loss: 7.4770\n",
      "Iteration 816/1072, Loss: 7.4654\n",
      "Iteration 817/1072, Loss: 7.5539\n",
      "Iteration 818/1072, Loss: 7.5463\n",
      "Iteration 819/1072, Loss: 7.5642\n",
      "Iteration 820/1072, Loss: 7.3052\n",
      "Iteration 821/1072, Loss: 7.4053\n",
      "Iteration 822/1072, Loss: 7.4605\n",
      "Iteration 823/1072, Loss: 7.3683\n",
      "Iteration 824/1072, Loss: 7.5208\n",
      "Iteration 825/1072, Loss: 7.5205\n",
      "Iteration 826/1072, Loss: 7.3739\n",
      "Iteration 827/1072, Loss: 7.4504\n",
      "Iteration 828/1072, Loss: 7.5878\n",
      "Iteration 829/1072, Loss: 7.5027\n",
      "Iteration 830/1072, Loss: 7.5330\n",
      "Iteration 831/1072, Loss: 7.5832\n",
      "Iteration 832/1072, Loss: 7.4410\n",
      "Iteration 833/1072, Loss: 7.5527\n",
      "Iteration 834/1072, Loss: 7.4504\n",
      "Iteration 835/1072, Loss: 7.3230\n",
      "Iteration 836/1072, Loss: 7.6644\n",
      "Iteration 837/1072, Loss: 7.5181\n",
      "Iteration 838/1072, Loss: 7.5324\n",
      "Iteration 839/1072, Loss: 7.5216\n",
      "Iteration 840/1072, Loss: 7.4004\n",
      "Iteration 841/1072, Loss: 7.3195\n",
      "Iteration 842/1072, Loss: 7.4496\n",
      "Iteration 843/1072, Loss: 7.3993\n",
      "Iteration 844/1072, Loss: 7.3834\n",
      "Iteration 845/1072, Loss: 7.2798\n",
      "Iteration 846/1072, Loss: 7.3976\n",
      "Iteration 847/1072, Loss: 7.6897\n",
      "Iteration 848/1072, Loss: 7.4663\n",
      "Iteration 849/1072, Loss: 7.5946\n",
      "Iteration 850/1072, Loss: 7.3876\n",
      "Iteration 851/1072, Loss: 7.3698\n",
      "Iteration 852/1072, Loss: 7.4105\n",
      "Iteration 853/1072, Loss: 7.4141\n",
      "Iteration 854/1072, Loss: 7.3726\n",
      "Iteration 855/1072, Loss: 7.4581\n",
      "Iteration 856/1072, Loss: 7.6683\n",
      "Iteration 857/1072, Loss: 7.2911\n",
      "Iteration 858/1072, Loss: 7.4979\n",
      "Iteration 859/1072, Loss: 7.3616\n",
      "Iteration 860/1072, Loss: 7.4921\n",
      "Iteration 861/1072, Loss: 7.3697\n",
      "Iteration 862/1072, Loss: 7.4923\n",
      "Iteration 863/1072, Loss: 7.2375\n",
      "Iteration 864/1072, Loss: 7.5015\n",
      "Iteration 865/1072, Loss: 7.4535\n",
      "Iteration 866/1072, Loss: 7.6018\n",
      "Iteration 867/1072, Loss: 7.3362\n",
      "Iteration 868/1072, Loss: 7.4413\n",
      "Iteration 869/1072, Loss: 7.4986\n",
      "Iteration 870/1072, Loss: 7.6106\n",
      "Iteration 871/1072, Loss: 7.3795\n",
      "Iteration 872/1072, Loss: 7.4921\n",
      "Iteration 873/1072, Loss: 7.4697\n",
      "Iteration 874/1072, Loss: 7.4091\n",
      "Iteration 875/1072, Loss: 7.4295\n",
      "Iteration 876/1072, Loss: 7.5252\n",
      "Iteration 877/1072, Loss: 7.4288\n",
      "Iteration 878/1072, Loss: 7.5611\n",
      "Iteration 879/1072, Loss: 7.5403\n",
      "Iteration 880/1072, Loss: 7.3189\n",
      "Iteration 881/1072, Loss: 7.4752\n",
      "Iteration 882/1072, Loss: 7.2167\n",
      "Iteration 883/1072, Loss: 7.2207\n",
      "Iteration 884/1072, Loss: 7.2282\n",
      "Iteration 885/1072, Loss: 7.3077\n",
      "Iteration 886/1072, Loss: 7.4354\n",
      "Iteration 887/1072, Loss: 7.3453\n",
      "Iteration 888/1072, Loss: 7.5563\n",
      "Iteration 889/1072, Loss: 7.5728\n",
      "Iteration 890/1072, Loss: 7.5480\n",
      "Iteration 891/1072, Loss: 7.5877\n",
      "Iteration 892/1072, Loss: 7.2985\n",
      "Iteration 893/1072, Loss: 7.3760\n",
      "Iteration 894/1072, Loss: 7.4680\n",
      "Iteration 895/1072, Loss: 7.4270\n",
      "Iteration 896/1072, Loss: 7.7412\n",
      "Iteration 897/1072, Loss: 7.4415\n",
      "Iteration 898/1072, Loss: 7.4529\n",
      "Iteration 899/1072, Loss: 7.3773\n",
      "Iteration 900/1072, Loss: 7.2996\n",
      "Iteration 901/1072, Loss: 7.4783\n",
      "Iteration 902/1072, Loss: 7.5489\n",
      "Iteration 903/1072, Loss: 7.4267\n",
      "Iteration 904/1072, Loss: 7.4473\n",
      "Iteration 905/1072, Loss: 7.4479\n",
      "Iteration 906/1072, Loss: 7.3941\n",
      "Iteration 907/1072, Loss: 7.2694\n",
      "Iteration 908/1072, Loss: 7.3956\n",
      "Iteration 909/1072, Loss: 7.3585\n",
      "Iteration 910/1072, Loss: 7.5274\n",
      "Iteration 911/1072, Loss: 7.4333\n",
      "Iteration 912/1072, Loss: 7.5290\n",
      "Iteration 913/1072, Loss: 7.3348\n",
      "Iteration 914/1072, Loss: 7.3463\n",
      "Iteration 915/1072, Loss: 7.3324\n",
      "Iteration 916/1072, Loss: 7.3555\n",
      "Iteration 917/1072, Loss: 7.3661\n",
      "Iteration 918/1072, Loss: 7.3457\n",
      "Iteration 919/1072, Loss: 7.4904\n",
      "Iteration 920/1072, Loss: 7.6211\n",
      "Iteration 921/1072, Loss: 7.3650\n",
      "Iteration 922/1072, Loss: 7.3357\n",
      "Iteration 923/1072, Loss: 7.2474\n",
      "Iteration 924/1072, Loss: 7.2488\n",
      "Iteration 925/1072, Loss: 7.5834\n",
      "Iteration 926/1072, Loss: 7.4728\n",
      "Iteration 927/1072, Loss: 7.3435\n",
      "Iteration 928/1072, Loss: 7.5508\n",
      "Iteration 929/1072, Loss: 7.4254\n",
      "Iteration 930/1072, Loss: 7.2142\n",
      "Iteration 931/1072, Loss: 7.6427\n",
      "Iteration 932/1072, Loss: 7.3382\n",
      "Iteration 933/1072, Loss: 7.2738\n",
      "Iteration 934/1072, Loss: 7.3492\n",
      "Iteration 935/1072, Loss: 7.6450\n",
      "Iteration 936/1072, Loss: 7.4984\n",
      "Iteration 937/1072, Loss: 7.4135\n",
      "Iteration 938/1072, Loss: 7.5179\n",
      "Iteration 939/1072, Loss: 7.4894\n",
      "Iteration 940/1072, Loss: 7.3469\n",
      "Iteration 941/1072, Loss: 7.4647\n",
      "Iteration 942/1072, Loss: 7.3651\n",
      "Iteration 943/1072, Loss: 7.2779\n",
      "Iteration 944/1072, Loss: 7.4439\n",
      "Iteration 945/1072, Loss: 7.3939\n",
      "Iteration 946/1072, Loss: 7.1593\n",
      "Iteration 947/1072, Loss: 7.5638\n",
      "Iteration 948/1072, Loss: 7.4348\n",
      "Iteration 949/1072, Loss: 7.3261\n",
      "Iteration 950/1072, Loss: 7.3657\n",
      "Iteration 951/1072, Loss: 7.6085\n",
      "Iteration 952/1072, Loss: 7.2479\n",
      "Iteration 953/1072, Loss: 7.2871\n",
      "Iteration 954/1072, Loss: 7.4700\n",
      "Iteration 955/1072, Loss: 7.5865\n",
      "Iteration 956/1072, Loss: 7.4193\n",
      "Iteration 957/1072, Loss: 7.4389\n",
      "Iteration 958/1072, Loss: 7.4565\n",
      "Iteration 959/1072, Loss: 7.4191\n",
      "Iteration 960/1072, Loss: 7.4063\n",
      "Iteration 961/1072, Loss: 7.3994\n",
      "Iteration 962/1072, Loss: 7.3202\n",
      "Iteration 963/1072, Loss: 7.2595\n",
      "Iteration 964/1072, Loss: 7.5601\n",
      "Iteration 965/1072, Loss: 7.2922\n",
      "Iteration 966/1072, Loss: 7.3556\n",
      "Iteration 967/1072, Loss: 7.3515\n",
      "Iteration 968/1072, Loss: 7.2849\n",
      "Iteration 969/1072, Loss: 7.3429\n",
      "Iteration 970/1072, Loss: 7.4759\n",
      "Iteration 971/1072, Loss: 7.2970\n",
      "Iteration 972/1072, Loss: 7.4855\n",
      "Iteration 973/1072, Loss: 7.2641\n",
      "Iteration 974/1072, Loss: 7.1802\n",
      "Iteration 975/1072, Loss: 7.4143\n",
      "Iteration 976/1072, Loss: 7.3745\n",
      "Iteration 977/1072, Loss: 7.5117\n",
      "Iteration 978/1072, Loss: 7.6112\n",
      "Iteration 979/1072, Loss: 7.6134\n",
      "Iteration 980/1072, Loss: 7.3364\n",
      "Iteration 981/1072, Loss: 7.3380\n",
      "Iteration 982/1072, Loss: 7.2112\n",
      "Iteration 983/1072, Loss: 7.4416\n",
      "Iteration 984/1072, Loss: 7.3139\n",
      "Iteration 985/1072, Loss: 7.4798\n",
      "Iteration 986/1072, Loss: 7.4677\n",
      "Iteration 987/1072, Loss: 7.3947\n",
      "Iteration 988/1072, Loss: 7.7351\n",
      "Iteration 989/1072, Loss: 7.4465\n",
      "Iteration 990/1072, Loss: 7.2282\n",
      "Iteration 991/1072, Loss: 7.5282\n",
      "Iteration 992/1072, Loss: 7.3724\n",
      "Iteration 993/1072, Loss: 7.3559\n",
      "Iteration 994/1072, Loss: 7.4834\n",
      "Iteration 995/1072, Loss: 7.3917\n",
      "Iteration 996/1072, Loss: 7.4366\n",
      "Iteration 997/1072, Loss: 7.3791\n",
      "Iteration 998/1072, Loss: 7.3369\n",
      "Iteration 999/1072, Loss: 7.3787\n",
      "Iteration 1000/1072, Loss: 7.4446\n",
      "Iteration 1001/1072, Loss: 7.5135\n",
      "Iteration 1002/1072, Loss: 7.3478\n",
      "Iteration 1003/1072, Loss: 7.4598\n",
      "Iteration 1004/1072, Loss: 7.4748\n",
      "Iteration 1005/1072, Loss: 7.3217\n",
      "Iteration 1006/1072, Loss: 7.3863\n",
      "Iteration 1007/1072, Loss: 7.2143\n",
      "Iteration 1008/1072, Loss: 7.3868\n",
      "Iteration 1009/1072, Loss: 7.4905\n",
      "Iteration 1010/1072, Loss: 7.3649\n",
      "Iteration 1011/1072, Loss: 7.5143\n",
      "Iteration 1012/1072, Loss: 7.4459\n",
      "Iteration 1013/1072, Loss: 7.4633\n",
      "Iteration 1014/1072, Loss: 7.5417\n",
      "Iteration 1015/1072, Loss: 7.3458\n",
      "Iteration 1016/1072, Loss: 7.2308\n",
      "Iteration 1017/1072, Loss: 7.4317\n",
      "Iteration 1018/1072, Loss: 7.4271\n",
      "Iteration 1019/1072, Loss: 7.4644\n",
      "Iteration 1020/1072, Loss: 7.2724\n",
      "Iteration 1021/1072, Loss: 7.3523\n",
      "Iteration 1022/1072, Loss: 7.3154\n",
      "Iteration 1023/1072, Loss: 7.3261\n",
      "Iteration 1024/1072, Loss: 7.3391\n",
      "Iteration 1025/1072, Loss: 7.4623\n",
      "Iteration 1026/1072, Loss: 7.5734\n",
      "Iteration 1027/1072, Loss: 7.1915\n",
      "Iteration 1028/1072, Loss: 7.5393\n",
      "Iteration 1029/1072, Loss: 7.2934\n",
      "Iteration 1030/1072, Loss: 7.4107\n",
      "Iteration 1031/1072, Loss: 7.4206\n",
      "Iteration 1032/1072, Loss: 7.5160\n",
      "Iteration 1033/1072, Loss: 7.4049\n",
      "Iteration 1034/1072, Loss: 7.2664\n",
      "Iteration 1035/1072, Loss: 7.2012\n",
      "Iteration 1036/1072, Loss: 7.3895\n",
      "Iteration 1037/1072, Loss: 7.4939\n",
      "Iteration 1038/1072, Loss: 7.4660\n",
      "Iteration 1039/1072, Loss: 7.2858\n",
      "Iteration 1040/1072, Loss: 7.2832\n",
      "Iteration 1041/1072, Loss: 7.3438\n",
      "Iteration 1042/1072, Loss: 7.4044\n",
      "Iteration 1043/1072, Loss: 7.4413\n",
      "Iteration 1044/1072, Loss: 7.4798\n",
      "Iteration 1045/1072, Loss: 7.2976\n",
      "Iteration 1046/1072, Loss: 7.4800\n",
      "Iteration 1047/1072, Loss: 7.4222\n",
      "Iteration 1048/1072, Loss: 7.4243\n",
      "Iteration 1049/1072, Loss: 7.3889\n",
      "Iteration 1050/1072, Loss: 7.4249\n",
      "Iteration 1051/1072, Loss: 7.3350\n",
      "Iteration 1052/1072, Loss: 7.4258\n",
      "Iteration 1053/1072, Loss: 7.3746\n",
      "Iteration 1054/1072, Loss: 7.4512\n",
      "Iteration 1055/1072, Loss: 7.4494\n",
      "Iteration 1056/1072, Loss: 7.2887\n",
      "Iteration 1057/1072, Loss: 7.3229\n",
      "Iteration 1058/1072, Loss: 7.2933\n",
      "Iteration 1059/1072, Loss: 7.5060\n",
      "Iteration 1060/1072, Loss: 7.3078\n",
      "Iteration 1061/1072, Loss: 7.4787\n",
      "Iteration 1062/1072, Loss: 7.5586\n",
      "Iteration 1063/1072, Loss: 7.4041\n",
      "Iteration 1064/1072, Loss: 7.4616\n",
      "Iteration 1065/1072, Loss: 7.4994\n",
      "Iteration 1066/1072, Loss: 7.2083\n",
      "Iteration 1067/1072, Loss: 7.6349\n",
      "Iteration 1068/1072, Loss: 7.5093\n",
      "Iteration 1069/1072, Loss: 7.2775\n",
      "Iteration 1070/1072, Loss: 7.3962\n",
      "Iteration 1071/1072, Loss: 7.4044\n",
      "Iteration 1072/1072, Loss: 7.7517\n",
      "Epoch 3/10, Loss: 7.4912\n",
      "Iteration 516/1072, Loss: 7.3138\n",
      "Iteration 517/1072, Loss: 7.3029\n",
      "Iteration 518/1072, Loss: 6.9420\n",
      "Iteration 519/1072, Loss: 7.1889\n",
      "Iteration 520/1072, Loss: 7.0611\n",
      "Iteration 521/1072, Loss: 7.1226\n",
      "Iteration 522/1072, Loss: 7.1379\n",
      "Iteration 523/1072, Loss: 7.2903\n",
      "Iteration 524/1072, Loss: 7.1493\n",
      "Iteration 525/1072, Loss: 7.3313\n",
      "Iteration 526/1072, Loss: 7.2001\n",
      "Iteration 527/1072, Loss: 7.0405\n",
      "Iteration 528/1072, Loss: 7.4016\n",
      "Iteration 529/1072, Loss: 6.9859\n",
      "Iteration 530/1072, Loss: 7.2506\n",
      "Iteration 531/1072, Loss: 6.9095\n",
      "Iteration 532/1072, Loss: 7.1952\n",
      "Iteration 533/1072, Loss: 7.3337\n",
      "Iteration 534/1072, Loss: 7.0012\n",
      "Iteration 535/1072, Loss: 7.3550\n",
      "Iteration 536/1072, Loss: 7.0029\n",
      "Iteration 537/1072, Loss: 7.3658\n",
      "Iteration 538/1072, Loss: 7.1168\n",
      "Iteration 539/1072, Loss: 7.0568\n",
      "Iteration 540/1072, Loss: 7.1628\n",
      "Iteration 541/1072, Loss: 7.0780\n",
      "Iteration 542/1072, Loss: 7.4955\n",
      "Iteration 543/1072, Loss: 7.1815\n",
      "Iteration 544/1072, Loss: 7.0325\n",
      "Iteration 545/1072, Loss: 6.7993\n",
      "Iteration 546/1072, Loss: 6.9007\n",
      "Iteration 547/1072, Loss: 7.0679\n",
      "Iteration 548/1072, Loss: 7.1905\n",
      "Iteration 549/1072, Loss: 7.1338\n",
      "Iteration 550/1072, Loss: 7.0530\n",
      "Iteration 551/1072, Loss: 6.8066\n",
      "Iteration 552/1072, Loss: 7.4423\n",
      "Iteration 553/1072, Loss: 7.0101\n",
      "Iteration 554/1072, Loss: 7.1627\n",
      "Iteration 555/1072, Loss: 7.0351\n",
      "Iteration 556/1072, Loss: 7.0898\n",
      "Iteration 557/1072, Loss: 7.2835\n",
      "Iteration 558/1072, Loss: 7.3476\n",
      "Iteration 559/1072, Loss: 7.4085\n",
      "Iteration 560/1072, Loss: 7.1271\n",
      "Iteration 561/1072, Loss: 7.1808\n",
      "Iteration 562/1072, Loss: 7.1432\n",
      "Iteration 563/1072, Loss: 6.9998\n",
      "Iteration 564/1072, Loss: 7.0685\n",
      "Iteration 565/1072, Loss: 6.9999\n",
      "Iteration 566/1072, Loss: 6.8024\n",
      "Iteration 567/1072, Loss: 7.2319\n",
      "Iteration 568/1072, Loss: 7.1575\n",
      "Iteration 569/1072, Loss: 7.2696\n",
      "Iteration 570/1072, Loss: 7.2052\n",
      "Iteration 571/1072, Loss: 7.1829\n",
      "Iteration 572/1072, Loss: 7.0097\n",
      "Iteration 573/1072, Loss: 6.9467\n",
      "Iteration 574/1072, Loss: 7.3229\n",
      "Iteration 575/1072, Loss: 7.0087\n",
      "Iteration 576/1072, Loss: 6.9890\n",
      "Iteration 577/1072, Loss: 7.2643\n",
      "Iteration 578/1072, Loss: 7.0984\n",
      "Iteration 579/1072, Loss: 7.1520\n",
      "Iteration 580/1072, Loss: 7.1557\n",
      "Iteration 581/1072, Loss: 7.1010\n",
      "Iteration 582/1072, Loss: 7.1305\n",
      "Iteration 583/1072, Loss: 7.0695\n",
      "Iteration 584/1072, Loss: 6.9238\n",
      "Iteration 585/1072, Loss: 7.2766\n",
      "Iteration 586/1072, Loss: 7.0476\n",
      "Iteration 587/1072, Loss: 6.7974\n",
      "Iteration 588/1072, Loss: 7.1739\n",
      "Iteration 589/1072, Loss: 6.9036\n",
      "Iteration 590/1072, Loss: 7.0456\n",
      "Iteration 591/1072, Loss: 7.1152\n",
      "Iteration 592/1072, Loss: 7.2694\n",
      "Iteration 593/1072, Loss: 7.1744\n",
      "Iteration 594/1072, Loss: 7.0123\n",
      "Iteration 595/1072, Loss: 6.9962\n",
      "Iteration 596/1072, Loss: 6.8905\n",
      "Iteration 597/1072, Loss: 7.3051\n",
      "Iteration 598/1072, Loss: 7.0382\n",
      "Iteration 599/1072, Loss: 6.9812\n",
      "Iteration 600/1072, Loss: 7.2547\n",
      "Iteration 601/1072, Loss: 7.2258\n",
      "Iteration 602/1072, Loss: 7.2931\n",
      "Iteration 603/1072, Loss: 7.1107\n",
      "Iteration 604/1072, Loss: 7.1427\n",
      "Iteration 605/1072, Loss: 7.1373\n",
      "Iteration 606/1072, Loss: 7.0204\n",
      "Iteration 607/1072, Loss: 7.0491\n",
      "Iteration 608/1072, Loss: 7.2712\n",
      "Iteration 609/1072, Loss: 7.0972\n",
      "Iteration 610/1072, Loss: 6.9937\n",
      "Iteration 611/1072, Loss: 7.0445\n",
      "Iteration 612/1072, Loss: 6.8269\n",
      "Iteration 613/1072, Loss: 7.0568\n",
      "Iteration 614/1072, Loss: 6.9797\n",
      "Iteration 615/1072, Loss: 7.1140\n",
      "Iteration 616/1072, Loss: 7.3448\n",
      "Iteration 617/1072, Loss: 7.0877\n",
      "Iteration 618/1072, Loss: 7.1056\n",
      "Iteration 619/1072, Loss: 7.2275\n",
      "Iteration 620/1072, Loss: 7.1031\n",
      "Iteration 621/1072, Loss: 7.0620\n",
      "Iteration 622/1072, Loss: 7.1439\n",
      "Iteration 623/1072, Loss: 7.1481\n",
      "Iteration 624/1072, Loss: 6.9180\n",
      "Iteration 625/1072, Loss: 7.2732\n",
      "Iteration 626/1072, Loss: 6.8195\n",
      "Iteration 627/1072, Loss: 7.0911\n",
      "Iteration 628/1072, Loss: 6.9559\n",
      "Iteration 629/1072, Loss: 7.3275\n",
      "Iteration 630/1072, Loss: 6.9104\n",
      "Iteration 631/1072, Loss: 7.0723\n",
      "Iteration 632/1072, Loss: 7.1052\n",
      "Iteration 633/1072, Loss: 6.8337\n",
      "Iteration 634/1072, Loss: 6.8436\n",
      "Iteration 635/1072, Loss: 7.2193\n",
      "Iteration 636/1072, Loss: 7.0288\n",
      "Iteration 637/1072, Loss: 7.0635\n",
      "Iteration 638/1072, Loss: 6.9188\n",
      "Iteration 639/1072, Loss: 7.2707\n",
      "Iteration 640/1072, Loss: 7.1136\n",
      "Iteration 641/1072, Loss: 6.8487\n",
      "Iteration 642/1072, Loss: 7.0545\n",
      "Iteration 643/1072, Loss: 7.0196\n",
      "Iteration 644/1072, Loss: 7.0028\n",
      "Iteration 645/1072, Loss: 7.1427\n",
      "Iteration 646/1072, Loss: 7.0671\n",
      "Iteration 647/1072, Loss: 6.9677\n",
      "Iteration 648/1072, Loss: 7.0871\n",
      "Iteration 649/1072, Loss: 7.2026\n",
      "Iteration 650/1072, Loss: 7.1963\n",
      "Iteration 651/1072, Loss: 7.2392\n",
      "Iteration 652/1072, Loss: 6.9443\n",
      "Iteration 653/1072, Loss: 6.8569\n",
      "Iteration 654/1072, Loss: 7.2632\n",
      "Iteration 655/1072, Loss: 7.1091\n",
      "Iteration 656/1072, Loss: 6.8737\n",
      "Iteration 657/1072, Loss: 7.1937\n",
      "Iteration 658/1072, Loss: 7.1135\n",
      "Iteration 659/1072, Loss: 7.1649\n",
      "Iteration 660/1072, Loss: 7.0470\n",
      "Iteration 661/1072, Loss: 7.2889\n",
      "Iteration 662/1072, Loss: 7.2551\n",
      "Iteration 663/1072, Loss: 7.0006\n",
      "Iteration 664/1072, Loss: 7.0566\n",
      "Iteration 665/1072, Loss: 7.0543\n",
      "Iteration 666/1072, Loss: 7.1523\n",
      "Iteration 667/1072, Loss: 7.2465\n",
      "Iteration 668/1072, Loss: 7.1686\n",
      "Iteration 669/1072, Loss: 7.3020\n",
      "Iteration 670/1072, Loss: 7.1389\n",
      "Iteration 671/1072, Loss: 7.0411\n",
      "Iteration 672/1072, Loss: 7.1167\n",
      "Iteration 673/1072, Loss: 7.1604\n",
      "Iteration 674/1072, Loss: 7.1517\n",
      "Iteration 675/1072, Loss: 6.8015\n",
      "Iteration 676/1072, Loss: 6.9701\n",
      "Iteration 677/1072, Loss: 7.1598\n",
      "Iteration 678/1072, Loss: 6.9369\n",
      "Iteration 679/1072, Loss: 7.1391\n",
      "Iteration 680/1072, Loss: 7.2771\n",
      "Iteration 681/1072, Loss: 7.1509\n",
      "Iteration 682/1072, Loss: 6.8086\n",
      "Iteration 683/1072, Loss: 7.0865\n",
      "Iteration 684/1072, Loss: 7.0112\n",
      "Iteration 685/1072, Loss: 7.2762\n",
      "Iteration 686/1072, Loss: 6.9389\n",
      "Iteration 687/1072, Loss: 7.1443\n",
      "Iteration 688/1072, Loss: 6.9784\n",
      "Iteration 689/1072, Loss: 7.2849\n",
      "Iteration 690/1072, Loss: 7.1456\n",
      "Iteration 691/1072, Loss: 7.0540\n",
      "Iteration 692/1072, Loss: 7.0007\n",
      "Iteration 693/1072, Loss: 6.9927\n",
      "Iteration 694/1072, Loss: 7.1748\n",
      "Iteration 695/1072, Loss: 7.2122\n",
      "Iteration 696/1072, Loss: 7.0455\n",
      "Iteration 697/1072, Loss: 7.0306\n",
      "Iteration 698/1072, Loss: 6.7683\n",
      "Iteration 699/1072, Loss: 7.1106\n",
      "Iteration 700/1072, Loss: 7.2162\n",
      "Iteration 701/1072, Loss: 7.1050\n",
      "Iteration 702/1072, Loss: 7.0782\n",
      "Iteration 703/1072, Loss: 7.3172\n",
      "Iteration 704/1072, Loss: 7.2541\n",
      "Iteration 705/1072, Loss: 7.0643\n",
      "Iteration 706/1072, Loss: 7.0802\n",
      "Iteration 707/1072, Loss: 7.2600\n",
      "Iteration 708/1072, Loss: 7.1773\n",
      "Iteration 709/1072, Loss: 7.1568\n",
      "Iteration 710/1072, Loss: 7.1282\n",
      "Iteration 711/1072, Loss: 7.1034\n",
      "Iteration 712/1072, Loss: 7.1787\n",
      "Iteration 713/1072, Loss: 7.2067\n",
      "Iteration 714/1072, Loss: 6.9580\n",
      "Iteration 715/1072, Loss: 7.1576\n",
      "Iteration 716/1072, Loss: 6.9962\n",
      "Iteration 717/1072, Loss: 7.2614\n",
      "Iteration 718/1072, Loss: 7.2251\n",
      "Iteration 719/1072, Loss: 6.8574\n",
      "Iteration 720/1072, Loss: 7.0078\n",
      "Iteration 721/1072, Loss: 7.0110\n",
      "Iteration 722/1072, Loss: 7.1396\n",
      "Iteration 723/1072, Loss: 6.9956\n",
      "Iteration 724/1072, Loss: 7.0050\n",
      "Iteration 725/1072, Loss: 6.8114\n",
      "Iteration 726/1072, Loss: 7.1173\n",
      "Iteration 727/1072, Loss: 6.9894\n",
      "Iteration 728/1072, Loss: 7.0505\n",
      "Iteration 729/1072, Loss: 7.3456\n",
      "Iteration 730/1072, Loss: 7.2127\n",
      "Iteration 731/1072, Loss: 7.2061\n",
      "Iteration 732/1072, Loss: 7.3677\n",
      "Iteration 733/1072, Loss: 7.4077\n",
      "Iteration 734/1072, Loss: 7.0436\n",
      "Iteration 735/1072, Loss: 6.8621\n",
      "Iteration 736/1072, Loss: 7.0183\n",
      "Iteration 737/1072, Loss: 6.8566\n",
      "Iteration 738/1072, Loss: 7.1937\n",
      "Iteration 739/1072, Loss: 7.1075\n",
      "Iteration 740/1072, Loss: 7.2782\n",
      "Iteration 741/1072, Loss: 6.8702\n",
      "Iteration 742/1072, Loss: 6.9026\n",
      "Iteration 743/1072, Loss: 6.9634\n",
      "Iteration 744/1072, Loss: 7.0484\n",
      "Iteration 745/1072, Loss: 7.1610\n",
      "Iteration 746/1072, Loss: 6.9894\n",
      "Iteration 747/1072, Loss: 7.0877\n",
      "Iteration 748/1072, Loss: 6.9817\n",
      "Iteration 749/1072, Loss: 6.7166\n",
      "Iteration 750/1072, Loss: 7.2497\n",
      "Iteration 751/1072, Loss: 6.9712\n",
      "Iteration 752/1072, Loss: 7.2391\n",
      "Iteration 753/1072, Loss: 7.0047\n",
      "Iteration 754/1072, Loss: 7.2540\n",
      "Iteration 755/1072, Loss: 6.9525\n",
      "Iteration 756/1072, Loss: 7.0791\n",
      "Iteration 757/1072, Loss: 7.0386\n",
      "Iteration 758/1072, Loss: 7.0423\n",
      "Iteration 759/1072, Loss: 7.1687\n",
      "Iteration 760/1072, Loss: 7.2097\n",
      "Iteration 761/1072, Loss: 7.2254\n",
      "Iteration 762/1072, Loss: 6.8863\n",
      "Iteration 763/1072, Loss: 6.9995\n",
      "Iteration 764/1072, Loss: 7.0583\n",
      "Iteration 765/1072, Loss: 7.0583\n",
      "Iteration 766/1072, Loss: 6.8916\n",
      "Iteration 767/1072, Loss: 7.1595\n",
      "Iteration 768/1072, Loss: 6.9319\n",
      "Iteration 769/1072, Loss: 7.0572\n",
      "Iteration 770/1072, Loss: 6.9966\n",
      "Iteration 771/1072, Loss: 7.1888\n",
      "Iteration 772/1072, Loss: 7.0362\n",
      "Iteration 773/1072, Loss: 7.2591\n",
      "Iteration 774/1072, Loss: 7.0431\n",
      "Iteration 775/1072, Loss: 7.1839\n",
      "Iteration 776/1072, Loss: 7.3516\n",
      "Iteration 777/1072, Loss: 7.1350\n",
      "Iteration 778/1072, Loss: 7.2502\n",
      "Iteration 779/1072, Loss: 7.0943\n",
      "Iteration 780/1072, Loss: 7.0536\n",
      "Iteration 781/1072, Loss: 7.2004\n",
      "Iteration 782/1072, Loss: 7.3669\n",
      "Iteration 783/1072, Loss: 6.9851\n",
      "Iteration 784/1072, Loss: 6.8765\n",
      "Iteration 785/1072, Loss: 7.2158\n",
      "Iteration 786/1072, Loss: 6.8158\n",
      "Iteration 787/1072, Loss: 6.7658\n",
      "Iteration 788/1072, Loss: 6.8934\n",
      "Iteration 789/1072, Loss: 7.1500\n",
      "Iteration 790/1072, Loss: 6.8818\n",
      "Iteration 791/1072, Loss: 6.9912\n",
      "Iteration 792/1072, Loss: 7.1791\n",
      "Iteration 793/1072, Loss: 7.0859\n",
      "Iteration 794/1072, Loss: 6.9831\n",
      "Iteration 795/1072, Loss: 7.0637\n",
      "Iteration 796/1072, Loss: 7.0505\n",
      "Iteration 797/1072, Loss: 7.1004\n",
      "Iteration 798/1072, Loss: 6.8913\n",
      "Iteration 799/1072, Loss: 6.9176\n",
      "Iteration 800/1072, Loss: 7.2855\n",
      "Iteration 801/1072, Loss: 7.1583\n",
      "Iteration 802/1072, Loss: 7.1159\n",
      "Iteration 803/1072, Loss: 6.9941\n",
      "Iteration 804/1072, Loss: 7.1140\n",
      "Iteration 805/1072, Loss: 7.0059\n",
      "Iteration 806/1072, Loss: 7.0401\n",
      "Iteration 807/1072, Loss: 7.1461\n",
      "Iteration 808/1072, Loss: 7.0250\n",
      "Iteration 809/1072, Loss: 7.0815\n",
      "Iteration 810/1072, Loss: 6.7778\n",
      "Iteration 811/1072, Loss: 6.9968\n",
      "Iteration 812/1072, Loss: 7.0320\n",
      "Iteration 813/1072, Loss: 6.7993\n",
      "Iteration 814/1072, Loss: 6.9326\n",
      "Iteration 815/1072, Loss: 7.2426\n",
      "Iteration 816/1072, Loss: 7.2168\n",
      "Iteration 817/1072, Loss: 7.0589\n",
      "Iteration 818/1072, Loss: 7.0186\n",
      "Iteration 819/1072, Loss: 7.0698\n",
      "Iteration 820/1072, Loss: 7.0713\n",
      "Iteration 821/1072, Loss: 7.1838\n",
      "Iteration 822/1072, Loss: 6.9660\n",
      "Iteration 823/1072, Loss: 7.0593\n",
      "Iteration 824/1072, Loss: 7.0646\n",
      "Iteration 825/1072, Loss: 7.1884\n",
      "Iteration 826/1072, Loss: 6.7661\n",
      "Iteration 827/1072, Loss: 7.3191\n",
      "Iteration 828/1072, Loss: 7.0669\n",
      "Iteration 829/1072, Loss: 7.1128\n",
      "Iteration 830/1072, Loss: 7.2232\n",
      "Iteration 831/1072, Loss: 6.9083\n",
      "Iteration 832/1072, Loss: 6.9112\n",
      "Iteration 833/1072, Loss: 7.3513\n",
      "Iteration 834/1072, Loss: 7.0712\n",
      "Iteration 835/1072, Loss: 7.2203\n",
      "Iteration 836/1072, Loss: 7.0817\n",
      "Iteration 837/1072, Loss: 6.9773\n",
      "Iteration 838/1072, Loss: 7.2090\n",
      "Iteration 839/1072, Loss: 6.7946\n",
      "Iteration 840/1072, Loss: 7.1065\n",
      "Iteration 841/1072, Loss: 6.9668\n",
      "Iteration 842/1072, Loss: 7.1925\n",
      "Iteration 843/1072, Loss: 7.1449\n",
      "Iteration 844/1072, Loss: 7.1115\n",
      "Iteration 845/1072, Loss: 7.0082\n",
      "Iteration 846/1072, Loss: 6.8412\n",
      "Iteration 847/1072, Loss: 7.2780\n",
      "Iteration 848/1072, Loss: 7.3388\n",
      "Iteration 849/1072, Loss: 6.9390\n",
      "Iteration 850/1072, Loss: 7.0724\n",
      "Iteration 851/1072, Loss: 7.0280\n",
      "Iteration 852/1072, Loss: 6.9439\n",
      "Iteration 853/1072, Loss: 7.1065\n",
      "Iteration 854/1072, Loss: 6.8943\n",
      "Iteration 855/1072, Loss: 7.1603\n",
      "Iteration 856/1072, Loss: 7.1994\n",
      "Iteration 857/1072, Loss: 7.0182\n",
      "Iteration 858/1072, Loss: 7.1897\n",
      "Iteration 859/1072, Loss: 7.1940\n",
      "Iteration 860/1072, Loss: 6.9178\n",
      "Iteration 861/1072, Loss: 6.8870\n",
      "Iteration 862/1072, Loss: 6.9399\n",
      "Iteration 863/1072, Loss: 7.2105\n",
      "Iteration 864/1072, Loss: 7.0252\n",
      "Iteration 865/1072, Loss: 6.8307\n",
      "Iteration 866/1072, Loss: 7.0118\n",
      "Iteration 867/1072, Loss: 6.8704\n",
      "Iteration 868/1072, Loss: 7.0156\n",
      "Iteration 869/1072, Loss: 7.0028\n",
      "Iteration 870/1072, Loss: 6.9049\n",
      "Iteration 871/1072, Loss: 6.9241\n",
      "Iteration 872/1072, Loss: 7.0646\n",
      "Iteration 873/1072, Loss: 7.1027\n",
      "Iteration 874/1072, Loss: 7.4369\n",
      "Iteration 875/1072, Loss: 7.0744\n",
      "Iteration 876/1072, Loss: 6.9767\n",
      "Iteration 877/1072, Loss: 7.1735\n",
      "Iteration 878/1072, Loss: 7.1950\n",
      "Iteration 879/1072, Loss: 7.3510\n",
      "Iteration 880/1072, Loss: 7.3662\n",
      "Iteration 881/1072, Loss: 7.2747\n",
      "Iteration 882/1072, Loss: 7.0734\n",
      "Iteration 883/1072, Loss: 7.0667\n",
      "Iteration 884/1072, Loss: 7.1188\n",
      "Iteration 885/1072, Loss: 6.9171\n",
      "Iteration 886/1072, Loss: 7.0947\n",
      "Iteration 887/1072, Loss: 7.0979\n",
      "Iteration 888/1072, Loss: 6.9164\n",
      "Iteration 889/1072, Loss: 6.9234\n",
      "Iteration 890/1072, Loss: 6.9113\n",
      "Iteration 891/1072, Loss: 6.9313\n",
      "Iteration 892/1072, Loss: 7.0022\n",
      "Iteration 893/1072, Loss: 6.8618\n",
      "Iteration 894/1072, Loss: 7.0523\n",
      "Iteration 895/1072, Loss: 7.1848\n",
      "Iteration 896/1072, Loss: 6.7811\n",
      "Iteration 897/1072, Loss: 7.1873\n",
      "Iteration 898/1072, Loss: 6.9390\n",
      "Iteration 899/1072, Loss: 6.9943\n",
      "Iteration 900/1072, Loss: 6.9412\n",
      "Iteration 901/1072, Loss: 7.2217\n",
      "Iteration 902/1072, Loss: 6.8234\n",
      "Iteration 903/1072, Loss: 6.9842\n",
      "Iteration 904/1072, Loss: 6.9772\n",
      "Iteration 905/1072, Loss: 7.0228\n",
      "Iteration 906/1072, Loss: 6.8984\n",
      "Iteration 907/1072, Loss: 6.5551\n",
      "Iteration 908/1072, Loss: 7.1995\n",
      "Iteration 909/1072, Loss: 7.0325\n",
      "Iteration 910/1072, Loss: 6.7823\n",
      "Iteration 911/1072, Loss: 7.2242\n",
      "Iteration 912/1072, Loss: 7.0356\n",
      "Iteration 913/1072, Loss: 6.8019\n",
      "Iteration 914/1072, Loss: 7.1141\n",
      "Iteration 915/1072, Loss: 7.0813\n",
      "Iteration 916/1072, Loss: 7.0326\n",
      "Iteration 917/1072, Loss: 6.8121\n",
      "Iteration 918/1072, Loss: 7.2243\n",
      "Iteration 919/1072, Loss: 7.2005\n",
      "Iteration 920/1072, Loss: 6.9243\n",
      "Iteration 921/1072, Loss: 7.0248\n",
      "Iteration 922/1072, Loss: 6.9903\n",
      "Iteration 923/1072, Loss: 7.1209\n",
      "Iteration 924/1072, Loss: 6.9395\n",
      "Iteration 925/1072, Loss: 7.0439\n",
      "Iteration 926/1072, Loss: 6.9643\n",
      "Iteration 927/1072, Loss: 6.9096\n",
      "Iteration 928/1072, Loss: 7.1979\n",
      "Iteration 929/1072, Loss: 7.0994\n",
      "Iteration 930/1072, Loss: 7.1411\n",
      "Iteration 931/1072, Loss: 7.0437\n",
      "Iteration 932/1072, Loss: 6.7705\n",
      "Iteration 933/1072, Loss: 6.7779\n",
      "Iteration 934/1072, Loss: 6.8388\n",
      "Iteration 935/1072, Loss: 7.1434\n",
      "Iteration 936/1072, Loss: 7.0515\n",
      "Iteration 937/1072, Loss: 7.1304\n",
      "Iteration 938/1072, Loss: 6.9795\n",
      "Iteration 939/1072, Loss: 6.7296\n",
      "Iteration 940/1072, Loss: 7.0932\n",
      "Iteration 941/1072, Loss: 7.1551\n",
      "Iteration 942/1072, Loss: 6.9540\n",
      "Iteration 943/1072, Loss: 7.2999\n",
      "Iteration 944/1072, Loss: 6.9922\n",
      "Iteration 945/1072, Loss: 7.1270\n",
      "Iteration 946/1072, Loss: 7.1212\n",
      "Iteration 947/1072, Loss: 6.9601\n",
      "Iteration 948/1072, Loss: 6.9754\n",
      "Iteration 949/1072, Loss: 7.0234\n",
      "Iteration 950/1072, Loss: 6.4201\n",
      "Iteration 951/1072, Loss: 7.2705\n",
      "Iteration 952/1072, Loss: 7.0989\n",
      "Iteration 953/1072, Loss: 6.6736\n",
      "Iteration 954/1072, Loss: 7.3171\n",
      "Iteration 955/1072, Loss: 7.0600\n",
      "Iteration 956/1072, Loss: 7.1974\n",
      "Iteration 957/1072, Loss: 7.1489\n",
      "Iteration 958/1072, Loss: 7.1904\n",
      "Iteration 959/1072, Loss: 7.0104\n",
      "Iteration 960/1072, Loss: 7.1430\n",
      "Iteration 961/1072, Loss: 6.5653\n",
      "Iteration 962/1072, Loss: 6.9554\n",
      "Iteration 963/1072, Loss: 7.0744\n",
      "Iteration 964/1072, Loss: 7.3504\n",
      "Iteration 965/1072, Loss: 7.0177\n",
      "Iteration 966/1072, Loss: 7.1347\n",
      "Iteration 967/1072, Loss: 7.0476\n",
      "Iteration 968/1072, Loss: 6.9102\n",
      "Iteration 969/1072, Loss: 7.0954\n",
      "Iteration 970/1072, Loss: 6.8304\n",
      "Iteration 971/1072, Loss: 6.9019\n",
      "Iteration 972/1072, Loss: 7.0077\n",
      "Iteration 973/1072, Loss: 7.0932\n",
      "Iteration 974/1072, Loss: 7.0003\n",
      "Iteration 975/1072, Loss: 7.0083\n",
      "Iteration 976/1072, Loss: 6.6593\n",
      "Iteration 977/1072, Loss: 6.8443\n",
      "Iteration 978/1072, Loss: 6.9666\n",
      "Iteration 979/1072, Loss: 7.1610\n",
      "Iteration 980/1072, Loss: 6.9915\n",
      "Iteration 981/1072, Loss: 6.8288\n",
      "Iteration 982/1072, Loss: 7.0990\n",
      "Iteration 983/1072, Loss: 7.1434\n",
      "Iteration 984/1072, Loss: 6.8947\n",
      "Iteration 985/1072, Loss: 6.8259\n",
      "Iteration 986/1072, Loss: 7.0804\n",
      "Iteration 987/1072, Loss: 7.1406\n",
      "Iteration 988/1072, Loss: 7.0682\n",
      "Iteration 989/1072, Loss: 7.2364\n",
      "Iteration 990/1072, Loss: 6.8641\n",
      "Iteration 991/1072, Loss: 7.0452\n",
      "Iteration 992/1072, Loss: 6.9494\n",
      "Iteration 993/1072, Loss: 6.8779\n",
      "Iteration 994/1072, Loss: 7.1485\n",
      "Iteration 995/1072, Loss: 7.3479\n",
      "Iteration 996/1072, Loss: 7.0063\n",
      "Iteration 997/1072, Loss: 7.1698\n",
      "Iteration 998/1072, Loss: 6.9540\n",
      "Iteration 999/1072, Loss: 6.9185\n",
      "Iteration 1000/1072, Loss: 6.8524\n",
      "Iteration 1001/1072, Loss: 6.9792\n",
      "Iteration 1002/1072, Loss: 7.0268\n",
      "Iteration 1003/1072, Loss: 7.0684\n",
      "Iteration 1004/1072, Loss: 7.0871\n",
      "Iteration 1005/1072, Loss: 6.8975\n",
      "Iteration 1006/1072, Loss: 7.0227\n",
      "Iteration 1007/1072, Loss: 7.0131\n",
      "Iteration 1008/1072, Loss: 6.9988\n",
      "Iteration 1009/1072, Loss: 6.4922\n",
      "Iteration 1010/1072, Loss: 7.1345\n",
      "Iteration 1011/1072, Loss: 7.0403\n",
      "Iteration 1012/1072, Loss: 6.9738\n",
      "Iteration 1013/1072, Loss: 7.0634\n",
      "Iteration 1014/1072, Loss: 7.0070\n",
      "Iteration 1015/1072, Loss: 7.1553\n",
      "Iteration 1016/1072, Loss: 6.8698\n",
      "Iteration 1017/1072, Loss: 7.0864\n",
      "Iteration 1018/1072, Loss: 7.0041\n",
      "Iteration 1019/1072, Loss: 7.0280\n",
      "Iteration 1020/1072, Loss: 7.1303\n",
      "Iteration 1021/1072, Loss: 7.0671\n",
      "Iteration 1022/1072, Loss: 6.7698\n",
      "Iteration 1023/1072, Loss: 6.9804\n",
      "Iteration 1024/1072, Loss: 7.0654\n",
      "Iteration 1025/1072, Loss: 6.9532\n",
      "Iteration 1026/1072, Loss: 7.2559\n",
      "Iteration 1027/1072, Loss: 7.0997\n",
      "Iteration 1028/1072, Loss: 6.6407\n",
      "Iteration 1029/1072, Loss: 7.1123\n",
      "Iteration 1030/1072, Loss: 7.2102\n",
      "Iteration 1031/1072, Loss: 7.1531\n",
      "Iteration 1032/1072, Loss: 7.0403\n",
      "Iteration 1033/1072, Loss: 6.8083\n",
      "Iteration 1034/1072, Loss: 6.9831\n",
      "Iteration 1035/1072, Loss: 7.0294\n",
      "Iteration 1036/1072, Loss: 6.6704\n",
      "Iteration 1037/1072, Loss: 7.1207\n",
      "Iteration 1038/1072, Loss: 7.1068\n",
      "Iteration 1039/1072, Loss: 6.7392\n",
      "Iteration 1040/1072, Loss: 7.0447\n",
      "Iteration 1041/1072, Loss: 6.9749\n",
      "Iteration 1042/1072, Loss: 7.1159\n",
      "Iteration 1043/1072, Loss: 7.0331\n",
      "Iteration 1044/1072, Loss: 6.9742\n",
      "Iteration 1045/1072, Loss: 7.0542\n",
      "Iteration 1046/1072, Loss: 6.8732\n",
      "Iteration 1047/1072, Loss: 7.1159\n",
      "Iteration 1048/1072, Loss: 7.0944\n",
      "Iteration 1049/1072, Loss: 6.8545\n",
      "Iteration 1050/1072, Loss: 6.9244\n",
      "Iteration 1051/1072, Loss: 7.0334\n",
      "Iteration 1052/1072, Loss: 6.6865\n",
      "Iteration 1053/1072, Loss: 7.2136\n",
      "Iteration 1054/1072, Loss: 7.1468\n",
      "Iteration 1055/1072, Loss: 6.8382\n",
      "Iteration 1056/1072, Loss: 6.9584\n",
      "Iteration 1057/1072, Loss: 7.0681\n",
      "Iteration 1058/1072, Loss: 6.9721\n",
      "Iteration 1059/1072, Loss: 6.8262\n",
      "Iteration 1060/1072, Loss: 7.0907\n",
      "Iteration 1061/1072, Loss: 6.8827\n",
      "Iteration 1062/1072, Loss: 6.8329\n",
      "Iteration 1063/1072, Loss: 6.8681\n",
      "Iteration 1064/1072, Loss: 7.2979\n",
      "Iteration 1065/1072, Loss: 6.8773\n",
      "Iteration 1066/1072, Loss: 6.9703\n",
      "Iteration 1067/1072, Loss: 7.0558\n",
      "Iteration 1068/1072, Loss: 7.1573\n",
      "Iteration 1069/1072, Loss: 6.9326\n",
      "Iteration 1070/1072, Loss: 7.0396\n",
      "Iteration 1071/1072, Loss: 6.9646\n",
      "Iteration 1072/1072, Loss: 7.4006\n",
      "Epoch 4/10, Loss: 7.1095\n",
      "Validation Accuracy: 8.18%\n",
      "Model checkpoint saved!\n",
      "Iteration 1/1072, Loss: 6.6799\n",
      "Iteration 2/1072, Loss: 6.7414\n",
      "Iteration 3/1072, Loss: 6.6523\n",
      "Iteration 4/1072, Loss: 6.7488\n",
      "Iteration 5/1072, Loss: 6.7325\n",
      "Iteration 6/1072, Loss: 6.7938\n",
      "Iteration 7/1072, Loss: 6.6401\n",
      "Iteration 8/1072, Loss: 6.8653\n",
      "Iteration 9/1072, Loss: 6.9336\n",
      "Iteration 10/1072, Loss: 7.0037\n",
      "Iteration 11/1072, Loss: 6.7831\n",
      "Iteration 12/1072, Loss: 6.6697\n",
      "Iteration 13/1072, Loss: 6.8515\n",
      "Iteration 14/1072, Loss: 6.5972\n",
      "Iteration 15/1072, Loss: 6.7360\n",
      "Iteration 16/1072, Loss: 7.0425\n",
      "Iteration 17/1072, Loss: 6.8967\n",
      "Iteration 18/1072, Loss: 6.6210\n",
      "Iteration 19/1072, Loss: 6.6867\n",
      "Iteration 20/1072, Loss: 6.5100\n",
      "Iteration 21/1072, Loss: 6.9221\n",
      "Iteration 22/1072, Loss: 6.3219\n",
      "Iteration 23/1072, Loss: 7.0167\n",
      "Iteration 24/1072, Loss: 6.6150\n",
      "Iteration 25/1072, Loss: 6.6340\n",
      "Iteration 26/1072, Loss: 6.9122\n",
      "Iteration 27/1072, Loss: 6.7451\n",
      "Iteration 28/1072, Loss: 6.4440\n",
      "Iteration 29/1072, Loss: 6.7138\n",
      "Iteration 30/1072, Loss: 6.4084\n",
      "Iteration 31/1072, Loss: 6.9701\n",
      "Iteration 32/1072, Loss: 6.8173\n",
      "Iteration 33/1072, Loss: 6.6675\n",
      "Iteration 34/1072, Loss: 6.5515\n",
      "Iteration 35/1072, Loss: 6.5198\n",
      "Iteration 36/1072, Loss: 6.6806\n",
      "Iteration 37/1072, Loss: 6.7313\n",
      "Iteration 38/1072, Loss: 6.6756\n",
      "Iteration 39/1072, Loss: 6.9220\n",
      "Iteration 40/1072, Loss: 6.6095\n",
      "Iteration 41/1072, Loss: 6.6201\n",
      "Iteration 42/1072, Loss: 6.9910\n",
      "Iteration 43/1072, Loss: 6.6779\n",
      "Iteration 44/1072, Loss: 6.9559\n",
      "Iteration 45/1072, Loss: 6.5515\n",
      "Iteration 46/1072, Loss: 6.5042\n",
      "Iteration 47/1072, Loss: 6.6459\n",
      "Iteration 48/1072, Loss: 6.8063\n",
      "Iteration 49/1072, Loss: 6.9137\n",
      "Iteration 50/1072, Loss: 6.7554\n",
      "Iteration 51/1072, Loss: 6.8953\n",
      "Iteration 52/1072, Loss: 6.8466\n",
      "Iteration 53/1072, Loss: 6.8689\n",
      "Iteration 54/1072, Loss: 6.8825\n",
      "Iteration 55/1072, Loss: 6.5628\n",
      "Iteration 56/1072, Loss: 6.7441\n",
      "Iteration 57/1072, Loss: 6.5600\n",
      "Iteration 58/1072, Loss: 6.6374\n",
      "Iteration 59/1072, Loss: 7.0038\n",
      "Iteration 60/1072, Loss: 6.7387\n",
      "Iteration 61/1072, Loss: 6.7850\n",
      "Iteration 62/1072, Loss: 6.8992\n",
      "Iteration 63/1072, Loss: 6.6653\n",
      "Iteration 64/1072, Loss: 6.7885\n",
      "Iteration 65/1072, Loss: 6.3350\n",
      "Iteration 66/1072, Loss: 6.9656\n",
      "Iteration 67/1072, Loss: 6.6613\n",
      "Iteration 68/1072, Loss: 6.8650\n",
      "Iteration 69/1072, Loss: 6.7389\n",
      "Iteration 70/1072, Loss: 6.9340\n",
      "Iteration 71/1072, Loss: 7.1245\n",
      "Iteration 72/1072, Loss: 6.9492\n",
      "Iteration 73/1072, Loss: 7.0194\n",
      "Iteration 74/1072, Loss: 6.8495\n",
      "Iteration 75/1072, Loss: 6.7863\n",
      "Iteration 76/1072, Loss: 6.6376\n",
      "Iteration 77/1072, Loss: 6.8920\n",
      "Iteration 78/1072, Loss: 6.9486\n",
      "Iteration 79/1072, Loss: 6.8192\n",
      "Iteration 80/1072, Loss: 6.8420\n",
      "Iteration 81/1072, Loss: 6.7561\n",
      "Iteration 82/1072, Loss: 6.5174\n",
      "Iteration 83/1072, Loss: 6.7027\n",
      "Iteration 84/1072, Loss: 6.8089\n",
      "Iteration 85/1072, Loss: 6.6800\n",
      "Iteration 86/1072, Loss: 6.6705\n",
      "Iteration 87/1072, Loss: 6.9576\n",
      "Iteration 88/1072, Loss: 6.8212\n",
      "Iteration 89/1072, Loss: 6.5920\n",
      "Iteration 90/1072, Loss: 6.7608\n",
      "Iteration 91/1072, Loss: 6.9015\n",
      "Iteration 92/1072, Loss: 6.9811\n",
      "Iteration 93/1072, Loss: 6.7133\n",
      "Iteration 94/1072, Loss: 6.8307\n",
      "Iteration 95/1072, Loss: 6.8399\n",
      "Iteration 96/1072, Loss: 7.0728\n",
      "Iteration 97/1072, Loss: 6.6300\n",
      "Iteration 98/1072, Loss: 6.7143\n",
      "Iteration 99/1072, Loss: 7.0597\n",
      "Iteration 100/1072, Loss: 6.9420\n",
      "Iteration 101/1072, Loss: 6.9083\n",
      "Iteration 102/1072, Loss: 6.7425\n",
      "Iteration 103/1072, Loss: 6.5152\n",
      "Iteration 104/1072, Loss: 6.4547\n",
      "Iteration 105/1072, Loss: 6.9566\n",
      "Iteration 106/1072, Loss: 6.6915\n",
      "Iteration 107/1072, Loss: 6.9893\n",
      "Iteration 108/1072, Loss: 6.6987\n",
      "Iteration 109/1072, Loss: 6.9639\n",
      "Iteration 110/1072, Loss: 6.6871\n",
      "Iteration 111/1072, Loss: 6.6677\n",
      "Iteration 112/1072, Loss: 6.9043\n",
      "Iteration 113/1072, Loss: 6.5809\n",
      "Iteration 114/1072, Loss: 6.8263\n",
      "Iteration 115/1072, Loss: 6.4273\n",
      "Iteration 116/1072, Loss: 6.8055\n",
      "Iteration 117/1072, Loss: 6.9239\n",
      "Iteration 118/1072, Loss: 6.7375\n",
      "Iteration 119/1072, Loss: 7.1546\n",
      "Iteration 120/1072, Loss: 6.7815\n",
      "Iteration 121/1072, Loss: 6.7644\n",
      "Iteration 122/1072, Loss: 6.5417\n",
      "Iteration 123/1072, Loss: 6.6160\n",
      "Iteration 124/1072, Loss: 6.6146\n",
      "Iteration 125/1072, Loss: 6.8002\n",
      "Iteration 126/1072, Loss: 6.8486\n",
      "Iteration 127/1072, Loss: 6.7983\n",
      "Iteration 128/1072, Loss: 6.8609\n",
      "Iteration 129/1072, Loss: 6.7891\n",
      "Iteration 130/1072, Loss: 6.7068\n",
      "Iteration 131/1072, Loss: 6.9716\n",
      "Iteration 132/1072, Loss: 6.7178\n",
      "Iteration 133/1072, Loss: 6.5809\n",
      "Iteration 134/1072, Loss: 6.6469\n",
      "Iteration 135/1072, Loss: 6.8675\n",
      "Iteration 136/1072, Loss: 6.6580\n",
      "Iteration 137/1072, Loss: 6.8747\n",
      "Iteration 138/1072, Loss: 6.6983\n",
      "Iteration 139/1072, Loss: 6.8841\n",
      "Iteration 140/1072, Loss: 6.6819\n",
      "Iteration 141/1072, Loss: 6.5520\n",
      "Iteration 142/1072, Loss: 6.6889\n",
      "Iteration 143/1072, Loss: 6.8980\n",
      "Iteration 144/1072, Loss: 6.6989\n",
      "Iteration 145/1072, Loss: 6.4488\n",
      "Iteration 146/1072, Loss: 6.8207\n",
      "Iteration 147/1072, Loss: 6.5261\n",
      "Iteration 148/1072, Loss: 6.7178\n",
      "Iteration 149/1072, Loss: 6.9158\n",
      "Iteration 150/1072, Loss: 6.5454\n",
      "Iteration 151/1072, Loss: 6.7152\n",
      "Iteration 152/1072, Loss: 6.6827\n",
      "Iteration 153/1072, Loss: 6.4975\n",
      "Iteration 154/1072, Loss: 6.6077\n",
      "Iteration 155/1072, Loss: 6.5105\n",
      "Iteration 156/1072, Loss: 6.8355\n",
      "Iteration 157/1072, Loss: 6.9175\n",
      "Iteration 158/1072, Loss: 6.5194\n",
      "Iteration 159/1072, Loss: 6.7290\n",
      "Iteration 160/1072, Loss: 6.6315\n",
      "Iteration 161/1072, Loss: 6.6989\n",
      "Iteration 162/1072, Loss: 6.7776\n",
      "Iteration 163/1072, Loss: 6.7019\n",
      "Iteration 164/1072, Loss: 6.8939\n",
      "Iteration 165/1072, Loss: 6.5746\n",
      "Iteration 166/1072, Loss: 6.9454\n",
      "Iteration 167/1072, Loss: 6.9398\n",
      "Iteration 168/1072, Loss: 6.7497\n",
      "Iteration 169/1072, Loss: 7.0133\n",
      "Iteration 170/1072, Loss: 6.8820\n",
      "Iteration 171/1072, Loss: 6.7619\n",
      "Iteration 172/1072, Loss: 6.8515\n",
      "Iteration 173/1072, Loss: 6.8291\n",
      "Iteration 174/1072, Loss: 6.4656\n",
      "Iteration 175/1072, Loss: 6.5284\n",
      "Iteration 176/1072, Loss: 6.8569\n",
      "Iteration 177/1072, Loss: 6.7826\n",
      "Iteration 178/1072, Loss: 6.9204\n",
      "Iteration 179/1072, Loss: 6.9469\n",
      "Iteration 180/1072, Loss: 6.8172\n",
      "Iteration 181/1072, Loss: 6.6646\n",
      "Iteration 182/1072, Loss: 6.6029\n",
      "Iteration 183/1072, Loss: 6.9881\n",
      "Iteration 184/1072, Loss: 6.9426\n",
      "Iteration 185/1072, Loss: 6.8541\n",
      "Iteration 186/1072, Loss: 6.7530\n",
      "Iteration 187/1072, Loss: 6.6916\n",
      "Iteration 188/1072, Loss: 6.8250\n",
      "Iteration 189/1072, Loss: 7.0081\n",
      "Iteration 190/1072, Loss: 6.7964\n",
      "Iteration 191/1072, Loss: 6.5761\n",
      "Iteration 192/1072, Loss: 6.9736\n",
      "Iteration 193/1072, Loss: 6.6022\n",
      "Iteration 194/1072, Loss: 6.7631\n",
      "Iteration 195/1072, Loss: 6.8736\n",
      "Iteration 196/1072, Loss: 6.8046\n",
      "Iteration 197/1072, Loss: 6.8679\n",
      "Iteration 198/1072, Loss: 6.6609\n",
      "Iteration 199/1072, Loss: 6.7141\n",
      "Iteration 200/1072, Loss: 7.1052\n",
      "Iteration 201/1072, Loss: 6.8038\n",
      "Iteration 202/1072, Loss: 6.9884\n",
      "Iteration 203/1072, Loss: 6.7916\n",
      "Iteration 204/1072, Loss: 6.8329\n",
      "Iteration 205/1072, Loss: 6.6957\n",
      "Iteration 206/1072, Loss: 6.9329\n",
      "Iteration 207/1072, Loss: 6.5500\n",
      "Iteration 208/1072, Loss: 6.8751\n",
      "Iteration 209/1072, Loss: 6.8195\n",
      "Iteration 210/1072, Loss: 6.6724\n",
      "Iteration 211/1072, Loss: 6.5074\n",
      "Iteration 212/1072, Loss: 6.4110\n",
      "Iteration 213/1072, Loss: 6.8588\n",
      "Iteration 214/1072, Loss: 7.0034\n",
      "Iteration 215/1072, Loss: 6.5483\n",
      "Iteration 216/1072, Loss: 6.6907\n",
      "Iteration 217/1072, Loss: 6.7635\n",
      "Iteration 218/1072, Loss: 6.7904\n",
      "Iteration 219/1072, Loss: 6.3316\n",
      "Iteration 220/1072, Loss: 6.7975\n",
      "Iteration 221/1072, Loss: 6.6208\n",
      "Iteration 222/1072, Loss: 6.5278\n",
      "Iteration 223/1072, Loss: 6.7033\n",
      "Iteration 224/1072, Loss: 6.5592\n",
      "Iteration 225/1072, Loss: 6.7633\n",
      "Iteration 226/1072, Loss: 6.8951\n",
      "Iteration 227/1072, Loss: 6.5917\n",
      "Iteration 228/1072, Loss: 6.8011\n",
      "Iteration 229/1072, Loss: 6.7342\n",
      "Iteration 230/1072, Loss: 6.7172\n",
      "Iteration 231/1072, Loss: 6.8365\n",
      "Iteration 232/1072, Loss: 6.5394\n",
      "Iteration 233/1072, Loss: 6.6930\n",
      "Iteration 234/1072, Loss: 6.5948\n",
      "Iteration 235/1072, Loss: 6.6696\n",
      "Iteration 236/1072, Loss: 6.8086\n",
      "Iteration 237/1072, Loss: 6.9067\n",
      "Iteration 238/1072, Loss: 6.8792\n",
      "Iteration 239/1072, Loss: 6.7346\n",
      "Iteration 240/1072, Loss: 6.5121\n",
      "Iteration 241/1072, Loss: 6.7511\n",
      "Iteration 242/1072, Loss: 6.9262\n",
      "Iteration 243/1072, Loss: 6.6698\n",
      "Iteration 244/1072, Loss: 6.7507\n",
      "Iteration 245/1072, Loss: 6.6123\n",
      "Iteration 246/1072, Loss: 6.7504\n",
      "Iteration 247/1072, Loss: 6.9713\n",
      "Iteration 248/1072, Loss: 6.6527\n",
      "Iteration 249/1072, Loss: 6.7213\n",
      "Iteration 250/1072, Loss: 6.6941\n",
      "Iteration 251/1072, Loss: 6.7724\n",
      "Iteration 252/1072, Loss: 6.7575\n",
      "Iteration 253/1072, Loss: 6.9291\n",
      "Iteration 254/1072, Loss: 6.7686\n",
      "Iteration 255/1072, Loss: 6.9093\n",
      "Iteration 256/1072, Loss: 6.9331\n",
      "Iteration 257/1072, Loss: 6.8672\n",
      "Iteration 258/1072, Loss: 6.9316\n",
      "Iteration 259/1072, Loss: 6.5849\n",
      "Iteration 260/1072, Loss: 6.8078\n",
      "Iteration 261/1072, Loss: 7.0636\n",
      "Iteration 262/1072, Loss: 6.8194\n",
      "Iteration 263/1072, Loss: 6.7807\n",
      "Iteration 264/1072, Loss: 6.9482\n",
      "Iteration 265/1072, Loss: 6.6992\n",
      "Iteration 266/1072, Loss: 6.7659\n",
      "Iteration 267/1072, Loss: 6.6713\n",
      "Iteration 268/1072, Loss: 6.8026\n",
      "Iteration 269/1072, Loss: 6.6181\n",
      "Iteration 270/1072, Loss: 6.7546\n",
      "Iteration 271/1072, Loss: 6.5934\n",
      "Iteration 272/1072, Loss: 6.5904\n",
      "Iteration 273/1072, Loss: 7.0387\n",
      "Iteration 274/1072, Loss: 6.4427\n",
      "Iteration 275/1072, Loss: 6.8464\n",
      "Iteration 276/1072, Loss: 6.7588\n",
      "Iteration 277/1072, Loss: 6.7297\n",
      "Iteration 278/1072, Loss: 6.5993\n",
      "Iteration 279/1072, Loss: 6.8847\n",
      "Iteration 280/1072, Loss: 6.8749\n",
      "Iteration 281/1072, Loss: 6.8712\n",
      "Iteration 282/1072, Loss: 6.7823\n",
      "Iteration 283/1072, Loss: 6.9640\n",
      "Iteration 284/1072, Loss: 6.5917\n",
      "Iteration 285/1072, Loss: 6.7364\n",
      "Iteration 286/1072, Loss: 6.6262\n",
      "Iteration 287/1072, Loss: 6.9315\n",
      "Iteration 288/1072, Loss: 6.6197\n",
      "Iteration 289/1072, Loss: 6.7630\n",
      "Iteration 290/1072, Loss: 6.6069\n",
      "Iteration 291/1072, Loss: 6.4516\n",
      "Iteration 292/1072, Loss: 6.7264\n",
      "Iteration 293/1072, Loss: 6.6714\n",
      "Iteration 294/1072, Loss: 6.6505\n",
      "Iteration 295/1072, Loss: 6.6458\n",
      "Iteration 296/1072, Loss: 6.6203\n",
      "Iteration 297/1072, Loss: 6.7181\n",
      "Iteration 298/1072, Loss: 6.8300\n",
      "Iteration 299/1072, Loss: 6.8122\n",
      "Iteration 300/1072, Loss: 6.8073\n",
      "Iteration 301/1072, Loss: 6.5977\n",
      "Iteration 302/1072, Loss: 6.8132\n",
      "Iteration 303/1072, Loss: 6.5544\n",
      "Iteration 304/1072, Loss: 6.9178\n",
      "Iteration 305/1072, Loss: 6.8338\n",
      "Iteration 306/1072, Loss: 7.1951\n",
      "Iteration 307/1072, Loss: 6.5679\n",
      "Iteration 308/1072, Loss: 6.3567\n",
      "Iteration 309/1072, Loss: 6.8596\n",
      "Iteration 310/1072, Loss: 6.4467\n",
      "Iteration 311/1072, Loss: 6.5192\n",
      "Iteration 312/1072, Loss: 6.6356\n",
      "Iteration 313/1072, Loss: 6.6186\n",
      "Iteration 314/1072, Loss: 6.6825\n",
      "Iteration 315/1072, Loss: 6.8184\n",
      "Iteration 316/1072, Loss: 6.7614\n",
      "Iteration 317/1072, Loss: 6.5028\n",
      "Iteration 318/1072, Loss: 6.3909\n",
      "Iteration 319/1072, Loss: 6.7540\n",
      "Iteration 320/1072, Loss: 6.3661\n",
      "Iteration 321/1072, Loss: 6.8299\n",
      "Iteration 322/1072, Loss: 6.6273\n",
      "Iteration 323/1072, Loss: 6.8150\n",
      "Iteration 324/1072, Loss: 6.6469\n",
      "Iteration 325/1072, Loss: 6.6921\n",
      "Iteration 326/1072, Loss: 7.0196\n",
      "Iteration 327/1072, Loss: 6.6971\n",
      "Iteration 328/1072, Loss: 6.6370\n",
      "Iteration 329/1072, Loss: 6.6151\n",
      "Iteration 330/1072, Loss: 6.4773\n",
      "Iteration 331/1072, Loss: 6.7563\n",
      "Iteration 332/1072, Loss: 6.4295\n",
      "Iteration 333/1072, Loss: 6.7285\n",
      "Iteration 334/1072, Loss: 7.0410\n",
      "Iteration 335/1072, Loss: 6.6143\n",
      "Iteration 336/1072, Loss: 6.8690\n",
      "Iteration 337/1072, Loss: 6.5302\n",
      "Iteration 338/1072, Loss: 6.7688\n",
      "Iteration 339/1072, Loss: 7.0425\n",
      "Iteration 340/1072, Loss: 6.5214\n",
      "Iteration 341/1072, Loss: 6.5918\n",
      "Iteration 342/1072, Loss: 6.8721\n",
      "Iteration 343/1072, Loss: 6.6300\n",
      "Iteration 344/1072, Loss: 6.8475\n",
      "Iteration 345/1072, Loss: 6.5168\n",
      "Iteration 346/1072, Loss: 6.7109\n",
      "Iteration 347/1072, Loss: 6.6720\n",
      "Iteration 348/1072, Loss: 6.7991\n",
      "Iteration 349/1072, Loss: 6.5428\n",
      "Iteration 350/1072, Loss: 6.8584\n",
      "Iteration 351/1072, Loss: 6.8849\n",
      "Iteration 352/1072, Loss: 6.5358\n",
      "Iteration 353/1072, Loss: 6.8151\n",
      "Iteration 354/1072, Loss: 6.7509\n",
      "Iteration 355/1072, Loss: 6.7336\n",
      "Iteration 356/1072, Loss: 6.6007\n",
      "Iteration 357/1072, Loss: 6.9068\n",
      "Iteration 358/1072, Loss: 6.7393\n",
      "Iteration 359/1072, Loss: 6.5202\n",
      "Iteration 360/1072, Loss: 6.7723\n",
      "Iteration 361/1072, Loss: 6.8738\n",
      "Iteration 362/1072, Loss: 6.6908\n",
      "Iteration 363/1072, Loss: 6.3568\n",
      "Iteration 364/1072, Loss: 6.9031\n",
      "Iteration 365/1072, Loss: 6.8527\n",
      "Iteration 366/1072, Loss: 6.8473\n",
      "Iteration 367/1072, Loss: 6.6011\n",
      "Iteration 368/1072, Loss: 6.7648\n",
      "Iteration 369/1072, Loss: 7.0457\n",
      "Iteration 370/1072, Loss: 6.5341\n",
      "Iteration 371/1072, Loss: 6.4310\n",
      "Iteration 372/1072, Loss: 6.9438\n",
      "Iteration 373/1072, Loss: 7.0668\n",
      "Iteration 374/1072, Loss: 6.9968\n",
      "Iteration 375/1072, Loss: 6.6967\n",
      "Iteration 376/1072, Loss: 6.8983\n",
      "Iteration 377/1072, Loss: 6.9633\n",
      "Iteration 378/1072, Loss: 6.6186\n",
      "Iteration 379/1072, Loss: 6.7908\n",
      "Iteration 380/1072, Loss: 6.4272\n",
      "Iteration 381/1072, Loss: 6.4137\n",
      "Iteration 382/1072, Loss: 6.6854\n",
      "Iteration 383/1072, Loss: 6.5094\n",
      "Iteration 384/1072, Loss: 6.8143\n",
      "Iteration 385/1072, Loss: 6.6962\n",
      "Iteration 386/1072, Loss: 6.8967\n",
      "Iteration 387/1072, Loss: 6.4245\n",
      "Iteration 388/1072, Loss: 6.9189\n",
      "Iteration 389/1072, Loss: 6.6379\n",
      "Iteration 390/1072, Loss: 6.7742\n",
      "Iteration 391/1072, Loss: 6.9804\n",
      "Iteration 392/1072, Loss: 6.6839\n",
      "Iteration 393/1072, Loss: 6.6517\n",
      "Iteration 394/1072, Loss: 6.7707\n",
      "Iteration 395/1072, Loss: 6.5504\n",
      "Iteration 396/1072, Loss: 6.7276\n",
      "Iteration 397/1072, Loss: 6.5916\n",
      "Iteration 398/1072, Loss: 6.2788\n",
      "Iteration 399/1072, Loss: 6.5796\n",
      "Iteration 400/1072, Loss: 6.5786\n",
      "Iteration 401/1072, Loss: 6.6120\n",
      "Iteration 402/1072, Loss: 6.6261\n",
      "Iteration 403/1072, Loss: 6.8104\n",
      "Iteration 404/1072, Loss: 6.3474\n",
      "Iteration 405/1072, Loss: 6.6216\n",
      "Iteration 406/1072, Loss: 6.8138\n",
      "Iteration 407/1072, Loss: 6.7940\n",
      "Iteration 408/1072, Loss: 6.7814\n",
      "Iteration 409/1072, Loss: 6.0545\n",
      "Iteration 410/1072, Loss: 6.6131\n",
      "Iteration 411/1072, Loss: 6.9737\n",
      "Iteration 412/1072, Loss: 6.7449\n",
      "Iteration 413/1072, Loss: 6.6677\n",
      "Iteration 414/1072, Loss: 6.4177\n",
      "Iteration 415/1072, Loss: 6.7506\n",
      "Iteration 416/1072, Loss: 6.9124\n",
      "Iteration 417/1072, Loss: 6.7784\n",
      "Iteration 418/1072, Loss: 6.9018\n",
      "Iteration 419/1072, Loss: 6.4562\n",
      "Iteration 420/1072, Loss: 6.7243\n",
      "Iteration 421/1072, Loss: 6.7860\n",
      "Iteration 422/1072, Loss: 6.7076\n",
      "Iteration 423/1072, Loss: 6.4573\n",
      "Iteration 424/1072, Loss: 6.7783\n",
      "Iteration 425/1072, Loss: 6.6988\n",
      "Iteration 426/1072, Loss: 6.9158\n",
      "Iteration 427/1072, Loss: 6.5974\n",
      "Iteration 428/1072, Loss: 6.8270\n",
      "Iteration 429/1072, Loss: 6.4369\n",
      "Iteration 430/1072, Loss: 6.7039\n",
      "Iteration 431/1072, Loss: 6.9472\n",
      "Iteration 432/1072, Loss: 7.0018\n",
      "Iteration 433/1072, Loss: 6.3746\n",
      "Iteration 434/1072, Loss: 6.6305\n",
      "Iteration 435/1072, Loss: 6.5738\n",
      "Iteration 436/1072, Loss: 6.6841\n",
      "Iteration 437/1072, Loss: 6.7522\n",
      "Iteration 438/1072, Loss: 6.8510\n",
      "Iteration 439/1072, Loss: 6.8666\n",
      "Iteration 440/1072, Loss: 7.0358\n",
      "Iteration 441/1072, Loss: 7.0185\n",
      "Iteration 442/1072, Loss: 6.8098\n",
      "Iteration 443/1072, Loss: 6.5578\n",
      "Iteration 444/1072, Loss: 6.8951\n",
      "Iteration 445/1072, Loss: 6.8889\n",
      "Iteration 446/1072, Loss: 6.5699\n",
      "Iteration 447/1072, Loss: 6.7070\n",
      "Iteration 448/1072, Loss: 6.6186\n",
      "Iteration 449/1072, Loss: 7.0025\n",
      "Iteration 450/1072, Loss: 6.9106\n",
      "Iteration 451/1072, Loss: 6.7684\n",
      "Iteration 452/1072, Loss: 6.6023\n",
      "Iteration 453/1072, Loss: 6.7196\n",
      "Iteration 454/1072, Loss: 6.7317\n",
      "Iteration 455/1072, Loss: 6.5190\n",
      "Iteration 456/1072, Loss: 6.6524\n",
      "Iteration 457/1072, Loss: 6.8078\n",
      "Iteration 458/1072, Loss: 6.6111\n",
      "Iteration 459/1072, Loss: 6.7306\n",
      "Iteration 460/1072, Loss: 6.8945\n",
      "Iteration 461/1072, Loss: 7.0667\n",
      "Iteration 462/1072, Loss: 6.4295\n",
      "Iteration 463/1072, Loss: 6.6135\n",
      "Iteration 464/1072, Loss: 6.6456\n",
      "Iteration 465/1072, Loss: 6.7138\n",
      "Iteration 466/1072, Loss: 6.6461\n",
      "Iteration 467/1072, Loss: 6.6816\n",
      "Iteration 468/1072, Loss: 6.7659\n",
      "Iteration 469/1072, Loss: 6.9374\n",
      "Iteration 470/1072, Loss: 6.4926\n",
      "Iteration 471/1072, Loss: 6.6496\n",
      "Iteration 472/1072, Loss: 6.4756\n",
      "Iteration 473/1072, Loss: 6.8450\n",
      "Iteration 474/1072, Loss: 6.6019\n",
      "Iteration 475/1072, Loss: 6.8003\n",
      "Iteration 476/1072, Loss: 6.6354\n",
      "Iteration 477/1072, Loss: 6.7246\n",
      "Iteration 478/1072, Loss: 6.5166\n",
      "Iteration 479/1072, Loss: 6.5232\n",
      "Iteration 480/1072, Loss: 6.8025\n",
      "Iteration 481/1072, Loss: 6.8823\n",
      "Iteration 482/1072, Loss: 6.6631\n",
      "Iteration 483/1072, Loss: 6.9807\n",
      "Iteration 484/1072, Loss: 6.7882\n",
      "Iteration 485/1072, Loss: 6.8087\n",
      "Iteration 486/1072, Loss: 6.8494\n",
      "Iteration 487/1072, Loss: 7.0475\n",
      "Iteration 488/1072, Loss: 6.8456\n",
      "Iteration 489/1072, Loss: 6.5641\n",
      "Iteration 490/1072, Loss: 6.4293\n",
      "Iteration 491/1072, Loss: 6.8733\n",
      "Iteration 492/1072, Loss: 6.9550\n",
      "Iteration 493/1072, Loss: 6.6762\n",
      "Iteration 494/1072, Loss: 6.7714\n",
      "Iteration 495/1072, Loss: 6.7374\n",
      "Iteration 496/1072, Loss: 6.7973\n",
      "Iteration 497/1072, Loss: 6.7896\n",
      "Iteration 498/1072, Loss: 6.8534\n",
      "Iteration 499/1072, Loss: 6.7121\n",
      "Iteration 500/1072, Loss: 6.5407\n",
      "Iteration 501/1072, Loss: 6.7583\n",
      "Iteration 502/1072, Loss: 6.5599\n",
      "Iteration 503/1072, Loss: 6.8475\n",
      "Iteration 504/1072, Loss: 6.7686\n",
      "Iteration 505/1072, Loss: 6.9522\n",
      "Iteration 506/1072, Loss: 6.3303\n",
      "Iteration 507/1072, Loss: 6.7717\n",
      "Iteration 508/1072, Loss: 6.4443\n",
      "Iteration 509/1072, Loss: 6.5253\n",
      "Iteration 510/1072, Loss: 6.5826\n",
      "Iteration 511/1072, Loss: 6.3393\n",
      "Iteration 512/1072, Loss: 6.8432\n",
      "Iteration 513/1072, Loss: 6.6808\n",
      "Iteration 514/1072, Loss: 6.6948\n",
      "Iteration 515/1072, Loss: 6.4820\n",
      "Iteration 516/1072, Loss: 6.8465\n",
      "Iteration 517/1072, Loss: 6.8107\n",
      "Iteration 518/1072, Loss: 6.6876\n",
      "Iteration 519/1072, Loss: 6.9180\n",
      "Iteration 520/1072, Loss: 6.4388\n",
      "Iteration 521/1072, Loss: 6.3997\n",
      "Iteration 522/1072, Loss: 6.5804\n",
      "Iteration 523/1072, Loss: 6.7576\n",
      "Iteration 524/1072, Loss: 6.6754\n",
      "Iteration 525/1072, Loss: 7.1278\n",
      "Iteration 526/1072, Loss: 6.8514\n",
      "Iteration 527/1072, Loss: 6.8249\n",
      "Iteration 528/1072, Loss: 6.9615\n",
      "Iteration 529/1072, Loss: 6.4320\n",
      "Iteration 530/1072, Loss: 6.7097\n",
      "Iteration 531/1072, Loss: 6.8642\n",
      "Iteration 532/1072, Loss: 6.8661\n",
      "Iteration 533/1072, Loss: 6.6721\n",
      "Iteration 534/1072, Loss: 6.9842\n",
      "Iteration 535/1072, Loss: 6.5869\n",
      "Iteration 536/1072, Loss: 6.6860\n",
      "Iteration 537/1072, Loss: 6.3920\n",
      "Iteration 538/1072, Loss: 6.9784\n",
      "Iteration 539/1072, Loss: 6.7270\n",
      "Iteration 540/1072, Loss: 6.6636\n",
      "Iteration 541/1072, Loss: 6.4360\n",
      "Iteration 542/1072, Loss: 6.4771\n",
      "Iteration 543/1072, Loss: 6.7477\n",
      "Iteration 544/1072, Loss: 6.5842\n",
      "Iteration 545/1072, Loss: 6.6121\n",
      "Iteration 546/1072, Loss: 6.7569\n",
      "Iteration 547/1072, Loss: 6.3974\n",
      "Iteration 548/1072, Loss: 6.6833\n",
      "Iteration 549/1072, Loss: 6.5844\n",
      "Iteration 550/1072, Loss: 6.7357\n",
      "Iteration 551/1072, Loss: 6.7785\n",
      "Iteration 552/1072, Loss: 6.7978\n",
      "Iteration 553/1072, Loss: 6.6196\n",
      "Iteration 554/1072, Loss: 6.6950\n",
      "Iteration 555/1072, Loss: 6.6567\n",
      "Iteration 556/1072, Loss: 6.3776\n",
      "Iteration 557/1072, Loss: 6.4544\n",
      "Iteration 558/1072, Loss: 6.6747\n",
      "Iteration 559/1072, Loss: 6.5468\n",
      "Iteration 560/1072, Loss: 6.7550\n",
      "Iteration 561/1072, Loss: 6.8075\n",
      "Iteration 562/1072, Loss: 6.5746\n",
      "Iteration 563/1072, Loss: 6.5635\n",
      "Iteration 564/1072, Loss: 6.9318\n",
      "Iteration 565/1072, Loss: 7.0887\n",
      "Iteration 566/1072, Loss: 6.5485\n",
      "Iteration 567/1072, Loss: 6.5809\n",
      "Iteration 568/1072, Loss: 7.1089\n",
      "Iteration 569/1072, Loss: 6.3318\n",
      "Iteration 570/1072, Loss: 6.6053\n",
      "Iteration 571/1072, Loss: 6.9085\n",
      "Iteration 572/1072, Loss: 6.7369\n",
      "Iteration 573/1072, Loss: 6.7726\n",
      "Iteration 574/1072, Loss: 6.6184\n",
      "Iteration 575/1072, Loss: 6.9706\n",
      "Iteration 576/1072, Loss: 6.6726\n",
      "Iteration 577/1072, Loss: 6.8549\n",
      "Iteration 578/1072, Loss: 6.9035\n",
      "Iteration 579/1072, Loss: 6.5997\n",
      "Iteration 580/1072, Loss: 6.9448\n",
      "Iteration 581/1072, Loss: 6.5905\n",
      "Iteration 582/1072, Loss: 6.8146\n",
      "Iteration 583/1072, Loss: 6.6540\n",
      "Iteration 584/1072, Loss: 6.6472\n",
      "Iteration 585/1072, Loss: 6.5955\n",
      "Iteration 586/1072, Loss: 6.8252\n",
      "Iteration 587/1072, Loss: 6.5658\n",
      "Iteration 588/1072, Loss: 6.8316\n",
      "Iteration 589/1072, Loss: 6.6946\n",
      "Iteration 590/1072, Loss: 6.4251\n",
      "Iteration 591/1072, Loss: 6.6828\n",
      "Iteration 592/1072, Loss: 6.6690\n",
      "Iteration 593/1072, Loss: 6.6301\n",
      "Iteration 594/1072, Loss: 6.7804\n",
      "Iteration 595/1072, Loss: 6.7608\n",
      "Iteration 596/1072, Loss: 6.4953\n",
      "Iteration 597/1072, Loss: 6.9472\n",
      "Iteration 598/1072, Loss: 6.5936\n",
      "Iteration 599/1072, Loss: 6.7229\n",
      "Iteration 600/1072, Loss: 6.6704\n",
      "Iteration 601/1072, Loss: 6.7148\n",
      "Iteration 602/1072, Loss: 6.7033\n",
      "Iteration 603/1072, Loss: 6.8231\n",
      "Iteration 604/1072, Loss: 6.7448\n",
      "Iteration 605/1072, Loss: 6.3761\n",
      "Iteration 606/1072, Loss: 6.8198\n",
      "Iteration 607/1072, Loss: 6.2675\n",
      "Iteration 608/1072, Loss: 6.6923\n",
      "Iteration 609/1072, Loss: 6.8456\n",
      "Iteration 610/1072, Loss: 6.6962\n",
      "Iteration 611/1072, Loss: 6.8713\n",
      "Iteration 612/1072, Loss: 6.5190\n",
      "Iteration 613/1072, Loss: 6.6154\n",
      "Iteration 614/1072, Loss: 6.4030\n",
      "Iteration 615/1072, Loss: 6.5267\n",
      "Iteration 616/1072, Loss: 6.5825\n",
      "Iteration 617/1072, Loss: 6.5481\n",
      "Iteration 618/1072, Loss: 6.6307\n",
      "Iteration 619/1072, Loss: 6.0512\n",
      "Iteration 620/1072, Loss: 6.4993\n",
      "Iteration 621/1072, Loss: 6.8416\n",
      "Iteration 622/1072, Loss: 6.4352\n",
      "Iteration 623/1072, Loss: 6.8274\n",
      "Iteration 624/1072, Loss: 7.1270\n",
      "Iteration 625/1072, Loss: 6.5022\n",
      "Iteration 626/1072, Loss: 6.5722\n",
      "Iteration 627/1072, Loss: 6.7003\n",
      "Iteration 628/1072, Loss: 6.7318\n",
      "Iteration 629/1072, Loss: 6.5957\n",
      "Iteration 630/1072, Loss: 6.5418\n",
      "Iteration 631/1072, Loss: 7.0391\n",
      "Iteration 632/1072, Loss: 6.5369\n",
      "Iteration 633/1072, Loss: 6.5290\n",
      "Iteration 634/1072, Loss: 6.4691\n",
      "Iteration 635/1072, Loss: 6.6523\n",
      "Iteration 636/1072, Loss: 6.4236\n",
      "Iteration 637/1072, Loss: 6.6556\n",
      "Iteration 638/1072, Loss: 6.9026\n",
      "Iteration 639/1072, Loss: 6.3252\n",
      "Iteration 640/1072, Loss: 6.7916\n",
      "Iteration 641/1072, Loss: 6.5875\n",
      "Iteration 642/1072, Loss: 6.9278\n",
      "Iteration 643/1072, Loss: 6.5870\n",
      "Iteration 644/1072, Loss: 6.6532\n",
      "Iteration 645/1072, Loss: 6.5029\n",
      "Iteration 646/1072, Loss: 6.4998\n",
      "Iteration 647/1072, Loss: 6.6323\n",
      "Iteration 648/1072, Loss: 6.2866\n",
      "Iteration 649/1072, Loss: 6.7082\n",
      "Iteration 650/1072, Loss: 6.3785\n",
      "Iteration 651/1072, Loss: 6.5701\n",
      "Iteration 652/1072, Loss: 6.6371\n",
      "Iteration 653/1072, Loss: 6.6002\n",
      "Iteration 654/1072, Loss: 6.3592\n",
      "Iteration 655/1072, Loss: 6.9796\n",
      "Iteration 656/1072, Loss: 6.5856\n",
      "Iteration 657/1072, Loss: 6.7553\n",
      "Iteration 658/1072, Loss: 6.5701\n",
      "Iteration 659/1072, Loss: 6.6999\n",
      "Iteration 660/1072, Loss: 6.7474\n",
      "Iteration 661/1072, Loss: 6.6712\n",
      "Iteration 662/1072, Loss: 6.1899\n",
      "Iteration 663/1072, Loss: 6.4715\n",
      "Iteration 664/1072, Loss: 6.9322\n",
      "Iteration 665/1072, Loss: 6.9029\n",
      "Iteration 666/1072, Loss: 6.5482\n",
      "Iteration 667/1072, Loss: 6.6569\n",
      "Iteration 668/1072, Loss: 6.6340\n",
      "Iteration 669/1072, Loss: 6.4232\n",
      "Iteration 670/1072, Loss: 6.7607\n",
      "Iteration 671/1072, Loss: 6.8581\n",
      "Iteration 673/1072, Loss: 6.5281\n",
      "Iteration 674/1072, Loss: 6.5169\n",
      "Iteration 675/1072, Loss: 6.7861\n",
      "Iteration 676/1072, Loss: 6.8587\n",
      "Iteration 677/1072, Loss: 6.8701\n",
      "Iteration 678/1072, Loss: 6.3854\n",
      "Iteration 679/1072, Loss: 6.6276\n",
      "Iteration 680/1072, Loss: 6.6105\n",
      "Iteration 681/1072, Loss: 6.5236\n",
      "Iteration 682/1072, Loss: 6.8485\n",
      "Iteration 683/1072, Loss: 6.9793\n",
      "Iteration 684/1072, Loss: 6.6759\n",
      "Iteration 685/1072, Loss: 6.2999\n",
      "Iteration 686/1072, Loss: 6.7329\n",
      "Iteration 687/1072, Loss: 6.5770\n",
      "Iteration 688/1072, Loss: 6.5928\n",
      "Iteration 689/1072, Loss: 6.8493\n",
      "Iteration 690/1072, Loss: 6.6932\n",
      "Iteration 691/1072, Loss: 6.8778\n",
      "Iteration 692/1072, Loss: 6.2246\n",
      "Iteration 693/1072, Loss: 6.4896\n",
      "Iteration 694/1072, Loss: 6.5521\n",
      "Iteration 695/1072, Loss: 6.8433\n",
      "Iteration 696/1072, Loss: 6.8001\n",
      "Iteration 697/1072, Loss: 6.6696\n",
      "Iteration 698/1072, Loss: 6.7717\n",
      "Iteration 699/1072, Loss: 6.7979\n",
      "Iteration 700/1072, Loss: 6.5637\n",
      "Iteration 701/1072, Loss: 6.5658\n",
      "Iteration 702/1072, Loss: 6.5643\n",
      "Iteration 703/1072, Loss: 6.9144\n",
      "Iteration 704/1072, Loss: 6.6935\n",
      "Iteration 705/1072, Loss: 6.8724\n",
      "Iteration 706/1072, Loss: 6.4394\n",
      "Iteration 707/1072, Loss: 6.6698\n",
      "Iteration 708/1072, Loss: 6.6755\n",
      "Iteration 709/1072, Loss: 6.2661\n",
      "Iteration 710/1072, Loss: 6.6779\n",
      "Iteration 711/1072, Loss: 6.8222\n",
      "Iteration 712/1072, Loss: 6.6064\n",
      "Iteration 713/1072, Loss: 6.9453\n",
      "Iteration 714/1072, Loss: 6.7163\n",
      "Iteration 715/1072, Loss: 6.6667\n",
      "Iteration 716/1072, Loss: 6.8605\n",
      "Iteration 717/1072, Loss: 6.9032\n",
      "Iteration 718/1072, Loss: 6.6811\n",
      "Iteration 719/1072, Loss: 6.5368\n",
      "Iteration 720/1072, Loss: 6.6894\n",
      "Iteration 721/1072, Loss: 6.6328\n",
      "Iteration 722/1072, Loss: 6.6152\n",
      "Iteration 723/1072, Loss: 6.5020\n",
      "Iteration 724/1072, Loss: 6.3191\n",
      "Iteration 725/1072, Loss: 6.5316\n",
      "Iteration 726/1072, Loss: 6.4447\n",
      "Iteration 727/1072, Loss: 6.7563\n",
      "Iteration 728/1072, Loss: 6.9683\n",
      "Iteration 729/1072, Loss: 6.5495\n",
      "Iteration 730/1072, Loss: 6.7727\n",
      "Iteration 731/1072, Loss: 6.6892\n",
      "Iteration 732/1072, Loss: 6.6365\n",
      "Iteration 733/1072, Loss: 6.5958\n",
      "Iteration 734/1072, Loss: 6.6877\n",
      "Iteration 735/1072, Loss: 6.9325\n",
      "Iteration 736/1072, Loss: 6.6006\n",
      "Iteration 777/1072, Loss: 6.5683\n",
      "Iteration 778/1072, Loss: 6.7071\n",
      "Iteration 779/1072, Loss: 6.7000\n",
      "Iteration 780/1072, Loss: 6.5031\n",
      "Iteration 781/1072, Loss: 6.9128\n",
      "Iteration 782/1072, Loss: 6.5505\n",
      "Iteration 783/1072, Loss: 6.4338\n",
      "Iteration 784/1072, Loss: 6.5085\n",
      "Iteration 785/1072, Loss: 6.6291\n",
      "Iteration 786/1072, Loss: 6.4856\n",
      "Iteration 787/1072, Loss: 6.5689\n",
      "Iteration 788/1072, Loss: 6.8656\n",
      "Iteration 789/1072, Loss: 6.5031\n",
      "Iteration 790/1072, Loss: 6.2883\n",
      "Iteration 791/1072, Loss: 6.9529\n",
      "Iteration 792/1072, Loss: 6.5879\n",
      "Iteration 793/1072, Loss: 6.4329\n",
      "Iteration 794/1072, Loss: 6.8371\n",
      "Iteration 795/1072, Loss: 6.7286\n",
      "Iteration 796/1072, Loss: 6.8748\n",
      "Iteration 797/1072, Loss: 6.7304\n",
      "Iteration 798/1072, Loss: 6.9531\n",
      "Iteration 799/1072, Loss: 6.3764\n",
      "Iteration 800/1072, Loss: 6.4993\n",
      "Iteration 801/1072, Loss: 6.5640\n",
      "Iteration 802/1072, Loss: 6.2953\n",
      "Iteration 803/1072, Loss: 6.9508\n",
      "Iteration 804/1072, Loss: 6.4541\n",
      "Iteration 805/1072, Loss: 6.6278\n",
      "Iteration 806/1072, Loss: 6.5542\n",
      "Iteration 807/1072, Loss: 6.8657\n",
      "Iteration 808/1072, Loss: 6.5208\n",
      "Iteration 809/1072, Loss: 6.6107\n",
      "Iteration 810/1072, Loss: 6.6538\n",
      "Iteration 811/1072, Loss: 6.5169\n",
      "Iteration 812/1072, Loss: 6.6959\n",
      "Iteration 813/1072, Loss: 6.8618\n",
      "Iteration 814/1072, Loss: 6.6191\n",
      "Iteration 815/1072, Loss: 6.6601\n",
      "Iteration 816/1072, Loss: 6.7294\n",
      "Iteration 817/1072, Loss: 6.7273\n",
      "Iteration 818/1072, Loss: 6.4898\n",
      "Iteration 819/1072, Loss: 6.4884\n",
      "Iteration 820/1072, Loss: 6.3487\n",
      "Iteration 821/1072, Loss: 6.8166\n",
      "Iteration 822/1072, Loss: 6.0739\n",
      "Iteration 823/1072, Loss: 6.5704\n",
      "Iteration 824/1072, Loss: 6.6460\n",
      "Iteration 825/1072, Loss: 6.7029\n",
      "Iteration 826/1072, Loss: 6.7475\n",
      "Iteration 827/1072, Loss: 6.5293\n",
      "Iteration 828/1072, Loss: 6.5721\n",
      "Iteration 829/1072, Loss: 6.7659\n",
      "Iteration 830/1072, Loss: 6.6951\n",
      "Iteration 831/1072, Loss: 6.6408\n",
      "Iteration 832/1072, Loss: 6.5407\n",
      "Iteration 833/1072, Loss: 6.6545\n",
      "Iteration 834/1072, Loss: 6.5578\n",
      "Iteration 835/1072, Loss: 6.8365\n",
      "Iteration 836/1072, Loss: 6.5753\n",
      "Iteration 837/1072, Loss: 6.9283\n",
      "Iteration 838/1072, Loss: 6.4358\n",
      "Iteration 839/1072, Loss: 6.4552\n",
      "Iteration 840/1072, Loss: 6.7128\n",
      "Iteration 841/1072, Loss: 6.5805\n",
      "Iteration 842/1072, Loss: 6.7224\n",
      "Iteration 843/1072, Loss: 6.8523\n",
      "Iteration 844/1072, Loss: 6.5275\n",
      "Iteration 845/1072, Loss: 6.3144\n",
      "Iteration 846/1072, Loss: 6.8829\n",
      "Iteration 847/1072, Loss: 6.0553\n",
      "Iteration 848/1072, Loss: 6.7508\n",
      "Iteration 849/1072, Loss: 6.9377\n",
      "Iteration 850/1072, Loss: 6.4710\n",
      "Iteration 851/1072, Loss: 6.6041\n",
      "Iteration 852/1072, Loss: 6.3239\n",
      "Iteration 853/1072, Loss: 6.6518\n",
      "Iteration 854/1072, Loss: 6.7574\n",
      "Iteration 855/1072, Loss: 6.6786\n",
      "Iteration 856/1072, Loss: 6.6543\n",
      "Iteration 857/1072, Loss: 6.0954\n",
      "Iteration 858/1072, Loss: 6.2472\n",
      "Iteration 859/1072, Loss: 6.6839\n",
      "Iteration 860/1072, Loss: 6.6745\n",
      "Iteration 861/1072, Loss: 6.6408\n",
      "Iteration 862/1072, Loss: 6.6365\n",
      "Iteration 863/1072, Loss: 6.7018\n",
      "Iteration 864/1072, Loss: 6.8290\n",
      "Iteration 865/1072, Loss: 6.5721\n",
      "Iteration 866/1072, Loss: 6.5453\n",
      "Iteration 867/1072, Loss: 6.7233\n",
      "Iteration 868/1072, Loss: 6.5380\n",
      "Iteration 869/1072, Loss: 6.1497\n",
      "Iteration 870/1072, Loss: 6.4967\n",
      "Iteration 871/1072, Loss: 6.5776\n",
      "Iteration 872/1072, Loss: 6.2110\n",
      "Iteration 873/1072, Loss: 6.6618\n",
      "Iteration 874/1072, Loss: 6.9246\n",
      "Iteration 875/1072, Loss: 6.7384\n",
      "Iteration 876/1072, Loss: 7.0228\n",
      "Iteration 877/1072, Loss: 6.5243\n",
      "Iteration 878/1072, Loss: 6.5753\n",
      "Iteration 879/1072, Loss: 6.6260\n",
      "Iteration 880/1072, Loss: 6.5913\n",
      "Iteration 881/1072, Loss: 6.6262\n",
      "Iteration 882/1072, Loss: 6.6193\n",
      "Iteration 883/1072, Loss: 6.6109\n",
      "Iteration 884/1072, Loss: 6.5507\n",
      "Iteration 885/1072, Loss: 6.5111\n",
      "Iteration 886/1072, Loss: 6.7406\n",
      "Iteration 887/1072, Loss: 6.5190\n",
      "Iteration 888/1072, Loss: 6.3087\n",
      "Iteration 889/1072, Loss: 6.8878\n",
      "Iteration 890/1072, Loss: 6.5702\n",
      "Iteration 891/1072, Loss: 6.4874\n",
      "Iteration 892/1072, Loss: 6.6258\n",
      "Iteration 893/1072, Loss: 6.9627\n",
      "Iteration 894/1072, Loss: 6.4394\n",
      "Iteration 895/1072, Loss: 6.9470\n",
      "Iteration 896/1072, Loss: 6.0330\n",
      "Iteration 897/1072, Loss: 6.8413\n",
      "Iteration 898/1072, Loss: 6.8970\n",
      "Iteration 899/1072, Loss: 6.5574\n",
      "Iteration 900/1072, Loss: 6.4917\n",
      "Iteration 901/1072, Loss: 6.7276\n",
      "Iteration 902/1072, Loss: 6.4070\n",
      "Iteration 903/1072, Loss: 6.3882\n",
      "Iteration 904/1072, Loss: 6.4643\n",
      "Iteration 905/1072, Loss: 6.1184\n",
      "Iteration 906/1072, Loss: 6.5995\n",
      "Iteration 907/1072, Loss: 6.8645\n",
      "Iteration 908/1072, Loss: 6.3189\n",
      "Iteration 909/1072, Loss: 6.7947\n",
      "Iteration 910/1072, Loss: 6.4470\n",
      "Iteration 911/1072, Loss: 6.4938\n",
      "Iteration 912/1072, Loss: 6.7196\n",
      "Iteration 913/1072, Loss: 6.3103\n",
      "Iteration 914/1072, Loss: 6.5964\n",
      "Iteration 915/1072, Loss: 6.7532\n",
      "Iteration 916/1072, Loss: 6.6399\n",
      "Iteration 917/1072, Loss: 6.7016\n",
      "Iteration 918/1072, Loss: 6.8203\n",
      "Iteration 919/1072, Loss: 6.4739\n",
      "Iteration 920/1072, Loss: 7.0396\n",
      "Iteration 921/1072, Loss: 6.5434\n",
      "Iteration 922/1072, Loss: 6.5726\n",
      "Iteration 923/1072, Loss: 6.6872\n",
      "Iteration 924/1072, Loss: 6.5276\n",
      "Iteration 925/1072, Loss: 6.1431\n",
      "Iteration 926/1072, Loss: 6.5274\n",
      "Iteration 927/1072, Loss: 6.7577\n",
      "Iteration 928/1072, Loss: 6.6902\n",
      "Iteration 929/1072, Loss: 6.2503\n",
      "Iteration 930/1072, Loss: 6.5814\n",
      "Iteration 931/1072, Loss: 6.4229\n",
      "Iteration 932/1072, Loss: 6.7425\n",
      "Iteration 933/1072, Loss: 6.6027\n",
      "Iteration 934/1072, Loss: 6.2696\n",
      "Iteration 935/1072, Loss: 6.5088\n",
      "Iteration 936/1072, Loss: 6.7688\n",
      "Iteration 937/1072, Loss: 6.6314\n",
      "Iteration 938/1072, Loss: 6.7049\n",
      "Iteration 939/1072, Loss: 6.7136\n",
      "Iteration 940/1072, Loss: 6.9217\n",
      "Iteration 941/1072, Loss: 6.6048\n",
      "Iteration 942/1072, Loss: 6.5346\n",
      "Iteration 943/1072, Loss: 6.4645\n",
      "Iteration 944/1072, Loss: 6.6278\n",
      "Iteration 945/1072, Loss: 6.3807\n",
      "Iteration 946/1072, Loss: 6.4243\n",
      "Iteration 947/1072, Loss: 6.7031\n",
      "Iteration 948/1072, Loss: 6.3237\n",
      "Iteration 949/1072, Loss: 6.7678\n",
      "Iteration 950/1072, Loss: 6.6795\n",
      "Iteration 951/1072, Loss: 6.5381\n",
      "Iteration 952/1072, Loss: 6.4845\n",
      "Iteration 953/1072, Loss: 6.8871\n",
      "Iteration 954/1072, Loss: 6.7071\n",
      "Iteration 955/1072, Loss: 6.6214\n",
      "Iteration 956/1072, Loss: 6.8224\n",
      "Iteration 957/1072, Loss: 6.4436\n",
      "Iteration 958/1072, Loss: 6.6971\n",
      "Iteration 959/1072, Loss: 6.5572\n",
      "Iteration 960/1072, Loss: 6.5356\n",
      "Iteration 961/1072, Loss: 6.6041\n",
      "Iteration 962/1072, Loss: 6.8026\n",
      "Iteration 963/1072, Loss: 6.6712\n",
      "Iteration 964/1072, Loss: 6.6945\n",
      "Iteration 965/1072, Loss: 6.7426\n",
      "Iteration 966/1072, Loss: 6.5221\n",
      "Iteration 967/1072, Loss: 6.9395\n",
      "Iteration 968/1072, Loss: 6.6252\n",
      "Iteration 969/1072, Loss: 6.8322\n",
      "Iteration 970/1072, Loss: 6.6356\n",
      "Iteration 971/1072, Loss: 6.1733\n",
      "Iteration 972/1072, Loss: 6.5537\n",
      "Iteration 973/1072, Loss: 6.2872\n",
      "Iteration 974/1072, Loss: 6.6709\n",
      "Iteration 975/1072, Loss: 6.5606\n",
      "Iteration 976/1072, Loss: 6.5158\n",
      "Iteration 977/1072, Loss: 6.6596\n",
      "Iteration 978/1072, Loss: 6.4573\n",
      "Iteration 979/1072, Loss: 6.4669\n",
      "Iteration 980/1072, Loss: 7.1120\n",
      "Iteration 981/1072, Loss: 6.4731\n",
      "Iteration 982/1072, Loss: 6.1784\n",
      "Iteration 983/1072, Loss: 6.6632\n",
      "Iteration 984/1072, Loss: 6.4755\n",
      "Iteration 985/1072, Loss: 6.6372\n",
      "Iteration 986/1072, Loss: 6.5789\n",
      "Iteration 987/1072, Loss: 6.4249\n",
      "Iteration 988/1072, Loss: 6.2280\n",
      "Iteration 989/1072, Loss: 6.7334\n",
      "Iteration 990/1072, Loss: 6.7179\n",
      "Iteration 991/1072, Loss: 6.5779\n",
      "Iteration 992/1072, Loss: 6.2693\n",
      "Iteration 993/1072, Loss: 6.3545\n",
      "Iteration 994/1072, Loss: 6.5306\n",
      "Iteration 995/1072, Loss: 6.9789\n",
      "Iteration 996/1072, Loss: 6.7014\n",
      "Iteration 997/1072, Loss: 6.7326\n",
      "Iteration 998/1072, Loss: 6.7389\n",
      "Iteration 999/1072, Loss: 6.5821\n",
      "Iteration 1000/1072, Loss: 6.7143\n",
      "Iteration 1001/1072, Loss: 6.6911\n",
      "Iteration 1002/1072, Loss: 6.4974\n",
      "Iteration 1003/1072, Loss: 6.2892\n",
      "Iteration 1004/1072, Loss: 6.5343\n",
      "Iteration 1005/1072, Loss: 6.4592\n",
      "Iteration 1006/1072, Loss: 6.7810\n",
      "Iteration 1007/1072, Loss: 6.6868\n",
      "Iteration 1008/1072, Loss: 6.6411\n",
      "Iteration 1009/1072, Loss: 6.5387\n",
      "Iteration 1010/1072, Loss: 6.6766\n",
      "Iteration 1011/1072, Loss: 6.7109\n",
      "Iteration 1012/1072, Loss: 6.4637\n",
      "Iteration 1013/1072, Loss: 6.6718\n",
      "Iteration 1014/1072, Loss: 6.3481\n",
      "Iteration 1015/1072, Loss: 6.6419\n",
      "Iteration 1016/1072, Loss: 6.6735\n",
      "Iteration 1017/1072, Loss: 6.5465\n",
      "Iteration 1018/1072, Loss: 6.7001\n",
      "Iteration 1019/1072, Loss: 6.5604\n",
      "Iteration 1020/1072, Loss: 6.5116\n",
      "Iteration 1021/1072, Loss: 6.7650\n",
      "Iteration 1022/1072, Loss: 6.8575\n",
      "Iteration 1023/1072, Loss: 6.0914\n",
      "Iteration 1024/1072, Loss: 6.3426\n",
      "Iteration 1025/1072, Loss: 6.6083\n",
      "Iteration 1026/1072, Loss: 6.3834\n",
      "Iteration 1027/1072, Loss: 6.7220\n",
      "Iteration 1028/1072, Loss: 6.7572\n",
      "Iteration 1029/1072, Loss: 6.0662\n",
      "Iteration 1030/1072, Loss: 6.4193\n",
      "Iteration 1031/1072, Loss: 6.7686\n",
      "Iteration 1032/1072, Loss: 6.8162\n",
      "Iteration 1033/1072, Loss: 6.7896\n",
      "Iteration 1034/1072, Loss: 6.4232\n",
      "Iteration 1035/1072, Loss: 6.8126\n",
      "Iteration 1036/1072, Loss: 6.3305\n",
      "Iteration 1037/1072, Loss: 6.4837\n",
      "Iteration 1038/1072, Loss: 6.4059\n",
      "Iteration 1039/1072, Loss: 6.6801\n",
      "Iteration 1040/1072, Loss: 6.6115\n",
      "Iteration 1041/1072, Loss: 6.8240\n",
      "Iteration 1042/1072, Loss: 6.7849\n",
      "Iteration 1043/1072, Loss: 6.6716\n",
      "Iteration 1044/1072, Loss: 6.5400\n",
      "Iteration 1045/1072, Loss: 6.5321\n",
      "Iteration 1046/1072, Loss: 6.6917\n",
      "Iteration 1047/1072, Loss: 6.7763\n",
      "Iteration 1048/1072, Loss: 6.5364\n",
      "Iteration 1049/1072, Loss: 6.7617\n",
      "Iteration 1050/1072, Loss: 6.2717\n",
      "Iteration 1051/1072, Loss: 6.8944\n",
      "Iteration 1052/1072, Loss: 6.4609\n",
      "Iteration 1053/1072, Loss: 6.5475\n",
      "Iteration 1054/1072, Loss: 6.4668\n",
      "Iteration 1055/1072, Loss: 6.7714\n",
      "Iteration 1056/1072, Loss: 6.3083\n",
      "Iteration 1057/1072, Loss: 6.4254\n",
      "Iteration 1058/1072, Loss: 6.5782\n",
      "Iteration 1059/1072, Loss: 6.2275\n",
      "Iteration 1060/1072, Loss: 6.7137\n",
      "Iteration 1061/1072, Loss: 6.3402\n",
      "Iteration 1062/1072, Loss: 6.5179\n",
      "Iteration 1063/1072, Loss: 6.4281\n",
      "Iteration 1064/1072, Loss: 7.0077\n",
      "Iteration 1065/1072, Loss: 6.4085\n",
      "Iteration 1066/1072, Loss: 6.4864\n",
      "Iteration 1067/1072, Loss: 6.7338\n",
      "Iteration 1068/1072, Loss: 6.6611\n",
      "Iteration 1069/1072, Loss: 6.2435\n",
      "Iteration 1070/1072, Loss: 6.6331\n",
      "Iteration 1071/1072, Loss: 6.4688\n",
      "Iteration 1072/1072, Loss: 7.3803\n",
      "Epoch 5/10, Loss: 6.6781\n",
      "Validation Accuracy: 11.08%\n",
      "Model checkpoint saved!\n",
      "Iteration 1/1072, Loss: 6.3968\n",
      "Iteration 2/1072, Loss: 6.4575\n",
      "Iteration 3/1072, Loss: 6.1558\n",
      "Iteration 4/1072, Loss: 6.3594\n",
      "Iteration 5/1072, Loss: 6.0762\n",
      "Iteration 6/1072, Loss: 6.3592\n",
      "Iteration 7/1072, Loss: 6.4447\n",
      "Iteration 8/1072, Loss: 6.3482\n",
      "Iteration 9/1072, Loss: 6.2737\n",
      "Iteration 10/1072, Loss: 6.5133\n",
      "Iteration 11/1072, Loss: 6.3734\n",
      "Iteration 12/1072, Loss: 6.4344\n",
      "Iteration 13/1072, Loss: 6.3288\n",
      "Iteration 14/1072, Loss: 6.3967\n",
      "Iteration 15/1072, Loss: 5.8594\n",
      "Iteration 16/1072, Loss: 6.3615\n",
      "Iteration 17/1072, Loss: 6.4078\n",
      "Iteration 18/1072, Loss: 6.4168\n",
      "Iteration 19/1072, Loss: 6.6086\n",
      "Iteration 20/1072, Loss: 6.0306\n",
      "Iteration 21/1072, Loss: 6.4364\n",
      "Iteration 22/1072, Loss: 6.5448\n",
      "Iteration 23/1072, Loss: 6.0082\n",
      "Iteration 24/1072, Loss: 6.4637\n",
      "Iteration 25/1072, Loss: 6.3592\n",
      "Iteration 26/1072, Loss: 6.2595\n",
      "Iteration 27/1072, Loss: 5.9845\n",
      "Iteration 28/1072, Loss: 6.5168\n",
      "Iteration 29/1072, Loss: 6.6734\n",
      "Iteration 30/1072, Loss: 6.7003\n",
      "Iteration 31/1072, Loss: 6.4748\n",
      "Iteration 32/1072, Loss: 6.5294\n",
      "Iteration 33/1072, Loss: 6.1935\n",
      "Iteration 34/1072, Loss: 6.0743\n",
      "Iteration 35/1072, Loss: 6.4870\n",
      "Iteration 36/1072, Loss: 6.2986\n",
      "Iteration 37/1072, Loss: 6.5360\n",
      "Iteration 38/1072, Loss: 6.2071\n",
      "Iteration 39/1072, Loss: 6.2054\n",
      "Iteration 40/1072, Loss: 6.4732\n",
      "Iteration 41/1072, Loss: 6.3568\n",
      "Iteration 42/1072, Loss: 6.5949\n",
      "Iteration 43/1072, Loss: 6.1813\n",
      "Iteration 44/1072, Loss: 6.4445\n",
      "Iteration 45/1072, Loss: 5.7469\n",
      "Iteration 46/1072, Loss: 6.2591\n",
      "Iteration 47/1072, Loss: 6.1882\n",
      "Iteration 48/1072, Loss: 6.4170\n",
      "Iteration 49/1072, Loss: 6.1792\n",
      "Iteration 50/1072, Loss: 6.0678\n",
      "Iteration 51/1072, Loss: 6.2329\n",
      "Iteration 52/1072, Loss: 6.4745\n",
      "Iteration 53/1072, Loss: 6.5380\n",
      "Iteration 54/1072, Loss: 6.2674\n",
      "Iteration 55/1072, Loss: 6.1105\n",
      "Iteration 56/1072, Loss: 6.3629\n",
      "Iteration 57/1072, Loss: 6.2532\n",
      "Iteration 58/1072, Loss: 6.2466\n",
      "Iteration 59/1072, Loss: 6.5549\n",
      "Iteration 60/1072, Loss: 6.3789\n",
      "Iteration 61/1072, Loss: 6.2445\n",
      "Iteration 62/1072, Loss: 6.5889\n",
      "Iteration 63/1072, Loss: 6.2205\n",
      "Iteration 64/1072, Loss: 6.0402\n",
      "Iteration 65/1072, Loss: 6.2784\n",
      "Iteration 66/1072, Loss: 5.7505\n",
      "Iteration 67/1072, Loss: 6.1628\n",
      "Iteration 68/1072, Loss: 6.2472\n",
      "Iteration 69/1072, Loss: 6.2130\n",
      "Iteration 70/1072, Loss: 6.5944\n",
      "Iteration 71/1072, Loss: 6.2128\n",
      "Iteration 72/1072, Loss: 6.5309\n",
      "Iteration 73/1072, Loss: 6.1360\n",
      "Iteration 74/1072, Loss: 6.3273\n",
      "Iteration 75/1072, Loss: 6.3380\n",
      "Iteration 76/1072, Loss: 6.2613\n",
      "Iteration 77/1072, Loss: 6.4091\n",
      "Iteration 78/1072, Loss: 6.3801\n",
      "Iteration 79/1072, Loss: 6.7731\n",
      "Iteration 80/1072, Loss: 6.2938\n",
      "Iteration 81/1072, Loss: 6.3368\n",
      "Iteration 82/1072, Loss: 6.2341\n",
      "Iteration 83/1072, Loss: 6.3870\n",
      "Iteration 84/1072, Loss: 6.3417\n",
      "Iteration 85/1072, Loss: 6.0872\n",
      "Iteration 86/1072, Loss: 6.4281\n",
      "Iteration 87/1072, Loss: 6.4383\n",
      "Iteration 88/1072, Loss: 6.2096\n",
      "Iteration 89/1072, Loss: 6.5487\n",
      "Iteration 90/1072, Loss: 6.0174\n",
      "Iteration 91/1072, Loss: 6.4340\n",
      "Iteration 92/1072, Loss: 6.5291\n",
      "Iteration 93/1072, Loss: 6.2871\n",
      "Iteration 94/1072, Loss: 6.1350\n",
      "Iteration 95/1072, Loss: 6.1962\n",
      "Iteration 96/1072, Loss: 6.5371\n",
      "Iteration 97/1072, Loss: 6.1209\n",
      "Iteration 98/1072, Loss: 6.3668\n",
      "Iteration 99/1072, Loss: 6.2233\n",
      "Iteration 254/1072, Loss: 6.3068\n",
      "Iteration 255/1072, Loss: 6.1032\n",
      "Iteration 256/1072, Loss: 6.0641\n",
      "Iteration 257/1072, Loss: 6.1531\n",
      "Iteration 258/1072, Loss: 6.4453\n",
      "Iteration 259/1072, Loss: 6.2407\n",
      "Iteration 260/1072, Loss: 6.0285\n",
      "Iteration 261/1072, Loss: 6.3784\n",
      "Iteration 262/1072, Loss: 6.1598\n",
      "Iteration 263/1072, Loss: 6.0569\n",
      "Iteration 264/1072, Loss: 6.7068\n",
      "Iteration 265/1072, Loss: 6.4413\n",
      "Iteration 266/1072, Loss: 6.5975\n",
      "Iteration 267/1072, Loss: 6.5356\n",
      "Iteration 268/1072, Loss: 6.3144\n",
      "Iteration 269/1072, Loss: 6.6548\n",
      "Iteration 270/1072, Loss: 5.8175\n",
      "Iteration 271/1072, Loss: 6.2549\n",
      "Iteration 272/1072, Loss: 6.3434\n",
      "Iteration 273/1072, Loss: 6.1967\n",
      "Iteration 274/1072, Loss: 5.7802\n",
      "Iteration 275/1072, Loss: 6.8608\n",
      "Iteration 276/1072, Loss: 5.9896\n",
      "Iteration 277/1072, Loss: 6.3845\n",
      "Iteration 278/1072, Loss: 5.8410\n",
      "Iteration 279/1072, Loss: 5.8868\n",
      "Iteration 280/1072, Loss: 6.2942\n",
      "Iteration 281/1072, Loss: 6.3513\n",
      "Iteration 282/1072, Loss: 6.2152\n",
      "Iteration 283/1072, Loss: 6.4101\n",
      "Iteration 284/1072, Loss: 6.6822\n",
      "Iteration 285/1072, Loss: 6.0965\n",
      "Iteration 286/1072, Loss: 6.3554\n",
      "Iteration 287/1072, Loss: 6.4030\n",
      "Iteration 288/1072, Loss: 6.2098\n",
      "Iteration 289/1072, Loss: 6.3753\n",
      "Iteration 290/1072, Loss: 6.3730\n",
      "Iteration 291/1072, Loss: 6.2773\n",
      "Iteration 292/1072, Loss: 6.5061\n",
      "Iteration 293/1072, Loss: 5.9842\n",
      "Iteration 294/1072, Loss: 5.9846\n",
      "Iteration 295/1072, Loss: 6.0886\n",
      "Iteration 296/1072, Loss: 6.2264\n",
      "Iteration 297/1072, Loss: 6.6388\n",
      "Iteration 298/1072, Loss: 6.5141\n",
      "Iteration 299/1072, Loss: 6.3372\n",
      "Iteration 300/1072, Loss: 6.3278\n",
      "Iteration 301/1072, Loss: 6.2299\n",
      "Iteration 302/1072, Loss: 6.1297\n",
      "Iteration 303/1072, Loss: 6.0145\n",
      "Iteration 304/1072, Loss: 6.5868\n",
      "Iteration 305/1072, Loss: 6.5986\n",
      "Iteration 306/1072, Loss: 6.4430\n",
      "Iteration 307/1072, Loss: 6.4843\n",
      "Iteration 308/1072, Loss: 6.0897\n",
      "Iteration 309/1072, Loss: 6.3506\n",
      "Iteration 310/1072, Loss: 6.1225\n",
      "Iteration 311/1072, Loss: 6.6877\n",
      "Iteration 312/1072, Loss: 6.1020\n",
      "Iteration 313/1072, Loss: 5.9943\n",
      "Iteration 314/1072, Loss: 6.1343\n",
      "Iteration 315/1072, Loss: 6.3899\n",
      "Iteration 316/1072, Loss: 6.5502\n",
      "Iteration 317/1072, Loss: 6.0565\n",
      "Iteration 318/1072, Loss: 6.0398\n",
      "Iteration 319/1072, Loss: 6.2678\n",
      "Iteration 320/1072, Loss: 6.3832\n",
      "Iteration 321/1072, Loss: 6.0986\n",
      "Iteration 322/1072, Loss: 6.0718\n",
      "Iteration 323/1072, Loss: 6.5028\n",
      "Iteration 324/1072, Loss: 6.4292\n",
      "Iteration 325/1072, Loss: 6.1507\n",
      "Iteration 326/1072, Loss: 6.2223\n",
      "Iteration 327/1072, Loss: 6.2243\n",
      "Iteration 328/1072, Loss: 5.8218\n",
      "Iteration 329/1072, Loss: 6.3839\n",
      "Iteration 330/1072, Loss: 6.5416\n",
      "Iteration 331/1072, Loss: 6.3841\n",
      "Iteration 332/1072, Loss: 6.6583\n",
      "Iteration 333/1072, Loss: 6.4577\n",
      "Iteration 334/1072, Loss: 5.9356\n",
      "Iteration 335/1072, Loss: 6.1347\n",
      "Iteration 336/1072, Loss: 6.2288\n",
      "Iteration 337/1072, Loss: 5.8761\n",
      "Iteration 338/1072, Loss: 6.0275\n",
      "Iteration 339/1072, Loss: 6.4786\n",
      "Iteration 340/1072, Loss: 6.3486\n",
      "Iteration 341/1072, Loss: 6.2975\n",
      "Iteration 342/1072, Loss: 6.4876\n",
      "Iteration 343/1072, Loss: 6.3329\n",
      "Iteration 344/1072, Loss: 6.0116\n",
      "Iteration 345/1072, Loss: 6.2353\n",
      "Iteration 346/1072, Loss: 6.3894\n",
      "Iteration 347/1072, Loss: 6.4259\n",
      "Iteration 348/1072, Loss: 6.3681\n",
      "Iteration 349/1072, Loss: 6.5433\n",
      "Iteration 350/1072, Loss: 6.1147\n",
      "Iteration 351/1072, Loss: 6.4762\n",
      "Iteration 352/1072, Loss: 6.2453\n",
      "Iteration 353/1072, Loss: 6.2350\n",
      "Iteration 354/1072, Loss: 6.0014\n",
      "Iteration 355/1072, Loss: 6.0072\n",
      "Iteration 356/1072, Loss: 6.2329\n",
      "Iteration 357/1072, Loss: 6.3985\n",
      "Iteration 358/1072, Loss: 6.4489\n",
      "Iteration 359/1072, Loss: 5.7270\n",
      "Iteration 360/1072, Loss: 6.3183\n",
      "Iteration 361/1072, Loss: 6.2720\n",
      "Iteration 362/1072, Loss: 6.3252\n",
      "Iteration 363/1072, Loss: 6.3867\n",
      "Iteration 867/1072, Loss: 6.5963\n",
      "Iteration 27/1072, Loss: 6.0251\n",
      "Iteration 28/1072, Loss: 5.5996\n",
      "Iteration 29/1072, Loss: 5.8272\n",
      "Iteration 30/1072, Loss: 5.8116\n",
      "Iteration 31/1072, Loss: 5.4427\n",
      "Iteration 32/1072, Loss: 5.9801\n",
      "Iteration 33/1072, Loss: 5.6072\n",
      "Iteration 34/1072, Loss: 6.1230\n",
      "Iteration 35/1072, Loss: 5.7804\n",
      "Iteration 36/1072, Loss: 5.9990\n",
      "Iteration 37/1072, Loss: 5.8399\n",
      "Iteration 38/1072, Loss: 5.5518\n",
      "Iteration 39/1072, Loss: 5.6322\n",
      "Iteration 40/1072, Loss: 6.2473\n",
      "Iteration 41/1072, Loss: 5.7315\n",
      "Iteration 42/1072, Loss: 5.9750\n",
      "Iteration 43/1072, Loss: 5.7427\n",
      "Iteration 44/1072, Loss: 6.0020\n",
      "Iteration 45/1072, Loss: 5.6283\n",
      "Iteration 46/1072, Loss: 5.7797\n",
      "Iteration 47/1072, Loss: 5.8560\n",
      "Iteration 48/1072, Loss: 5.8229\n",
      "Iteration 49/1072, Loss: 6.1796\n",
      "Iteration 50/1072, Loss: 5.7708\n",
      "Iteration 51/1072, Loss: 5.7481\n",
      "Iteration 52/1072, Loss: 6.1222\n",
      "Iteration 53/1072, Loss: 6.0877\n",
      "Iteration 54/1072, Loss: 5.7559\n",
      "Iteration 55/1072, Loss: 5.8497\n",
      "Iteration 56/1072, Loss: 5.4238\n",
      "Iteration 57/1072, Loss: 5.6126\n",
      "Iteration 58/1072, Loss: 6.1095\n",
      "Iteration 59/1072, Loss: 5.8556\n",
      "Iteration 60/1072, Loss: 6.0073\n",
      "Iteration 61/1072, Loss: 5.9012\n",
      "Iteration 62/1072, Loss: 5.8919\n",
      "Iteration 63/1072, Loss: 5.3518\n",
      "Iteration 64/1072, Loss: 5.9565\n",
      "Iteration 65/1072, Loss: 5.2399\n",
      "Iteration 66/1072, Loss: 5.3916\n",
      "Iteration 67/1072, Loss: 5.8648\n",
      "Iteration 68/1072, Loss: 5.4444\n",
      "Iteration 69/1072, Loss: 5.2127\n",
      "Iteration 70/1072, Loss: 5.6646\n",
      "Iteration 71/1072, Loss: 5.3569\n",
      "Iteration 72/1072, Loss: 5.7256\n",
      "Iteration 73/1072, Loss: 6.0930\n",
      "Iteration 74/1072, Loss: 5.4799\n",
      "Iteration 75/1072, Loss: 5.6749\n",
      "Iteration 76/1072, Loss: 5.8393\n",
      "Iteration 77/1072, Loss: 5.8542\n",
      "Iteration 78/1072, Loss: 6.3869\n",
      "Iteration 79/1072, Loss: 5.8627\n",
      "Iteration 80/1072, Loss: 5.7388\n",
      "Iteration 81/1072, Loss: 5.7867\n",
      "Iteration 82/1072, Loss: 5.4398\n",
      "Iteration 83/1072, Loss: 5.9183\n",
      "Iteration 84/1072, Loss: 6.0025\n",
      "Iteration 85/1072, Loss: 5.2809\n",
      "Iteration 86/1072, Loss: 5.8250\n",
      "Iteration 87/1072, Loss: 6.0295\n",
      "Iteration 88/1072, Loss: 5.6303\n",
      "Iteration 89/1072, Loss: 5.8689\n",
      "Iteration 90/1072, Loss: 5.9097\n",
      "Iteration 91/1072, Loss: 6.0525\n",
      "Iteration 92/1072, Loss: 5.8649\n",
      "Iteration 93/1072, Loss: 5.4025\n",
      "Iteration 94/1072, Loss: 6.1852\n",
      "Iteration 95/1072, Loss: 5.8279\n",
      "Iteration 96/1072, Loss: 6.0616\n",
      "Iteration 97/1072, Loss: 5.7384\n",
      "Iteration 98/1072, Loss: 6.2843\n",
      "Iteration 99/1072, Loss: 6.0913\n",
      "Iteration 100/1072, Loss: 5.4302\n",
      "Iteration 101/1072, Loss: 5.8630\n",
      "Iteration 102/1072, Loss: 5.8979\n",
      "Iteration 103/1072, Loss: 5.8004\n",
      "Iteration 104/1072, Loss: 5.7495\n",
      "Iteration 105/1072, Loss: 6.0018\n",
      "Iteration 106/1072, Loss: 5.8281\n",
      "Iteration 107/1072, Loss: 6.2383\n",
      "Iteration 108/1072, Loss: 6.2508\n",
      "Iteration 109/1072, Loss: 6.0164\n",
      "Iteration 174/1072, Loss: 5.7322\n",
      "Iteration 175/1072, Loss: 5.7234\n",
      "Iteration 176/1072, Loss: 5.5459\n",
      "Iteration 177/1072, Loss: 5.8832\n",
      "Iteration 178/1072, Loss: 5.7476\n",
      "Iteration 179/1072, Loss: 5.8603\n",
      "Iteration 180/1072, Loss: 6.0496\n",
      "Iteration 181/1072, Loss: 5.9428\n",
      "Iteration 182/1072, Loss: 5.5187\n",
      "Iteration 183/1072, Loss: 5.8643\n",
      "Iteration 184/1072, Loss: 5.7929\n",
      "Iteration 185/1072, Loss: 5.6586\n",
      "Iteration 186/1072, Loss: 6.2309\n",
      "Iteration 187/1072, Loss: 5.5861\n",
      "Iteration 188/1072, Loss: 5.5169\n",
      "Iteration 189/1072, Loss: 5.2249\n",
      "Iteration 190/1072, Loss: 5.6098\n",
      "Iteration 191/1072, Loss: 6.2465\n",
      "Iteration 192/1072, Loss: 6.3621\n",
      "Iteration 193/1072, Loss: 5.6343\n",
      "Iteration 194/1072, Loss: 5.4193\n",
      "Iteration 195/1072, Loss: 5.6062\n",
      "Iteration 196/1072, Loss: 6.0619\n",
      "Iteration 197/1072, Loss: 5.9482\n",
      "Iteration 198/1072, Loss: 5.5350\n",
      "Iteration 199/1072, Loss: 5.8146\n",
      "Iteration 200/1072, Loss: 5.8038\n",
      "Iteration 201/1072, Loss: 6.0980\n",
      "Iteration 202/1072, Loss: 6.0217\n",
      "Iteration 203/1072, Loss: 6.0312\n",
      "Iteration 204/1072, Loss: 6.1000\n",
      "Iteration 205/1072, Loss: 5.5940\n",
      "Iteration 206/1072, Loss: 5.9714\n",
      "Iteration 207/1072, Loss: 5.7933\n",
      "Iteration 208/1072, Loss: 5.9763\n",
      "Iteration 209/1072, Loss: 6.0088\n",
      "Iteration 210/1072, Loss: 5.8481\n",
      "Iteration 211/1072, Loss: 5.7948\n",
      "Iteration 212/1072, Loss: 6.2018\n",
      "Iteration 213/1072, Loss: 5.9083\n",
      "Iteration 214/1072, Loss: 6.0021\n",
      "Iteration 215/1072, Loss: 6.0219\n",
      "Iteration 216/1072, Loss: 5.7467\n",
      "Iteration 217/1072, Loss: 6.0979\n",
      "Iteration 218/1072, Loss: 5.8503\n",
      "Iteration 219/1072, Loss: 5.7217\n",
      "Iteration 220/1072, Loss: 5.5862\n",
      "Iteration 221/1072, Loss: 6.0694\n",
      "Iteration 222/1072, Loss: 5.9958\n",
      "Iteration 223/1072, Loss: 5.9532\n",
      "Iteration 224/1072, Loss: 6.0956\n",
      "Iteration 225/1072, Loss: 5.6796\n",
      "Iteration 226/1072, Loss: 6.2605\n",
      "Iteration 227/1072, Loss: 5.7733\n",
      "Iteration 228/1072, Loss: 5.6495\n",
      "Iteration 229/1072, Loss: 5.8674\n",
      "Iteration 230/1072, Loss: 5.5711\n",
      "Iteration 231/1072, Loss: 6.0007\n",
      "Iteration 232/1072, Loss: 5.9761\n",
      "Iteration 233/1072, Loss: 5.6090\n",
      "Iteration 234/1072, Loss: 5.6705\n",
      "Iteration 235/1072, Loss: 5.9543\n",
      "Iteration 236/1072, Loss: 5.6228\n",
      "Iteration 237/1072, Loss: 6.0600\n",
      "Iteration 238/1072, Loss: 6.2107\n",
      "Iteration 239/1072, Loss: 5.8160\n",
      "Iteration 240/1072, Loss: 5.5093\n",
      "Iteration 241/1072, Loss: 5.8117\n",
      "Iteration 242/1072, Loss: 5.4635\n",
      "Iteration 243/1072, Loss: 5.5918\n",
      "Iteration 244/1072, Loss: 6.0082\n",
      "Iteration 245/1072, Loss: 6.1754\n",
      "Iteration 246/1072, Loss: 5.8225\n",
      "Iteration 247/1072, Loss: 5.7143\n",
      "Iteration 248/1072, Loss: 5.8266\n",
      "Iteration 249/1072, Loss: 6.0026\n",
      "Iteration 250/1072, Loss: 6.1863\n",
      "Iteration 251/1072, Loss: 5.5323\n",
      "Iteration 252/1072, Loss: 6.0679\n",
      "Iteration 253/1072, Loss: 5.8615\n",
      "Iteration 254/1072, Loss: 6.0464\n",
      "Iteration 255/1072, Loss: 5.5412\n",
      "Iteration 256/1072, Loss: 6.1059\n",
      "Iteration 257/1072, Loss: 5.6363\n",
      "Iteration 258/1072, Loss: 5.8849\n",
      "Iteration 259/1072, Loss: 5.4973\n",
      "Iteration 260/1072, Loss: 6.0027\n",
      "Iteration 261/1072, Loss: 5.4560\n",
      "Iteration 262/1072, Loss: 5.8116\n",
      "Iteration 263/1072, Loss: 5.8606\n",
      "Iteration 264/1072, Loss: 6.2611\n",
      "Iteration 265/1072, Loss: 5.7099\n",
      "Iteration 266/1072, Loss: 6.2560\n",
      "Iteration 267/1072, Loss: 5.6345\n",
      "Iteration 268/1072, Loss: 5.8935\n",
      "Iteration 269/1072, Loss: 5.9069\n",
      "Iteration 270/1072, Loss: 6.0622\n",
      "Iteration 271/1072, Loss: 5.8767\n",
      "Iteration 272/1072, Loss: 5.8418\n",
      "Iteration 273/1072, Loss: 5.5138\n",
      "Iteration 274/1072, Loss: 5.7925\n",
      "Iteration 275/1072, Loss: 5.4508\n",
      "Iteration 276/1072, Loss: 5.6713\n",
      "Iteration 277/1072, Loss: 5.5790\n",
      "Iteration 278/1072, Loss: 5.9490\n",
      "Iteration 279/1072, Loss: 5.7693\n",
      "Iteration 280/1072, Loss: 6.1493\n",
      "Iteration 281/1072, Loss: 5.6647\n",
      "Iteration 282/1072, Loss: 5.7723\n",
      "Iteration 283/1072, Loss: 6.1042\n",
      "Iteration 284/1072, Loss: 5.6336\n",
      "Iteration 285/1072, Loss: 5.5197\n",
      "Iteration 286/1072, Loss: 6.1580\n",
      "Iteration 287/1072, Loss: 5.6952\n",
      "Iteration 288/1072, Loss: 5.6134\n",
      "Iteration 289/1072, Loss: 5.9341\n",
      "Iteration 290/1072, Loss: 5.7331\n",
      "Iteration 291/1072, Loss: 5.7942\n",
      "Iteration 292/1072, Loss: 5.4107\n",
      "Iteration 293/1072, Loss: 5.6780\n",
      "Iteration 294/1072, Loss: 5.7014\n",
      "Iteration 295/1072, Loss: 5.9445\n",
      "Iteration 296/1072, Loss: 6.0314\n",
      "Iteration 297/1072, Loss: 5.6254\n",
      "Iteration 298/1072, Loss: 5.8236\n",
      "Iteration 299/1072, Loss: 5.9444\n",
      "Iteration 300/1072, Loss: 5.8748\n",
      "Iteration 301/1072, Loss: 5.5976\n",
      "Iteration 302/1072, Loss: 5.9012\n",
      "Iteration 303/1072, Loss: 5.6703\n",
      "Iteration 304/1072, Loss: 6.0708\n",
      "Iteration 305/1072, Loss: 5.9925\n",
      "Iteration 306/1072, Loss: 6.3028\n",
      "Iteration 307/1072, Loss: 5.9948\n",
      "Iteration 308/1072, Loss: 5.6759\n",
      "Iteration 309/1072, Loss: 5.6180\n",
      "Iteration 310/1072, Loss: 6.0717\n",
      "Iteration 311/1072, Loss: 5.9045\n",
      "Iteration 312/1072, Loss: 5.9973\n",
      "Iteration 313/1072, Loss: 6.1872\n",
      "Iteration 314/1072, Loss: 5.8552\n",
      "Iteration 315/1072, Loss: 5.5590\n",
      "Iteration 316/1072, Loss: 5.8044\n",
      "Iteration 317/1072, Loss: 5.5958\n",
      "Iteration 318/1072, Loss: 5.9852\n",
      "Iteration 319/1072, Loss: 5.4520\n",
      "Iteration 320/1072, Loss: 5.7344\n",
      "Iteration 321/1072, Loss: 5.5800\n",
      "Iteration 322/1072, Loss: 5.1934\n",
      "Iteration 323/1072, Loss: 5.5634\n",
      "Iteration 324/1072, Loss: 5.3285\n",
      "Iteration 325/1072, Loss: 6.0923\n",
      "Iteration 326/1072, Loss: 5.6401\n",
      "Iteration 327/1072, Loss: 5.6722\n",
      "Iteration 328/1072, Loss: 5.6724\n",
      "Iteration 329/1072, Loss: 5.3768\n",
      "Iteration 330/1072, Loss: 6.2482\n",
      "Iteration 331/1072, Loss: 5.7606\n",
      "Iteration 332/1072, Loss: 6.0738\n",
      "Iteration 333/1072, Loss: 6.0311\n",
      "Iteration 334/1072, Loss: 5.3282\n",
      "Iteration 335/1072, Loss: 6.2621\n",
      "Iteration 336/1072, Loss: 5.9434\n",
      "Iteration 337/1072, Loss: 5.8336\n",
      "Iteration 338/1072, Loss: 5.8872\n",
      "Iteration 339/1072, Loss: 5.8775\n",
      "Iteration 340/1072, Loss: 5.4310\n",
      "Iteration 341/1072, Loss: 6.0106\n",
      "Iteration 342/1072, Loss: 5.4705\n",
      "Iteration 343/1072, Loss: 6.0564\n",
      "Iteration 344/1072, Loss: 5.6467\n",
      "Iteration 345/1072, Loss: 6.1489\n",
      "Iteration 346/1072, Loss: 5.6019\n",
      "Iteration 347/1072, Loss: 5.7585\n",
      "Iteration 348/1072, Loss: 5.7193\n",
      "Iteration 349/1072, Loss: 5.9111\n",
      "Iteration 350/1072, Loss: 5.8163\n",
      "Iteration 351/1072, Loss: 6.0530\n",
      "Iteration 352/1072, Loss: 5.7348\n",
      "Iteration 353/1072, Loss: 5.2616\n",
      "Iteration 354/1072, Loss: 5.7587\n",
      "Iteration 355/1072, Loss: 5.6811\n",
      "Iteration 356/1072, Loss: 5.8221\n",
      "Iteration 357/1072, Loss: 5.4442\n",
      "Iteration 358/1072, Loss: 5.1872\n",
      "Iteration 359/1072, Loss: 5.8066\n",
      "Iteration 360/1072, Loss: 6.1389\n",
      "Iteration 361/1072, Loss: 5.7848\n",
      "Iteration 362/1072, Loss: 5.8029\n",
      "Iteration 363/1072, Loss: 5.4776\n",
      "Iteration 364/1072, Loss: 5.8587\n",
      "Iteration 365/1072, Loss: 5.5841\n",
      "Iteration 366/1072, Loss: 5.7242\n",
      "Iteration 367/1072, Loss: 5.9389\n",
      "Iteration 368/1072, Loss: 5.6373\n",
      "Iteration 369/1072, Loss: 5.3098\n",
      "Iteration 370/1072, Loss: 6.1567\n",
      "Iteration 371/1072, Loss: 5.7042\n",
      "Iteration 372/1072, Loss: 6.1355\n",
      "Iteration 373/1072, Loss: 6.0721\n",
      "Iteration 374/1072, Loss: 6.0952\n",
      "Iteration 375/1072, Loss: 5.5962\n",
      "Iteration 376/1072, Loss: 5.6344\n",
      "Iteration 377/1072, Loss: 5.5593\n",
      "Iteration 378/1072, Loss: 5.5985\n",
      "Iteration 379/1072, Loss: 6.2038\n",
      "Iteration 380/1072, Loss: 6.0841\n",
      "Iteration 381/1072, Loss: 5.5019\n",
      "Iteration 382/1072, Loss: 5.6202\n",
      "Iteration 383/1072, Loss: 5.9106\n",
      "Iteration 384/1072, Loss: 5.9813\n",
      "Iteration 385/1072, Loss: 6.0843\n",
      "Iteration 386/1072, Loss: 5.6709\n",
      "Iteration 387/1072, Loss: 5.9629\n",
      "Iteration 388/1072, Loss: 6.0128\n",
      "Iteration 389/1072, Loss: 5.6244\n",
      "Iteration 390/1072, Loss: 5.6633\n",
      "Iteration 391/1072, Loss: 5.1722\n",
      "Iteration 392/1072, Loss: 5.6029\n",
      "Iteration 393/1072, Loss: 6.0023\n",
      "Iteration 394/1072, Loss: 5.4737\n",
      "Iteration 395/1072, Loss: 5.7382\n",
      "Iteration 396/1072, Loss: 5.7393\n",
      "Iteration 397/1072, Loss: 5.7167\n",
      "Iteration 398/1072, Loss: 5.4261\n",
      "Iteration 399/1072, Loss: 5.6548\n",
      "Iteration 400/1072, Loss: 5.7922\n",
      "Iteration 401/1072, Loss: 6.0793\n",
      "Iteration 402/1072, Loss: 5.8151\n",
      "Iteration 403/1072, Loss: 5.6891\n",
      "Iteration 404/1072, Loss: 6.0667\n",
      "Iteration 405/1072, Loss: 5.6483\n",
      "Iteration 406/1072, Loss: 5.7973\n",
      "Iteration 407/1072, Loss: 5.9607\n",
      "Iteration 408/1072, Loss: 6.1480\n",
      "Iteration 409/1072, Loss: 5.7801\n",
      "Iteration 410/1072, Loss: 5.6538\n",
      "Iteration 411/1072, Loss: 5.6658\n",
      "Iteration 412/1072, Loss: 5.8049\n",
      "Iteration 413/1072, Loss: 5.6678\n",
      "Iteration 414/1072, Loss: 5.8230\n",
      "Iteration 415/1072, Loss: 5.7893\n",
      "Iteration 416/1072, Loss: 5.3972\n",
      "Iteration 417/1072, Loss: 6.0687\n",
      "Iteration 418/1072, Loss: 5.5715\n",
      "Iteration 419/1072, Loss: 5.2904\n",
      "Iteration 420/1072, Loss: 5.9676\n",
      "Iteration 421/1072, Loss: 5.6292\n",
      "Iteration 422/1072, Loss: 5.6096\n",
      "Iteration 423/1072, Loss: 5.8896\n",
      "Iteration 424/1072, Loss: 5.9782\n",
      "Iteration 425/1072, Loss: 5.3655\n",
      "Iteration 426/1072, Loss: 5.7461\n",
      "Iteration 427/1072, Loss: 5.5507\n",
      "Iteration 428/1072, Loss: 5.5568\n",
      "Iteration 429/1072, Loss: 5.1348\n",
      "Iteration 430/1072, Loss: 5.7285\n",
      "Iteration 431/1072, Loss: 5.8245\n",
      "Iteration 432/1072, Loss: 5.9204\n",
      "Iteration 433/1072, Loss: 5.7346\n",
      "Iteration 434/1072, Loss: 5.9067\n",
      "Iteration 435/1072, Loss: 5.7710\n",
      "Iteration 436/1072, Loss: 5.6561\n",
      "Iteration 437/1072, Loss: 6.0906\n",
      "Iteration 438/1072, Loss: 6.4331\n",
      "Iteration 439/1072, Loss: 5.6439\n",
      "Iteration 440/1072, Loss: 5.4596\n",
      "Iteration 441/1072, Loss: 5.8070\n",
      "Iteration 442/1072, Loss: 5.7379\n",
      "Iteration 443/1072, Loss: 5.2004\n",
      "Iteration 444/1072, Loss: 6.3875\n",
      "Iteration 445/1072, Loss: 5.8678\n",
      "Iteration 446/1072, Loss: 5.8770\n",
      "Iteration 447/1072, Loss: 5.4048\n",
      "Iteration 448/1072, Loss: 5.4864\n",
      "Iteration 449/1072, Loss: 5.7204\n",
      "Iteration 450/1072, Loss: 5.5088\n",
      "Iteration 451/1072, Loss: 5.2535\n",
      "Iteration 452/1072, Loss: 6.2056\n",
      "Iteration 453/1072, Loss: 5.3111\n",
      "Iteration 454/1072, Loss: 5.9486\n",
      "Iteration 455/1072, Loss: 5.8195\n",
      "Iteration 456/1072, Loss: 5.3704\n",
      "Iteration 457/1072, Loss: 5.7077\n",
      "Iteration 458/1072, Loss: 5.7618\n",
      "Iteration 459/1072, Loss: 5.5006\n",
      "Iteration 460/1072, Loss: 5.8035\n",
      "Iteration 461/1072, Loss: 5.5974\n",
      "Iteration 462/1072, Loss: 5.7912\n",
      "Iteration 463/1072, Loss: 5.3270\n",
      "Iteration 464/1072, Loss: 5.6353\n",
      "Iteration 465/1072, Loss: 5.5098\n",
      "Iteration 466/1072, Loss: 5.8645\n",
      "Iteration 467/1072, Loss: 5.4661\n",
      "Iteration 468/1072, Loss: 5.4892\n",
      "Iteration 469/1072, Loss: 5.7940\n",
      "Iteration 470/1072, Loss: 5.7554\n",
      "Iteration 471/1072, Loss: 5.4015\n",
      "Iteration 472/1072, Loss: 5.5425\n",
      "Iteration 473/1072, Loss: 5.8637\n",
      "Iteration 474/1072, Loss: 5.5323\n",
      "Iteration 475/1072, Loss: 5.2779\n",
      "Iteration 476/1072, Loss: 5.6757\n",
      "Iteration 477/1072, Loss: 5.7275\n",
      "Iteration 478/1072, Loss: 5.7985\n",
      "Iteration 479/1072, Loss: 6.1964\n",
      "Iteration 480/1072, Loss: 5.9950\n",
      "Iteration 481/1072, Loss: 6.0583\n",
      "Iteration 482/1072, Loss: 5.9556\n",
      "Iteration 483/1072, Loss: 6.4317\n",
      "Iteration 484/1072, Loss: 5.4434\n",
      "Iteration 485/1072, Loss: 5.8311\n",
      "Iteration 486/1072, Loss: 5.5955\n",
      "Iteration 487/1072, Loss: 5.8713\n",
      "Iteration 488/1072, Loss: 5.8216\n",
      "Iteration 489/1072, Loss: 5.7064\n",
      "Iteration 490/1072, Loss: 5.6012\n",
      "Iteration 491/1072, Loss: 5.8772\n",
      "Iteration 492/1072, Loss: 5.5097\n",
      "Iteration 493/1072, Loss: 5.5962\n",
      "Iteration 494/1072, Loss: 5.9459\n",
      "Iteration 495/1072, Loss: 5.7161\n",
      "Iteration 496/1072, Loss: 5.6343\n",
      "Iteration 497/1072, Loss: 5.6793\n",
      "Iteration 498/1072, Loss: 5.5110\n",
      "Iteration 499/1072, Loss: 5.6067\n",
      "Iteration 500/1072, Loss: 6.0175\n",
      "Iteration 501/1072, Loss: 6.1425\n",
      "Iteration 502/1072, Loss: 5.5005\n",
      "Iteration 503/1072, Loss: 5.1689\n",
      "Iteration 504/1072, Loss: 5.4288\n",
      "Iteration 505/1072, Loss: 5.7814\n",
      "Iteration 506/1072, Loss: 5.2833\n",
      "Iteration 507/1072, Loss: 5.5666\n",
      "Iteration 508/1072, Loss: 5.5063\n",
      "Iteration 509/1072, Loss: 5.9603\n",
      "Iteration 510/1072, Loss: 5.4733\n",
      "Iteration 511/1072, Loss: 5.6597\n",
      "Iteration 512/1072, Loss: 5.5723\n",
      "Iteration 513/1072, Loss: 5.5925\n",
      "Iteration 514/1072, Loss: 5.4291\n",
      "Iteration 515/1072, Loss: 5.7667\n",
      "Iteration 516/1072, Loss: 5.3875\n",
      "Iteration 517/1072, Loss: 5.6900\n",
      "Iteration 518/1072, Loss: 5.9829\n",
      "Iteration 519/1072, Loss: 5.7265\n",
      "Iteration 520/1072, Loss: 5.7978\n",
      "Iteration 521/1072, Loss: 5.3482\n",
      "Iteration 522/1072, Loss: 5.2676\n",
      "Iteration 523/1072, Loss: 5.8134\n",
      "Iteration 524/1072, Loss: 5.7360\n",
      "Iteration 525/1072, Loss: 5.9388\n",
      "Iteration 526/1072, Loss: 5.9525\n",
      "Iteration 527/1072, Loss: 5.3616\n",
      "Iteration 528/1072, Loss: 5.3796\n",
      "Iteration 529/1072, Loss: 5.9973\n",
      "Iteration 530/1072, Loss: 5.9969\n",
      "Iteration 531/1072, Loss: 5.9122\n",
      "Iteration 532/1072, Loss: 5.7440\n",
      "Iteration 533/1072, Loss: 5.3530\n",
      "Iteration 534/1072, Loss: 6.1492\n",
      "Iteration 535/1072, Loss: 5.6130\n",
      "Iteration 536/1072, Loss: 5.9349\n",
      "Iteration 537/1072, Loss: 5.7715\n",
      "Iteration 538/1072, Loss: 5.3450\n",
      "Iteration 539/1072, Loss: 6.0553\n",
      "Iteration 540/1072, Loss: 5.4651\n",
      "Iteration 541/1072, Loss: 5.5783\n",
      "Iteration 542/1072, Loss: 5.3157\n",
      "Iteration 543/1072, Loss: 5.4055\n",
      "Iteration 544/1072, Loss: 5.8406\n",
      "Iteration 545/1072, Loss: 5.8380\n",
      "Iteration 546/1072, Loss: 5.7125\n",
      "Iteration 547/1072, Loss: 5.2291\n",
      "Iteration 548/1072, Loss: 5.9677\n",
      "Iteration 549/1072, Loss: 5.6056\n",
      "Iteration 550/1072, Loss: 5.6286\n",
      "Iteration 551/1072, Loss: 5.0348\n",
      "Iteration 552/1072, Loss: 5.4984\n",
      "Iteration 553/1072, Loss: 5.9255\n",
      "Iteration 554/1072, Loss: 5.8356\n",
      "Iteration 555/1072, Loss: 5.4802\n",
      "Iteration 556/1072, Loss: 5.4040\n",
      "Iteration 557/1072, Loss: 5.7099\n",
      "Iteration 558/1072, Loss: 5.5509\n",
      "Iteration 559/1072, Loss: 5.8784\n",
      "Iteration 560/1072, Loss: 5.5650\n",
      "Iteration 561/1072, Loss: 5.3631\n",
      "Iteration 562/1072, Loss: 5.4925\n",
      "Iteration 563/1072, Loss: 6.0723\n",
      "Iteration 564/1072, Loss: 5.8502\n",
      "Iteration 565/1072, Loss: 5.1504\n",
      "Iteration 566/1072, Loss: 5.6559\n",
      "Iteration 567/1072, Loss: 5.4858\n",
      "Iteration 568/1072, Loss: 5.8629\n",
      "Iteration 569/1072, Loss: 5.4212\n",
      "Iteration 570/1072, Loss: 5.7052\n",
      "Iteration 571/1072, Loss: 5.8267\n",
      "Iteration 572/1072, Loss: 5.8581\n",
      "Iteration 573/1072, Loss: 5.7675\n",
      "Iteration 574/1072, Loss: 5.7547\n",
      "Iteration 575/1072, Loss: 5.5704\n",
      "Iteration 576/1072, Loss: 6.0221\n",
      "Iteration 577/1072, Loss: 5.8776\n",
      "Iteration 578/1072, Loss: 5.9107\n",
      "Iteration 579/1072, Loss: 5.7239\n",
      "Iteration 580/1072, Loss: 6.1565\n",
      "Iteration 581/1072, Loss: 5.5648\n",
      "Iteration 582/1072, Loss: 5.9080\n",
      "Iteration 583/1072, Loss: 5.7603\n",
      "Iteration 584/1072, Loss: 5.7094\n",
      "Iteration 585/1072, Loss: 5.9965\n",
      "Iteration 586/1072, Loss: 5.8848\n",
      "Iteration 587/1072, Loss: 5.4905\n",
      "Iteration 588/1072, Loss: 5.8528\n",
      "Iteration 589/1072, Loss: 5.8367\n",
      "Iteration 590/1072, Loss: 6.0608\n",
      "Iteration 591/1072, Loss: 6.0707\n",
      "Iteration 592/1072, Loss: 5.5971\n",
      "Iteration 593/1072, Loss: 5.7261\n",
      "Iteration 594/1072, Loss: 5.4698\n",
      "Iteration 595/1072, Loss: 5.3771\n",
      "Iteration 596/1072, Loss: 5.9017\n",
      "Iteration 597/1072, Loss: 5.3502\n",
      "Iteration 598/1072, Loss: 5.8680\n",
      "Iteration 599/1072, Loss: 5.8523\n",
      "Iteration 600/1072, Loss: 5.3653\n",
      "Iteration 601/1072, Loss: 5.7998\n",
      "Iteration 602/1072, Loss: 5.7568\n",
      "Iteration 603/1072, Loss: 5.9225\n",
      "Iteration 604/1072, Loss: 5.5651\n",
      "Iteration 605/1072, Loss: 6.2041\n",
      "Iteration 606/1072, Loss: 6.0616\n",
      "Iteration 607/1072, Loss: 5.7981\n",
      "Iteration 608/1072, Loss: 5.8684\n",
      "Iteration 609/1072, Loss: 5.9485\n",
      "Iteration 610/1072, Loss: 5.6703\n",
      "Iteration 611/1072, Loss: 5.9637\n",
      "Iteration 612/1072, Loss: 5.7086\n",
      "Iteration 613/1072, Loss: 5.7307\n",
      "Iteration 614/1072, Loss: 5.6514\n",
      "Iteration 615/1072, Loss: 5.3936\n",
      "Iteration 616/1072, Loss: 5.3071\n",
      "Iteration 617/1072, Loss: 5.6272\n",
      "Iteration 618/1072, Loss: 6.0264\n",
      "Iteration 619/1072, Loss: 5.4338\n",
      "Iteration 620/1072, Loss: 5.6368\n",
      "Iteration 621/1072, Loss: 5.5621\n",
      "Iteration 622/1072, Loss: 5.4141\n",
      "Iteration 623/1072, Loss: 5.7703\n",
      "Iteration 624/1072, Loss: 5.6205\n",
      "Iteration 625/1072, Loss: 5.9872\n",
      "Iteration 626/1072, Loss: 5.7660\n",
      "Iteration 627/1072, Loss: 5.7977\n",
      "Iteration 628/1072, Loss: 6.0613\n",
      "Iteration 629/1072, Loss: 5.6671\n",
      "Iteration 630/1072, Loss: 5.6709\n",
      "Iteration 631/1072, Loss: 5.8978\n",
      "Iteration 632/1072, Loss: 5.3242\n",
      "Iteration 633/1072, Loss: 5.1185\n",
      "Iteration 634/1072, Loss: 5.9387\n",
      "Iteration 635/1072, Loss: 6.1180\n",
      "Iteration 636/1072, Loss: 5.7018\n",
      "Iteration 637/1072, Loss: 5.2079\n",
      "Iteration 638/1072, Loss: 5.8800\n",
      "Iteration 639/1072, Loss: 5.8458\n",
      "Iteration 640/1072, Loss: 5.6787\n",
      "Iteration 641/1072, Loss: 5.7958\n",
      "Iteration 642/1072, Loss: 4.9059\n",
      "Iteration 643/1072, Loss: 5.5681\n",
      "Iteration 644/1072, Loss: 5.2540\n",
      "Iteration 645/1072, Loss: 5.5845\n",
      "Iteration 646/1072, Loss: 5.7577\n",
      "Iteration 647/1072, Loss: 5.8724\n",
      "Iteration 648/1072, Loss: 5.6634\n",
      "Iteration 649/1072, Loss: 5.7171\n",
      "Iteration 650/1072, Loss: 5.6372\n",
      "Iteration 651/1072, Loss: 5.4081\n",
      "Iteration 652/1072, Loss: 5.8834\n",
      "Iteration 653/1072, Loss: 5.2377\n",
      "Iteration 654/1072, Loss: 5.3978\n",
      "Iteration 655/1072, Loss: 6.0725\n",
      "Iteration 656/1072, Loss: 5.7812\n",
      "Iteration 657/1072, Loss: 5.8320\n",
      "Iteration 658/1072, Loss: 5.6733\n",
      "Iteration 659/1072, Loss: 5.1895\n",
      "Iteration 660/1072, Loss: 5.7794\n",
      "Iteration 661/1072, Loss: 5.5876\n",
      "Iteration 662/1072, Loss: 5.4411\n",
      "Iteration 663/1072, Loss: 5.8346\n",
      "Iteration 664/1072, Loss: 5.5421\n",
      "Iteration 665/1072, Loss: 5.4702\n",
      "Iteration 666/1072, Loss: 5.5914\n",
      "Iteration 667/1072, Loss: 5.5863\n",
      "Iteration 668/1072, Loss: 5.1520\n",
      "Iteration 669/1072, Loss: 5.4216\n",
      "Iteration 670/1072, Loss: 6.1738\n",
      "Iteration 671/1072, Loss: 5.7696\n",
      "Iteration 672/1072, Loss: 6.0118\n",
      "Iteration 673/1072, Loss: 5.4521\n",
      "Iteration 674/1072, Loss: 5.5510\n",
      "Iteration 675/1072, Loss: 5.2689\n",
      "Iteration 676/1072, Loss: 5.2329\n",
      "Iteration 677/1072, Loss: 5.9712\n",
      "Iteration 678/1072, Loss: 5.6820\n",
      "Iteration 679/1072, Loss: 5.1079\n",
      "Iteration 680/1072, Loss: 5.8072\n",
      "Iteration 681/1072, Loss: 5.2670\n",
      "Iteration 682/1072, Loss: 6.0490\n",
      "Iteration 683/1072, Loss: 5.8026\n",
      "Iteration 684/1072, Loss: 5.9870\n",
      "Iteration 685/1072, Loss: 6.3288\n",
      "Iteration 686/1072, Loss: 6.1382\n",
      "Iteration 687/1072, Loss: 5.2922\n",
      "Iteration 688/1072, Loss: 5.3135\n",
      "Iteration 689/1072, Loss: 5.5114\n",
      "Iteration 690/1072, Loss: 6.2300\n",
      "Iteration 691/1072, Loss: 5.6005\n",
      "Iteration 692/1072, Loss: 5.4843\n",
      "Iteration 693/1072, Loss: 5.6863\n",
      "Iteration 694/1072, Loss: 5.9135\n",
      "Iteration 695/1072, Loss: 5.8251\n",
      "Iteration 696/1072, Loss: 5.4613\n",
      "Iteration 697/1072, Loss: 5.6691\n",
      "Iteration 698/1072, Loss: 5.4932\n",
      "Iteration 699/1072, Loss: 5.4762\n",
      "Iteration 700/1072, Loss: 5.4539\n",
      "Iteration 701/1072, Loss: 5.6592\n",
      "Iteration 702/1072, Loss: 5.9097\n",
      "Iteration 703/1072, Loss: 5.4289\n",
      "Iteration 704/1072, Loss: 5.3587\n",
      "Iteration 705/1072, Loss: 6.1299\n",
      "Iteration 706/1072, Loss: 6.1243\n",
      "Iteration 707/1072, Loss: 5.7487\n",
      "Iteration 708/1072, Loss: 6.0479\n",
      "Iteration 709/1072, Loss: 5.8155\n",
      "Iteration 710/1072, Loss: 5.8409\n",
      "Iteration 711/1072, Loss: 5.6804\n",
      "Iteration 712/1072, Loss: 5.8137\n",
      "Iteration 713/1072, Loss: 5.8808\n",
      "Iteration 714/1072, Loss: 5.9174\n",
      "Iteration 715/1072, Loss: 5.5757\n",
      "Iteration 716/1072, Loss: 5.7481\n",
      "Iteration 717/1072, Loss: 5.9188\n",
      "Iteration 718/1072, Loss: 5.8039\n",
      "Iteration 719/1072, Loss: 5.4892\n",
      "Iteration 720/1072, Loss: 6.1518\n",
      "Iteration 721/1072, Loss: 5.7107\n",
      "Iteration 722/1072, Loss: 5.6676\n",
      "Iteration 723/1072, Loss: 5.3242\n",
      "Iteration 724/1072, Loss: 6.0715\n",
      "Iteration 725/1072, Loss: 5.3627\n",
      "Iteration 726/1072, Loss: 5.5563\n",
      "Iteration 727/1072, Loss: 6.1248\n",
      "Iteration 728/1072, Loss: 5.1683\n",
      "Iteration 729/1072, Loss: 5.8197\n",
      "Iteration 730/1072, Loss: 5.6552\n",
      "Iteration 731/1072, Loss: 5.4646\n",
      "Iteration 732/1072, Loss: 5.8256\n",
      "Iteration 733/1072, Loss: 5.5318\n",
      "Iteration 734/1072, Loss: 6.0676\n",
      "Iteration 735/1072, Loss: 5.8371\n",
      "Iteration 736/1072, Loss: 5.5772\n",
      "Iteration 737/1072, Loss: 5.7394\n",
      "Iteration 738/1072, Loss: 5.7257\n",
      "Iteration 739/1072, Loss: 5.4760\n",
      "Iteration 740/1072, Loss: 5.7212\n",
      "Iteration 741/1072, Loss: 5.5188\n",
      "Iteration 742/1072, Loss: 5.9395\n",
      "Iteration 743/1072, Loss: 5.2993\n",
      "Iteration 744/1072, Loss: 5.5600\n",
      "Iteration 745/1072, Loss: 5.5982\n",
      "Iteration 746/1072, Loss: 6.0231\n",
      "Iteration 747/1072, Loss: 5.6567\n",
      "Iteration 748/1072, Loss: 5.7979\n",
      "Iteration 749/1072, Loss: 5.8781\n",
      "Iteration 750/1072, Loss: 5.9986\n",
      "Iteration 751/1072, Loss: 5.7674\n",
      "Iteration 752/1072, Loss: 5.7233\n",
      "Iteration 753/1072, Loss: 5.4344\n",
      "Iteration 754/1072, Loss: 5.6124\n",
      "Iteration 755/1072, Loss: 5.7512\n",
      "Iteration 756/1072, Loss: 6.1296\n",
      "Iteration 757/1072, Loss: 5.5469\n",
      "Iteration 758/1072, Loss: 5.7177\n",
      "Iteration 759/1072, Loss: 5.7831\n",
      "Iteration 760/1072, Loss: 5.0078\n",
      "Iteration 761/1072, Loss: 5.2816\n",
      "Iteration 762/1072, Loss: 6.0113\n",
      "Iteration 763/1072, Loss: 5.7224\n",
      "Iteration 764/1072, Loss: 5.1809\n",
      "Iteration 765/1072, Loss: 5.9965\n",
      "Iteration 766/1072, Loss: 5.6752\n",
      "Iteration 767/1072, Loss: 6.0302\n",
      "Iteration 768/1072, Loss: 5.7919\n",
      "Iteration 769/1072, Loss: 5.4901\n",
      "Iteration 770/1072, Loss: 5.5635\n",
      "Iteration 771/1072, Loss: 6.0864\n",
      "Iteration 772/1072, Loss: 5.3488\n",
      "Iteration 773/1072, Loss: 5.8828\n",
      "Iteration 774/1072, Loss: 5.4400\n",
      "Iteration 775/1072, Loss: 5.8166\n",
      "Iteration 776/1072, Loss: 5.8345\n",
      "Iteration 777/1072, Loss: 5.3248\n",
      "Iteration 778/1072, Loss: 5.8936\n",
      "Iteration 779/1072, Loss: 5.8319\n",
      "Iteration 780/1072, Loss: 6.2280\n",
      "Iteration 781/1072, Loss: 5.6152\n",
      "Iteration 782/1072, Loss: 5.3848\n",
      "Iteration 783/1072, Loss: 5.8902\n",
      "Iteration 784/1072, Loss: 5.8931\n",
      "Iteration 785/1072, Loss: 5.3761\n",
      "Iteration 786/1072, Loss: 5.4711\n",
      "Iteration 787/1072, Loss: 5.4725\n",
      "Iteration 788/1072, Loss: 5.8125\n",
      "Iteration 789/1072, Loss: 5.9244\n",
      "Iteration 790/1072, Loss: 5.4331\n",
      "Iteration 791/1072, Loss: 5.8850\n",
      "Iteration 792/1072, Loss: 5.9989\n",
      "Iteration 793/1072, Loss: 5.6289\n",
      "Iteration 794/1072, Loss: 5.7611\n",
      "Iteration 795/1072, Loss: 5.4794\n",
      "Iteration 796/1072, Loss: 5.3925\n",
      "Iteration 797/1072, Loss: 5.7074\n",
      "Iteration 798/1072, Loss: 5.2915\n",
      "Iteration 799/1072, Loss: 5.5531\n",
      "Iteration 800/1072, Loss: 5.6060\n",
      "Iteration 840/1072, Loss: 5.3175\n",
      "Iteration 841/1072, Loss: 5.9663\n",
      "Iteration 842/1072, Loss: 5.4946\n",
      "Iteration 843/1072, Loss: 5.6765\n",
      "Iteration 844/1072, Loss: 5.5475\n",
      "Iteration 845/1072, Loss: 5.4938\n",
      "Iteration 846/1072, Loss: 5.5896\n",
      "Iteration 847/1072, Loss: 5.5791\n",
      "Iteration 848/1072, Loss: 5.3687\n",
      "Iteration 849/1072, Loss: 5.5160\n",
      "Iteration 850/1072, Loss: 5.6088\n",
      "Iteration 851/1072, Loss: 5.6266\n",
      "Iteration 852/1072, Loss: 5.5971\n",
      "Iteration 853/1072, Loss: 5.5269\n",
      "Iteration 854/1072, Loss: 5.7724\n",
      "Iteration 855/1072, Loss: 5.4136\n",
      "Iteration 856/1072, Loss: 5.6476\n",
      "Iteration 857/1072, Loss: 5.7242\n",
      "Iteration 858/1072, Loss: 6.2356\n",
      "Iteration 859/1072, Loss: 5.6395\n",
      "Iteration 860/1072, Loss: 5.4961\n",
      "Iteration 861/1072, Loss: 5.9067\n",
      "Iteration 862/1072, Loss: 5.7456\n",
      "Iteration 863/1072, Loss: 5.8694\n",
      "Iteration 864/1072, Loss: 5.4898\n",
      "Iteration 865/1072, Loss: 5.7226\n",
      "Iteration 866/1072, Loss: 5.8882\n",
      "Iteration 867/1072, Loss: 5.7735\n",
      "Iteration 868/1072, Loss: 5.6453\n",
      "Iteration 869/1072, Loss: 5.6397\n",
      "Iteration 870/1072, Loss: 5.9100\n",
      "Iteration 871/1072, Loss: 5.8468\n",
      "Iteration 872/1072, Loss: 5.3719\n",
      "Iteration 873/1072, Loss: 5.5079\n",
      "Iteration 874/1072, Loss: 5.7514\n",
      "Iteration 875/1072, Loss: 5.7204\n",
      "Iteration 876/1072, Loss: 5.6744\n",
      "Iteration 877/1072, Loss: 5.5756\n",
      "Iteration 878/1072, Loss: 4.8493\n",
      "Iteration 879/1072, Loss: 5.5794\n",
      "Iteration 880/1072, Loss: 5.5174\n",
      "Iteration 881/1072, Loss: 5.7607\n",
      "Iteration 882/1072, Loss: 5.5708\n",
      "Iteration 883/1072, Loss: 5.1960\n",
      "Iteration 884/1072, Loss: 5.4244\n",
      "Iteration 885/1072, Loss: 5.4165\n",
      "Iteration 886/1072, Loss: 5.6179\n",
      "Iteration 887/1072, Loss: 5.5571\n",
      "Iteration 888/1072, Loss: 5.5636\n",
      "Iteration 889/1072, Loss: 5.2020\n",
      "Iteration 890/1072, Loss: 5.6628\n",
      "Iteration 891/1072, Loss: 5.2997\n",
      "Iteration 892/1072, Loss: 5.7789\n",
      "Iteration 893/1072, Loss: 5.7578\n",
      "Iteration 894/1072, Loss: 5.8400\n",
      "Iteration 895/1072, Loss: 5.9202\n",
      "Iteration 896/1072, Loss: 5.8207\n",
      "Iteration 897/1072, Loss: 5.8171\n",
      "Iteration 898/1072, Loss: 5.4301\n",
      "Iteration 899/1072, Loss: 5.9001\n",
      "Iteration 900/1072, Loss: 5.6841\n",
      "Iteration 901/1072, Loss: 5.8665\n",
      "Iteration 902/1072, Loss: 5.6487\n",
      "Iteration 903/1072, Loss: 5.4765\n",
      "Iteration 904/1072, Loss: 5.7040\n",
      "Iteration 905/1072, Loss: 5.3894\n",
      "Iteration 906/1072, Loss: 5.0732\n",
      "Iteration 907/1072, Loss: 5.6473\n",
      "Iteration 908/1072, Loss: 5.5472\n",
      "Iteration 909/1072, Loss: 5.6349\n",
      "Iteration 910/1072, Loss: 5.3977\n",
      "Iteration 911/1072, Loss: 5.7025\n",
      "Iteration 912/1072, Loss: 5.8393\n",
      "Iteration 913/1072, Loss: 5.8772\n",
      "Iteration 914/1072, Loss: 5.8259\n",
      "Iteration 915/1072, Loss: 5.8164\n",
      "Iteration 916/1072, Loss: 5.4882\n",
      "Iteration 917/1072, Loss: 5.8176\n",
      "Iteration 918/1072, Loss: 5.4593\n",
      "Iteration 919/1072, Loss: 6.1136\n",
      "Iteration 920/1072, Loss: 5.5769\n",
      "Iteration 921/1072, Loss: 5.7355\n",
      "Iteration 922/1072, Loss: 5.6705\n",
      "Iteration 923/1072, Loss: 5.7830\n",
      "Iteration 924/1072, Loss: 5.9556\n",
      "Iteration 925/1072, Loss: 5.7083\n",
      "Iteration 926/1072, Loss: 5.1932\n",
      "Iteration 927/1072, Loss: 5.4135\n",
      "Iteration 928/1072, Loss: 5.9148\n",
      "Iteration 929/1072, Loss: 5.8529\n",
      "Iteration 930/1072, Loss: 5.6051\n",
      "Iteration 931/1072, Loss: 5.6071\n",
      "Iteration 932/1072, Loss: 5.6813\n",
      "Iteration 933/1072, Loss: 5.9783\n",
      "Iteration 934/1072, Loss: 5.6772\n",
      "Iteration 935/1072, Loss: 5.4363\n",
      "Iteration 936/1072, Loss: 5.6761\n",
      "Iteration 937/1072, Loss: 5.7108\n",
      "Iteration 938/1072, Loss: 5.3290\n",
      "Iteration 939/1072, Loss: 5.4518\n",
      "Iteration 940/1072, Loss: 5.9877\n",
      "Iteration 941/1072, Loss: 5.7722\n",
      "Iteration 942/1072, Loss: 5.9927\n",
      "Iteration 943/1072, Loss: 5.9563\n",
      "Iteration 944/1072, Loss: 5.8679\n",
      "Iteration 945/1072, Loss: 5.7372\n",
      "Iteration 946/1072, Loss: 5.9174\n",
      "Iteration 947/1072, Loss: 5.4823\n",
      "Iteration 948/1072, Loss: 5.5224\n",
      "Iteration 949/1072, Loss: 5.8039\n",
      "Iteration 950/1072, Loss: 5.7102\n",
      "Iteration 951/1072, Loss: 5.6165\n",
      "Iteration 952/1072, Loss: 5.8086\n",
      "Iteration 953/1072, Loss: 5.4289\n",
      "Iteration 954/1072, Loss: 5.2641\n",
      "Iteration 955/1072, Loss: 5.1291\n",
      "Iteration 956/1072, Loss: 5.7570\n",
      "Iteration 957/1072, Loss: 5.4119\n",
      "Iteration 958/1072, Loss: 5.7745\n",
      "Iteration 959/1072, Loss: 5.7859\n",
      "Iteration 960/1072, Loss: 5.7788\n",
      "Iteration 961/1072, Loss: 5.6245\n",
      "Iteration 962/1072, Loss: 5.3210\n",
      "Iteration 963/1072, Loss: 5.5148\n",
      "Iteration 964/1072, Loss: 5.5658\n",
      "Iteration 965/1072, Loss: 5.2907\n",
      "Iteration 966/1072, Loss: 5.7475\n",
      "Iteration 967/1072, Loss: 5.4716\n",
      "Iteration 968/1072, Loss: 5.5968\n",
      "Iteration 969/1072, Loss: 5.7659\n",
      "Iteration 970/1072, Loss: 5.6829\n",
      "Iteration 971/1072, Loss: 6.0774\n",
      "Iteration 972/1072, Loss: 5.9523\n",
      "Iteration 973/1072, Loss: 5.4666\n",
      "Iteration 974/1072, Loss: 5.4184\n",
      "Iteration 975/1072, Loss: 5.5143\n",
      "Iteration 976/1072, Loss: 5.8311\n",
      "Iteration 977/1072, Loss: 6.2923\n",
      "Iteration 978/1072, Loss: 5.4598\n",
      "Iteration 979/1072, Loss: 5.6466\n",
      "Iteration 980/1072, Loss: 5.7643\n",
      "Iteration 981/1072, Loss: 5.2641\n",
      "Iteration 982/1072, Loss: 5.1569\n",
      "Iteration 983/1072, Loss: 5.1057\n",
      "Iteration 984/1072, Loss: 5.9603\n",
      "Iteration 985/1072, Loss: 5.5762\n",
      "Iteration 986/1072, Loss: 5.4410\n",
      "Iteration 987/1072, Loss: 5.6705\n",
      "Iteration 988/1072, Loss: 5.7726\n",
      "Iteration 989/1072, Loss: 5.9672\n",
      "Iteration 990/1072, Loss: 5.3619\n",
      "Iteration 991/1072, Loss: 5.3491\n",
      "Iteration 992/1072, Loss: 5.7375\n",
      "Iteration 993/1072, Loss: 5.4053\n",
      "Iteration 994/1072, Loss: 5.6270\n",
      "Iteration 995/1072, Loss: 5.8036\n",
      "Iteration 996/1072, Loss: 5.2377\n",
      "Iteration 997/1072, Loss: 5.5811\n",
      "Iteration 998/1072, Loss: 5.2011\n",
      "Iteration 999/1072, Loss: 5.4580\n",
      "Iteration 1000/1072, Loss: 5.5195\n",
      "Iteration 1001/1072, Loss: 5.3364\n",
      "Iteration 1002/1072, Loss: 5.6584\n",
      "Iteration 1003/1072, Loss: 5.4225\n",
      "Iteration 1004/1072, Loss: 6.0826\n",
      "Iteration 1005/1072, Loss: 5.5504\n",
      "Iteration 1006/1072, Loss: 5.6383\n",
      "Iteration 1007/1072, Loss: 5.9353\n",
      "Iteration 1008/1072, Loss: 5.7670\n",
      "Iteration 1009/1072, Loss: 5.9036\n",
      "Iteration 1010/1072, Loss: 4.9236\n",
      "Iteration 1011/1072, Loss: 5.9543\n",
      "Iteration 1012/1072, Loss: 5.4178\n",
      "Iteration 1013/1072, Loss: 5.7591\n",
      "Iteration 1014/1072, Loss: 5.8941\n",
      "Iteration 1015/1072, Loss: 5.9137\n",
      "Iteration 1016/1072, Loss: 5.2728\n",
      "Iteration 1017/1072, Loss: 5.6031\n",
      "Iteration 1018/1072, Loss: 5.8236\n",
      "Iteration 1019/1072, Loss: 5.3227\n",
      "Iteration 1020/1072, Loss: 6.1846\n",
      "Iteration 1021/1072, Loss: 5.3351\n",
      "Iteration 1022/1072, Loss: 5.2619\n",
      "Iteration 1023/1072, Loss: 5.7463\n",
      "Iteration 1024/1072, Loss: 5.4739\n",
      "Iteration 1025/1072, Loss: 6.1581\n",
      "Iteration 1026/1072, Loss: 5.4516\n",
      "Iteration 1027/1072, Loss: 5.9757\n",
      "Iteration 1028/1072, Loss: 5.7991\n",
      "Iteration 1029/1072, Loss: 5.6949\n",
      "Iteration 1030/1072, Loss: 5.7231\n",
      "Iteration 1031/1072, Loss: 5.3875\n",
      "Iteration 1032/1072, Loss: 5.7527\n",
      "Iteration 1033/1072, Loss: 5.5051\n",
      "Iteration 1034/1072, Loss: 5.1991\n",
      "Iteration 1035/1072, Loss: 5.2542\n",
      "Iteration 1036/1072, Loss: 5.5657\n",
      "Iteration 1037/1072, Loss: 5.6063\n",
      "Iteration 1038/1072, Loss: 5.4937\n",
      "Iteration 1039/1072, Loss: 5.8976\n",
      "Iteration 1040/1072, Loss: 5.4550\n",
      "Iteration 1041/1072, Loss: 5.2027\n",
      "Iteration 1042/1072, Loss: 5.8584\n",
      "Iteration 1043/1072, Loss: 5.7048\n",
      "Iteration 1044/1072, Loss: 5.4617\n",
      "Iteration 1045/1072, Loss: 5.7803\n",
      "Iteration 1046/1072, Loss: 6.0062\n",
      "Iteration 1047/1072, Loss: 5.6742\n",
      "Iteration 1048/1072, Loss: 5.4810\n",
      "Iteration 1049/1072, Loss: 5.7899\n",
      "Iteration 1050/1072, Loss: 5.5564\n",
      "Iteration 1051/1072, Loss: 5.6689\n",
      "Iteration 1052/1072, Loss: 5.4827\n",
      "Iteration 1053/1072, Loss: 5.4928\n",
      "Iteration 1054/1072, Loss: 5.4043\n",
      "Iteration 1055/1072, Loss: 5.8783\n",
      "Iteration 1056/1072, Loss: 5.5384\n",
      "Iteration 1057/1072, Loss: 5.6142\n",
      "Iteration 1058/1072, Loss: 6.0756\n",
      "Iteration 1059/1072, Loss: 5.3950\n",
      "Iteration 1060/1072, Loss: 5.8911\n",
      "Iteration 1061/1072, Loss: 5.6873\n",
      "Iteration 1062/1072, Loss: 5.6098\n",
      "Iteration 1063/1072, Loss: 5.5094\n",
      "Iteration 1064/1072, Loss: 5.3285\n",
      "Iteration 1065/1072, Loss: 5.5076\n",
      "Iteration 1066/1072, Loss: 5.4494\n",
      "Iteration 1067/1072, Loss: 5.3253\n",
      "Iteration 1068/1072, Loss: 5.8179\n",
      "Iteration 1069/1072, Loss: 5.7436\n",
      "Iteration 1070/1072, Loss: 5.6969\n",
      "Iteration 1071/1072, Loss: 5.8495\n",
      "Iteration 1072/1072, Loss: 7.5123\n",
      "Epoch 7/10, Loss: 5.7238\n",
      "Validation Accuracy: 18.00%\n",
      "Model checkpoint saved!\n",
      "Iteration 1/1072, Loss: 5.0626\n",
      "Iteration 2/1072, Loss: 5.3763\n",
      "Iteration 3/1072, Loss: 5.3260\n",
      "Iteration 4/1072, Loss: 5.1107\n",
      "Iteration 5/1072, Loss: 5.1501\n",
      "Iteration 6/1072, Loss: 5.3555\n",
      "Iteration 7/1072, Loss: 5.3290\n",
      "Iteration 8/1072, Loss: 4.9507\n",
      "Iteration 9/1072, Loss: 5.1447\n",
      "Iteration 10/1072, Loss: 5.6567\n",
      "Iteration 11/1072, Loss: 5.3856\n",
      "Iteration 12/1072, Loss: 5.1064\n",
      "Iteration 13/1072, Loss: 5.1298\n",
      "Iteration 14/1072, Loss: 4.9874\n",
      "Iteration 15/1072, Loss: 5.5265\n",
      "Iteration 16/1072, Loss: 5.5642\n",
      "Iteration 17/1072, Loss: 5.2320\n",
      "Iteration 18/1072, Loss: 5.4686\n",
      "Iteration 19/1072, Loss: 5.0837\n",
      "Iteration 20/1072, Loss: 5.2039\n",
      "Iteration 21/1072, Loss: 5.3995\n",
      "Iteration 22/1072, Loss: 5.4945\n",
      "Iteration 23/1072, Loss: 4.7155\n",
      "Iteration 24/1072, Loss: 5.7268\n",
      "Iteration 25/1072, Loss: 5.2635\n",
      "Iteration 26/1072, Loss: 5.1381\n",
      "Iteration 27/1072, Loss: 5.4086\n",
      "Iteration 28/1072, Loss: 5.2518\n",
      "Iteration 29/1072, Loss: 4.9870\n",
      "Iteration 30/1072, Loss: 5.6258\n",
      "Iteration 31/1072, Loss: 5.1197\n",
      "Iteration 32/1072, Loss: 5.5819\n",
      "Iteration 72/1072, Loss: 5.5254\n",
      "Iteration 73/1072, Loss: 5.3333\n",
      "Iteration 74/1072, Loss: 5.2499\n",
      "Iteration 75/1072, Loss: 5.5058\n",
      "Iteration 76/1072, Loss: 5.0952\n",
      "Iteration 77/1072, Loss: 5.6035\n",
      "Iteration 78/1072, Loss: 5.3212\n",
      "Iteration 79/1072, Loss: 5.2234\n",
      "Iteration 80/1072, Loss: 5.7840\n",
      "Iteration 81/1072, Loss: 5.2621\n",
      "Iteration 82/1072, Loss: 5.0507\n",
      "Iteration 83/1072, Loss: 5.3728\n",
      "Iteration 84/1072, Loss: 5.4501\n",
      "Iteration 85/1072, Loss: 4.8654\n",
      "Iteration 86/1072, Loss: 5.1183\n",
      "Iteration 87/1072, Loss: 5.4097\n",
      "Iteration 88/1072, Loss: 5.2335\n",
      "Iteration 89/1072, Loss: 5.2736\n",
      "Iteration 90/1072, Loss: 5.2575\n",
      "Iteration 91/1072, Loss: 4.9409\n",
      "Iteration 92/1072, Loss: 5.3686\n",
      "Iteration 93/1072, Loss: 5.5209\n",
      "Iteration 94/1072, Loss: 5.2712\n",
      "Iteration 95/1072, Loss: 5.5286\n",
      "Iteration 96/1072, Loss: 5.4108\n",
      "Iteration 97/1072, Loss: 5.3050\n",
      "Iteration 98/1072, Loss: 5.1098\n",
      "Iteration 99/1072, Loss: 5.4657\n",
      "Iteration 100/1072, Loss: 5.3960\n",
      "Iteration 101/1072, Loss: 5.8653\n",
      "Iteration 102/1072, Loss: 5.6489\n",
      "Iteration 103/1072, Loss: 5.1765\n",
      "Iteration 104/1072, Loss: 5.0444\n",
      "Iteration 105/1072, Loss: 5.6249\n",
      "Iteration 106/1072, Loss: 5.0782\n",
      "Iteration 107/1072, Loss: 5.0226\n",
      "Iteration 108/1072, Loss: 5.7147\n",
      "Iteration 109/1072, Loss: 4.9240\n",
      "Iteration 110/1072, Loss: 5.1294\n",
      "Iteration 111/1072, Loss: 5.5248\n",
      "Iteration 112/1072, Loss: 5.1885\n",
      "Iteration 113/1072, Loss: 5.2820\n",
      "Iteration 114/1072, Loss: 5.2636\n",
      "Iteration 115/1072, Loss: 5.2084\n",
      "Iteration 116/1072, Loss: 5.1364\n",
      "Iteration 117/1072, Loss: 5.5194\n",
      "Iteration 118/1072, Loss: 5.3542\n",
      "Iteration 119/1072, Loss: 5.5369\n",
      "Iteration 120/1072, Loss: 5.0341\n",
      "Iteration 121/1072, Loss: 5.1328\n",
      "Iteration 122/1072, Loss: 5.1146\n",
      "Iteration 123/1072, Loss: 4.8587\n",
      "Iteration 124/1072, Loss: 4.9089\n",
      "Iteration 125/1072, Loss: 5.1105\n",
      "Iteration 126/1072, Loss: 5.1038\n",
      "Iteration 127/1072, Loss: 5.0214\n",
      "Iteration 128/1072, Loss: 5.7244\n",
      "Iteration 129/1072, Loss: 5.5295\n",
      "Iteration 130/1072, Loss: 5.4903\n",
      "Iteration 131/1072, Loss: 5.5362\n",
      "Iteration 132/1072, Loss: 5.4055\n",
      "Iteration 133/1072, Loss: 5.6652\n",
      "Iteration 134/1072, Loss: 5.3417\n",
      "Iteration 135/1072, Loss: 5.0604\n",
      "Iteration 136/1072, Loss: 5.2381\n",
      "Iteration 137/1072, Loss: 5.0177\n",
      "Iteration 138/1072, Loss: 5.1713\n",
      "Iteration 139/1072, Loss: 5.4193\n",
      "Iteration 140/1072, Loss: 5.2989\n",
      "Iteration 141/1072, Loss: 5.3315\n",
      "Iteration 142/1072, Loss: 5.0549\n",
      "Iteration 143/1072, Loss: 5.2165\n",
      "Iteration 144/1072, Loss: 5.3063\n",
      "Iteration 145/1072, Loss: 5.1388\n",
      "Iteration 146/1072, Loss: 5.0496\n",
      "Iteration 147/1072, Loss: 5.7165\n",
      "Iteration 148/1072, Loss: 5.5862\n",
      "Iteration 149/1072, Loss: 5.3752\n",
      "Iteration 150/1072, Loss: 5.5922\n",
      "Iteration 151/1072, Loss: 5.4285\n",
      "Iteration 152/1072, Loss: 5.2709\n",
      "Iteration 153/1072, Loss: 5.3628\n",
      "Iteration 154/1072, Loss: 5.1488\n",
      "Iteration 155/1072, Loss: 5.4146\n",
      "Iteration 156/1072, Loss: 5.1000\n",
      "Iteration 157/1072, Loss: 5.5186\n",
      "Iteration 158/1072, Loss: 5.0634\n",
      "Iteration 159/1072, Loss: 5.5899\n",
      "Iteration 160/1072, Loss: 5.2152\n",
      "Iteration 161/1072, Loss: 5.3429\n",
      "Iteration 162/1072, Loss: 5.3734\n",
      "Iteration 163/1072, Loss: 5.0564\n",
      "Iteration 164/1072, Loss: 5.8995\n",
      "Iteration 165/1072, Loss: 5.4000\n",
      "Iteration 166/1072, Loss: 5.0170\n",
      "Iteration 167/1072, Loss: 5.9145\n",
      "Iteration 168/1072, Loss: 5.7077\n",
      "Iteration 169/1072, Loss: 5.6729\n",
      "Iteration 170/1072, Loss: 5.6209\n",
      "Iteration 171/1072, Loss: 5.4112\n",
      "Iteration 172/1072, Loss: 4.8470\n",
      "Iteration 173/1072, Loss: 5.6177\n",
      "Iteration 174/1072, Loss: 5.3700\n",
      "Iteration 175/1072, Loss: 5.0578\n",
      "Iteration 176/1072, Loss: 5.3198\n",
      "Iteration 177/1072, Loss: 5.1981\n",
      "Iteration 178/1072, Loss: 5.1520\n",
      "Iteration 179/1072, Loss: 5.4714\n",
      "Iteration 180/1072, Loss: 5.3684\n",
      "Iteration 181/1072, Loss: 6.3427\n",
      "Iteration 182/1072, Loss: 4.8759\n",
      "Iteration 183/1072, Loss: 5.2788\n",
      "Iteration 184/1072, Loss: 5.3114\n",
      "Iteration 185/1072, Loss: 5.1643\n",
      "Iteration 186/1072, Loss: 5.5419\n",
      "Iteration 187/1072, Loss: 5.2216\n",
      "Iteration 188/1072, Loss: 4.9669\n",
      "Iteration 189/1072, Loss: 5.6589\n",
      "Iteration 190/1072, Loss: 5.3065\n",
      "Iteration 191/1072, Loss: 5.5794\n",
      "Iteration 192/1072, Loss: 5.5688\n",
      "Iteration 193/1072, Loss: 6.0383\n",
      "Iteration 194/1072, Loss: 5.0646\n",
      "Iteration 195/1072, Loss: 5.3333\n",
      "Iteration 196/1072, Loss: 5.1336\n",
      "Iteration 197/1072, Loss: 5.0494\n",
      "Iteration 198/1072, Loss: 5.0078\n",
      "Iteration 199/1072, Loss: 4.9561\n",
      "Iteration 200/1072, Loss: 5.7509\n",
      "Iteration 201/1072, Loss: 5.1881\n",
      "Iteration 202/1072, Loss: 5.4433\n",
      "Iteration 203/1072, Loss: 5.4752\n",
      "Iteration 204/1072, Loss: 5.3149\n",
      "Iteration 205/1072, Loss: 5.0973\n",
      "Iteration 206/1072, Loss: 5.0265\n",
      "Iteration 207/1072, Loss: 5.1878\n",
      "Iteration 208/1072, Loss: 5.1881\n",
      "Iteration 209/1072, Loss: 5.2651\n",
      "Iteration 210/1072, Loss: 5.2080\n",
      "Iteration 211/1072, Loss: 5.2852\n",
      "Iteration 212/1072, Loss: 5.4323\n",
      "Iteration 213/1072, Loss: 5.0036\n",
      "Iteration 214/1072, Loss: 5.6924\n",
      "Iteration 215/1072, Loss: 5.7687\n",
      "Iteration 216/1072, Loss: 5.3365\n",
      "Iteration 217/1072, Loss: 5.4281\n",
      "Iteration 218/1072, Loss: 5.2247\n",
      "Iteration 219/1072, Loss: 5.9306\n",
      "Iteration 220/1072, Loss: 5.0727\n",
      "Iteration 221/1072, Loss: 5.3619\n",
      "Iteration 222/1072, Loss: 5.7354\n",
      "Iteration 223/1072, Loss: 5.1465\n",
      "Iteration 224/1072, Loss: 5.2799\n",
      "Iteration 225/1072, Loss: 4.7911\n",
      "Iteration 226/1072, Loss: 5.2315\n",
      "Iteration 227/1072, Loss: 5.4158\n",
      "Iteration 228/1072, Loss: 5.0755\n",
      "Iteration 229/1072, Loss: 5.2361\n",
      "Iteration 230/1072, Loss: 5.2123\n",
      "Iteration 231/1072, Loss: 5.8964\n",
      "Iteration 232/1072, Loss: 5.6129\n",
      "Iteration 233/1072, Loss: 4.9458\n",
      "Iteration 234/1072, Loss: 5.7992\n",
      "Iteration 235/1072, Loss: 5.4957\n",
      "Iteration 236/1072, Loss: 4.9039\n",
      "Iteration 237/1072, Loss: 5.4804\n",
      "Iteration 238/1072, Loss: 5.2933\n",
      "Iteration 239/1072, Loss: 5.3648\n",
      "Iteration 240/1072, Loss: 5.0993\n",
      "Iteration 241/1072, Loss: 5.1680\n",
      "Iteration 242/1072, Loss: 5.3254\n",
      "Iteration 243/1072, Loss: 5.1038\n",
      "Iteration 244/1072, Loss: 4.9163\n",
      "Iteration 245/1072, Loss: 5.1993\n",
      "Iteration 246/1072, Loss: 5.5037\n",
      "Iteration 247/1072, Loss: 5.2586\n",
      "Iteration 248/1072, Loss: 5.7680\n",
      "Iteration 249/1072, Loss: 4.9737\n",
      "Iteration 250/1072, Loss: 4.9295\n",
      "Iteration 251/1072, Loss: 4.9228\n",
      "Iteration 252/1072, Loss: 5.2461\n",
      "Iteration 253/1072, Loss: 5.0210\n",
      "Iteration 254/1072, Loss: 4.8882\n",
      "Iteration 255/1072, Loss: 5.3603\n",
      "Iteration 256/1072, Loss: 4.9312\n",
      "Iteration 257/1072, Loss: 5.5981\n",
      "Iteration 258/1072, Loss: 5.6358\n",
      "Iteration 259/1072, Loss: 5.4248\n",
      "Iteration 260/1072, Loss: 4.9661\n",
      "Iteration 261/1072, Loss: 5.0846\n",
      "Iteration 262/1072, Loss: 5.0523\n",
      "Iteration 263/1072, Loss: 5.4695\n",
      "Iteration 264/1072, Loss: 5.6041\n",
      "Iteration 265/1072, Loss: 5.3391\n",
      "Iteration 266/1072, Loss: 5.5120\n",
      "Iteration 267/1072, Loss: 5.5712\n",
      "Iteration 268/1072, Loss: 5.3112\n",
      "Iteration 269/1072, Loss: 5.0418\n",
      "Iteration 270/1072, Loss: 5.0113\n",
      "Iteration 271/1072, Loss: 4.9363\n",
      "Iteration 272/1072, Loss: 5.0456\n",
      "Iteration 273/1072, Loss: 5.3919\n",
      "Iteration 274/1072, Loss: 4.9606\n",
      "Iteration 275/1072, Loss: 5.6168\n",
      "Iteration 276/1072, Loss: 5.7957\n",
      "Iteration 277/1072, Loss: 5.4773\n",
      "Iteration 278/1072, Loss: 5.2687\n",
      "Iteration 279/1072, Loss: 5.3378\n",
      "Iteration 280/1072, Loss: 5.2431\n",
      "Iteration 281/1072, Loss: 5.4308\n",
      "Iteration 282/1072, Loss: 5.1690\n",
      "Iteration 283/1072, Loss: 5.4712\n",
      "Iteration 284/1072, Loss: 5.3812\n",
      "Iteration 285/1072, Loss: 5.5030\n",
      "Iteration 286/1072, Loss: 5.6046\n",
      "Iteration 287/1072, Loss: 5.1856\n",
      "Iteration 288/1072, Loss: 5.1483\n",
      "Iteration 289/1072, Loss: 5.4485\n",
      "Iteration 290/1072, Loss: 5.1107\n",
      "Iteration 291/1072, Loss: 4.9636\n",
      "Iteration 292/1072, Loss: 5.5603\n",
      "Iteration 293/1072, Loss: 4.8271\n",
      "Iteration 294/1072, Loss: 5.0406\n",
      "Iteration 295/1072, Loss: 5.1656\n",
      "Iteration 296/1072, Loss: 4.9475\n",
      "Iteration 297/1072, Loss: 5.4938\n",
      "Iteration 298/1072, Loss: 5.3302\n",
      "Iteration 299/1072, Loss: 5.2373\n",
      "Iteration 300/1072, Loss: 5.5025\n",
      "Iteration 301/1072, Loss: 4.9698\n",
      "Iteration 302/1072, Loss: 5.1865\n",
      "Iteration 303/1072, Loss: 5.5498\n",
      "Iteration 304/1072, Loss: 5.0571\n",
      "Iteration 305/1072, Loss: 5.7246\n",
      "Iteration 306/1072, Loss: 5.5107\n",
      "Iteration 307/1072, Loss: 5.5548\n",
      "Iteration 308/1072, Loss: 5.3259\n",
      "Iteration 309/1072, Loss: 5.4984\n",
      "Iteration 310/1072, Loss: 5.7883\n",
      "Iteration 311/1072, Loss: 5.4444\n",
      "Iteration 312/1072, Loss: 5.1388\n",
      "Iteration 313/1072, Loss: 5.6147\n",
      "Iteration 314/1072, Loss: 5.9444\n",
      "Iteration 315/1072, Loss: 5.7924\n",
      "Iteration 316/1072, Loss: 4.7628\n",
      "Iteration 317/1072, Loss: 4.8953\n",
      "Iteration 318/1072, Loss: 5.2334\n",
      "Iteration 319/1072, Loss: 5.9661\n",
      "Iteration 320/1072, Loss: 5.3690\n",
      "Iteration 321/1072, Loss: 5.8850\n",
      "Iteration 322/1072, Loss: 5.3205\n",
      "Iteration 323/1072, Loss: 5.0988\n",
      "Iteration 324/1072, Loss: 5.2976\n",
      "Iteration 325/1072, Loss: 5.5407\n",
      "Iteration 326/1072, Loss: 5.6153\n",
      "Iteration 327/1072, Loss: 5.2530\n",
      "Iteration 328/1072, Loss: 5.6440\n",
      "Iteration 329/1072, Loss: 4.5350\n",
      "Iteration 330/1072, Loss: 4.6943\n",
      "Iteration 331/1072, Loss: 5.2462\n",
      "Iteration 332/1072, Loss: 5.4504\n",
      "Iteration 333/1072, Loss: 5.4095\n",
      "Iteration 334/1072, Loss: 5.2075\n",
      "Iteration 335/1072, Loss: 5.9042\n",
      "Iteration 336/1072, Loss: 5.1702\n",
      "Iteration 337/1072, Loss: 5.1387\n",
      "Iteration 338/1072, Loss: 5.0292\n",
      "Iteration 339/1072, Loss: 5.0982\n",
      "Iteration 340/1072, Loss: 5.6050\n",
      "Iteration 341/1072, Loss: 5.3607\n",
      "Iteration 342/1072, Loss: 4.8707\n",
      "Iteration 343/1072, Loss: 5.1188\n",
      "Iteration 344/1072, Loss: 5.8145\n",
      "Iteration 345/1072, Loss: 5.1322\n",
      "Iteration 346/1072, Loss: 5.3977\n",
      "Iteration 347/1072, Loss: 5.2112\n",
      "Iteration 348/1072, Loss: 5.1641\n",
      "Iteration 349/1072, Loss: 5.3524\n",
      "Iteration 350/1072, Loss: 5.3667\n",
      "Iteration 351/1072, Loss: 5.4411\n",
      "Iteration 352/1072, Loss: 5.4416\n",
      "Iteration 353/1072, Loss: 5.1874\n",
      "Iteration 354/1072, Loss: 5.4974\n",
      "Iteration 355/1072, Loss: 4.9555\n",
      "Iteration 356/1072, Loss: 5.2549\n",
      "Iteration 357/1072, Loss: 5.4872\n",
      "Iteration 358/1072, Loss: 5.0843\n",
      "Iteration 359/1072, Loss: 5.6500\n",
      "Iteration 360/1072, Loss: 5.2953\n",
      "Iteration 361/1072, Loss: 5.0648\n",
      "Iteration 362/1072, Loss: 5.3170\n",
      "Iteration 363/1072, Loss: 5.0473\n",
      "Iteration 364/1072, Loss: 5.2926\n",
      "Iteration 365/1072, Loss: 5.1569\n",
      "Iteration 366/1072, Loss: 5.4735\n",
      "Iteration 367/1072, Loss: 4.8159\n",
      "Iteration 368/1072, Loss: 5.7487\n",
      "Iteration 369/1072, Loss: 5.4617\n",
      "Iteration 370/1072, Loss: 5.0361\n",
      "Iteration 371/1072, Loss: 4.8384\n",
      "Iteration 372/1072, Loss: 5.3964\n",
      "Iteration 373/1072, Loss: 5.4039\n",
      "Iteration 374/1072, Loss: 5.2165\n",
      "Iteration 375/1072, Loss: 4.8564\n",
      "Iteration 376/1072, Loss: 5.3067\n",
      "Iteration 377/1072, Loss: 5.0084\n",
      "Iteration 378/1072, Loss: 4.8014\n",
      "Iteration 379/1072, Loss: 5.3205\n",
      "Iteration 380/1072, Loss: 4.7910\n",
      "Iteration 381/1072, Loss: 5.0961\n",
      "Iteration 382/1072, Loss: 5.0869\n",
      "Iteration 383/1072, Loss: 5.8473\n",
      "Iteration 384/1072, Loss: 4.9748\n",
      "Iteration 385/1072, Loss: 5.5211\n",
      "Iteration 386/1072, Loss: 5.4449\n",
      "Iteration 387/1072, Loss: 5.3642\n",
      "Iteration 388/1072, Loss: 5.4099\n",
      "Iteration 389/1072, Loss: 5.5980\n",
      "Iteration 390/1072, Loss: 5.2519\n",
      "Iteration 391/1072, Loss: 4.9807\n",
      "Iteration 392/1072, Loss: 5.9035\n",
      "Iteration 393/1072, Loss: 5.5437\n",
      "Iteration 394/1072, Loss: 5.6050\n",
      "Iteration 395/1072, Loss: 5.4106\n",
      "Iteration 396/1072, Loss: 5.6714\n",
      "Iteration 397/1072, Loss: 5.7184\n",
      "Iteration 398/1072, Loss: 4.9014\n",
      "Iteration 399/1072, Loss: 5.6108\n",
      "Iteration 400/1072, Loss: 5.2606\n",
      "Iteration 401/1072, Loss: 5.1084\n",
      "Iteration 402/1072, Loss: 5.6060\n",
      "Iteration 403/1072, Loss: 5.0558\n",
      "Iteration 404/1072, Loss: 5.5733\n",
      "Iteration 405/1072, Loss: 4.7794\n",
      "Iteration 406/1072, Loss: 5.1062\n",
      "Iteration 407/1072, Loss: 5.3118\n",
      "Iteration 408/1072, Loss: 5.5741\n",
      "Iteration 409/1072, Loss: 5.6565\n",
      "Iteration 410/1072, Loss: 5.4414\n",
      "Iteration 411/1072, Loss: 5.2267\n",
      "Iteration 412/1072, Loss: 4.7699\n",
      "Iteration 413/1072, Loss: 5.3105\n",
      "Iteration 414/1072, Loss: 5.3235\n",
      "Iteration 415/1072, Loss: 5.1964\n",
      "Iteration 416/1072, Loss: 5.0123\n",
      "Iteration 417/1072, Loss: 5.7255\n",
      "Iteration 418/1072, Loss: 4.7317\n",
      "Iteration 419/1072, Loss: 5.3818\n",
      "Iteration 420/1072, Loss: 5.1923\n",
      "Iteration 421/1072, Loss: 5.2675\n",
      "Iteration 422/1072, Loss: 5.2140\n",
      "Iteration 423/1072, Loss: 5.2214\n",
      "Iteration 424/1072, Loss: 5.1845\n",
      "Iteration 425/1072, Loss: 5.0764\n",
      "Iteration 426/1072, Loss: 5.3480\n",
      "Iteration 427/1072, Loss: 5.2919\n",
      "Iteration 428/1072, Loss: 5.0643\n",
      "Iteration 429/1072, Loss: 5.1131\n",
      "Iteration 430/1072, Loss: 5.3204\n",
      "Iteration 431/1072, Loss: 5.3352\n",
      "Iteration 432/1072, Loss: 5.5499\n",
      "Iteration 433/1072, Loss: 5.0348\n",
      "Iteration 434/1072, Loss: 5.0341\n",
      "Iteration 435/1072, Loss: 5.0459\n",
      "Iteration 436/1072, Loss: 5.2466\n",
      "Iteration 437/1072, Loss: 5.3678\n",
      "Iteration 438/1072, Loss: 5.1408\n",
      "Iteration 439/1072, Loss: 5.0774\n",
      "Iteration 440/1072, Loss: 5.6933\n",
      "Iteration 441/1072, Loss: 5.6574\n",
      "Iteration 442/1072, Loss: 4.8199\n",
      "Iteration 443/1072, Loss: 5.6115\n",
      "Iteration 444/1072, Loss: 5.4851\n",
      "Iteration 445/1072, Loss: 4.8001\n",
      "Iteration 446/1072, Loss: 5.2185\n",
      "Iteration 447/1072, Loss: 5.0930\n",
      "Iteration 448/1072, Loss: 5.1248\n",
      "Iteration 449/1072, Loss: 5.0605\n",
      "Iteration 450/1072, Loss: 5.1070\n",
      "Iteration 451/1072, Loss: 5.6755\n",
      "Iteration 452/1072, Loss: 5.3632\n",
      "Iteration 453/1072, Loss: 5.3361\n",
      "Iteration 454/1072, Loss: 5.2639\n",
      "Iteration 455/1072, Loss: 4.9254\n",
      "Iteration 456/1072, Loss: 5.3083\n",
      "Iteration 457/1072, Loss: 5.2432\n",
      "Iteration 458/1072, Loss: 5.2141\n",
      "Iteration 459/1072, Loss: 5.3056\n",
      "Iteration 460/1072, Loss: 5.0065\n",
      "Iteration 461/1072, Loss: 5.1259\n",
      "Iteration 462/1072, Loss: 5.5261\n",
      "Iteration 463/1072, Loss: 5.8075\n",
      "Iteration 464/1072, Loss: 5.6936\n",
      "Iteration 465/1072, Loss: 4.9775\n",
      "Iteration 466/1072, Loss: 5.1353\n",
      "Iteration 467/1072, Loss: 5.1416\n",
      "Iteration 468/1072, Loss: 5.4045\n",
      "Iteration 469/1072, Loss: 5.1719\n",
      "Iteration 470/1072, Loss: 5.3500\n",
      "Iteration 471/1072, Loss: 5.0308\n",
      "Iteration 472/1072, Loss: 5.2802\n",
      "Iteration 473/1072, Loss: 5.5164\n",
      "Iteration 474/1072, Loss: 5.5545\n",
      "Iteration 475/1072, Loss: 5.2646\n",
      "Iteration 476/1072, Loss: 5.1018\n",
      "Iteration 477/1072, Loss: 5.4583\n",
      "Iteration 478/1072, Loss: 5.2786\n",
      "Iteration 479/1072, Loss: 5.7785\n",
      "Iteration 480/1072, Loss: 5.1584\n",
      "Iteration 481/1072, Loss: 5.1959\n",
      "Iteration 482/1072, Loss: 5.4768\n",
      "Iteration 483/1072, Loss: 4.8990\n",
      "Iteration 484/1072, Loss: 4.8628\n",
      "Iteration 485/1072, Loss: 5.4322\n",
      "Iteration 486/1072, Loss: 5.5967\n",
      "Iteration 487/1072, Loss: 5.4366\n",
      "Iteration 488/1072, Loss: 5.1181\n",
      "Iteration 489/1072, Loss: 5.1606\n",
      "Iteration 490/1072, Loss: 5.3693\n",
      "Iteration 491/1072, Loss: 5.3718\n",
      "Iteration 492/1072, Loss: 5.3376\n",
      "Iteration 493/1072, Loss: 5.3007\n",
      "Iteration 494/1072, Loss: 4.5429\n",
      "Iteration 495/1072, Loss: 5.1660\n",
      "Iteration 496/1072, Loss: 5.0104\n",
      "Iteration 497/1072, Loss: 5.2714\n",
      "Iteration 498/1072, Loss: 4.6896\n",
      "Iteration 499/1072, Loss: 5.0377\n",
      "Iteration 500/1072, Loss: 5.1705\n",
      "Iteration 501/1072, Loss: 5.3512\n",
      "Iteration 502/1072, Loss: 5.2934\n",
      "Iteration 503/1072, Loss: 5.0508\n",
      "Iteration 504/1072, Loss: 5.2560\n",
      "Iteration 505/1072, Loss: 4.9605\n",
      "Iteration 506/1072, Loss: 5.7047\n",
      "Iteration 507/1072, Loss: 5.5254\n",
      "Iteration 508/1072, Loss: 5.3174\n",
      "Iteration 509/1072, Loss: 5.6757\n",
      "Iteration 510/1072, Loss: 5.1662\n",
      "Iteration 511/1072, Loss: 4.6676\n",
      "Iteration 512/1072, Loss: 5.4591\n",
      "Iteration 513/1072, Loss: 5.4697\n",
      "Iteration 514/1072, Loss: 5.2141\n",
      "Iteration 515/1072, Loss: 5.5108\n",
      "Iteration 516/1072, Loss: 5.2607\n",
      "Iteration 517/1072, Loss: 5.3162\n",
      "Iteration 518/1072, Loss: 5.0111\n",
      "Iteration 519/1072, Loss: 5.1615\n",
      "Iteration 520/1072, Loss: 5.7401\n",
      "Iteration 521/1072, Loss: 5.0275\n",
      "Iteration 522/1072, Loss: 5.6659\n",
      "Iteration 523/1072, Loss: 5.3828\n",
      "Iteration 524/1072, Loss: 5.2469\n",
      "Iteration 525/1072, Loss: 5.6926\n",
      "Iteration 526/1072, Loss: 5.3811\n",
      "Iteration 527/1072, Loss: 5.6991\n",
      "Iteration 528/1072, Loss: 5.6490\n",
      "Iteration 529/1072, Loss: 4.8729\n",
      "Iteration 530/1072, Loss: 4.6086\n",
      "Iteration 531/1072, Loss: 5.4776\n",
      "Iteration 532/1072, Loss: 4.8713\n",
      "Iteration 533/1072, Loss: 5.0113\n",
      "Iteration 534/1072, Loss: 5.2192\n",
      "Iteration 535/1072, Loss: 5.1877\n",
      "Iteration 536/1072, Loss: 5.7424\n",
      "Iteration 537/1072, Loss: 4.5786\n",
      "Iteration 538/1072, Loss: 5.1560\n",
      "Iteration 539/1072, Loss: 4.5045\n",
      "Iteration 540/1072, Loss: 5.1241\n",
      "Iteration 541/1072, Loss: 5.2288\n",
      "Iteration 542/1072, Loss: 5.3529\n",
      "Iteration 543/1072, Loss: 4.6688\n",
      "Iteration 544/1072, Loss: 5.3999\n",
      "Iteration 545/1072, Loss: 5.2057\n",
      "Iteration 546/1072, Loss: 4.6903\n",
      "Iteration 547/1072, Loss: 5.0437\n",
      "Iteration 548/1072, Loss: 5.1987\n",
      "Iteration 549/1072, Loss: 5.5849\n",
      "Iteration 550/1072, Loss: 5.0973\n",
      "Iteration 551/1072, Loss: 5.0448\n",
      "Iteration 552/1072, Loss: 4.8697\n",
      "Iteration 553/1072, Loss: 5.2090\n",
      "Iteration 554/1072, Loss: 5.6847\n",
      "Iteration 555/1072, Loss: 5.5044\n",
      "Iteration 556/1072, Loss: 4.8584\n",
      "Iteration 557/1072, Loss: 5.2456\n",
      "Iteration 558/1072, Loss: 5.1780\n",
      "Iteration 559/1072, Loss: 5.0395\n",
      "Iteration 560/1072, Loss: 5.4028\n",
      "Iteration 561/1072, Loss: 5.3284\n",
      "Iteration 562/1072, Loss: 5.4430\n",
      "Iteration 563/1072, Loss: 5.1478\n",
      "Iteration 564/1072, Loss: 5.6423\n",
      "Iteration 565/1072, Loss: 5.4542\n",
      "Iteration 566/1072, Loss: 5.1709\n",
      "Iteration 567/1072, Loss: 5.3710\n",
      "Iteration 568/1072, Loss: 5.1736\n",
      "Iteration 569/1072, Loss: 5.0368\n",
      "Iteration 570/1072, Loss: 5.4418\n",
      "Iteration 571/1072, Loss: 5.4398\n",
      "Iteration 572/1072, Loss: 5.5291\n",
      "Iteration 573/1072, Loss: 5.6654\n",
      "Iteration 574/1072, Loss: 5.4534\n",
      "Iteration 575/1072, Loss: 5.2507\n",
      "Iteration 576/1072, Loss: 5.4560\n",
      "Iteration 577/1072, Loss: 4.8500\n",
      "Iteration 578/1072, Loss: 5.1518\n",
      "Iteration 579/1072, Loss: 5.1122\n",
      "Iteration 580/1072, Loss: 5.2921\n",
      "Iteration 581/1072, Loss: 4.7722\n",
      "Iteration 582/1072, Loss: 5.2810\n",
      "Iteration 583/1072, Loss: 5.3274\n",
      "Iteration 584/1072, Loss: 5.3760\n",
      "Iteration 585/1072, Loss: 5.3732\n",
      "Iteration 586/1072, Loss: 5.1898\n",
      "Iteration 587/1072, Loss: 5.0738\n",
      "Iteration 588/1072, Loss: 5.5282\n",
      "Iteration 589/1072, Loss: 5.4954\n",
      "Iteration 590/1072, Loss: 4.8699\n",
      "Iteration 591/1072, Loss: 4.6753\n",
      "Iteration 592/1072, Loss: 4.7897\n",
      "Iteration 593/1072, Loss: 4.4612\n",
      "Iteration 594/1072, Loss: 4.9160\n",
      "Iteration 595/1072, Loss: 5.4855\n",
      "Iteration 596/1072, Loss: 5.4149\n",
      "Iteration 597/1072, Loss: 5.4567\n",
      "Iteration 598/1072, Loss: 5.0413\n",
      "Iteration 599/1072, Loss: 5.3075\n",
      "Iteration 600/1072, Loss: 5.2703\n",
      "Iteration 601/1072, Loss: 5.1484\n",
      "Iteration 602/1072, Loss: 4.9404\n",
      "Iteration 603/1072, Loss: 4.7667\n",
      "Iteration 604/1072, Loss: 5.0695\n",
      "Iteration 605/1072, Loss: 5.3585\n",
      "Iteration 606/1072, Loss: 4.9761\n",
      "Iteration 607/1072, Loss: 5.3214\n",
      "Iteration 608/1072, Loss: 5.6440\n",
      "Iteration 609/1072, Loss: 5.5962\n",
      "Iteration 610/1072, Loss: 5.3301\n",
      "Iteration 611/1072, Loss: 5.0822\n",
      "Iteration 612/1072, Loss: 5.2046\n",
      "Iteration 613/1072, Loss: 5.4400\n",
      "Iteration 614/1072, Loss: 5.3791\n",
      "Iteration 615/1072, Loss: 5.4926\n",
      "Iteration 616/1072, Loss: 5.3098\n",
      "Iteration 617/1072, Loss: 5.1343\n",
      "Iteration 618/1072, Loss: 5.1673\n",
      "Iteration 619/1072, Loss: 4.7657\n",
      "Iteration 620/1072, Loss: 5.3703\n",
      "Iteration 621/1072, Loss: 5.2061\n",
      "Iteration 622/1072, Loss: 5.1671\n",
      "Iteration 623/1072, Loss: 5.4852\n",
      "Iteration 624/1072, Loss: 5.0403\n",
      "Iteration 625/1072, Loss: 5.0977\n",
      "Iteration 626/1072, Loss: 5.4937\n",
      "Iteration 627/1072, Loss: 5.5062\n",
      "Iteration 628/1072, Loss: 5.2820\n",
      "Iteration 629/1072, Loss: 4.9220\n",
      "Iteration 630/1072, Loss: 4.9526\n",
      "Iteration 631/1072, Loss: 4.8965\n",
      "Iteration 632/1072, Loss: 5.1703\n",
      "Iteration 633/1072, Loss: 5.0975\n",
      "Iteration 634/1072, Loss: 5.2999\n",
      "Iteration 635/1072, Loss: 4.7469\n",
      "Iteration 636/1072, Loss: 5.5878\n",
      "Iteration 637/1072, Loss: 5.1853\n",
      "Iteration 638/1072, Loss: 5.3663\n",
      "Iteration 639/1072, Loss: 5.0655\n",
      "Iteration 640/1072, Loss: 5.4362\n",
      "Iteration 641/1072, Loss: 5.3332\n",
      "Iteration 642/1072, Loss: 4.8274\n",
      "Iteration 643/1072, Loss: 5.3444\n",
      "Iteration 644/1072, Loss: 4.9097\n",
      "Iteration 645/1072, Loss: 5.8065\n",
      "Iteration 646/1072, Loss: 5.0042\n",
      "Iteration 647/1072, Loss: 5.2776\n",
      "Iteration 648/1072, Loss: 4.7111\n",
      "Iteration 649/1072, Loss: 5.2490\n",
      "Iteration 650/1072, Loss: 5.3233\n",
      "Iteration 651/1072, Loss: 4.7781\n",
      "Iteration 652/1072, Loss: 4.9984\n",
      "Iteration 653/1072, Loss: 5.7490\n",
      "Iteration 654/1072, Loss: 5.2020\n",
      "Iteration 655/1072, Loss: 5.6002\n",
      "Iteration 656/1072, Loss: 4.9887\n",
      "Iteration 657/1072, Loss: 5.1107\n",
      "Iteration 658/1072, Loss: 4.9337\n",
      "Iteration 659/1072, Loss: 5.2657\n",
      "Iteration 660/1072, Loss: 5.4019\n",
      "Iteration 661/1072, Loss: 5.5905\n",
      "Iteration 662/1072, Loss: 5.2923\n",
      "Iteration 663/1072, Loss: 4.7784\n",
      "Iteration 664/1072, Loss: 5.4054\n",
      "Iteration 665/1072, Loss: 5.6629\n",
      "Iteration 666/1072, Loss: 4.8457\n",
      "Iteration 667/1072, Loss: 4.9396\n",
      "Iteration 668/1072, Loss: 5.0774\n",
      "Iteration 669/1072, Loss: 5.7383\n",
      "Iteration 670/1072, Loss: 4.9076\n",
      "Iteration 671/1072, Loss: 4.8031\n",
      "Iteration 672/1072, Loss: 4.9743\n",
      "Iteration 673/1072, Loss: 5.8913\n",
      "Iteration 674/1072, Loss: 5.1425\n",
      "Iteration 675/1072, Loss: 5.2350\n",
      "Iteration 676/1072, Loss: 5.5601\n",
      "Iteration 677/1072, Loss: 4.9961\n",
      "Iteration 678/1072, Loss: 5.7711\n",
      "Iteration 679/1072, Loss: 5.1227\n",
      "Iteration 680/1072, Loss: 5.2096\n",
      "Iteration 681/1072, Loss: 5.6192\n",
      "Iteration 682/1072, Loss: 5.5244\n",
      "Iteration 683/1072, Loss: 5.1277\n",
      "Iteration 684/1072, Loss: 4.9374\n",
      "Iteration 685/1072, Loss: 5.0892\n",
      "Iteration 686/1072, Loss: 5.1074\n",
      "Iteration 687/1072, Loss: 4.6892\n",
      "Iteration 688/1072, Loss: 5.1224\n",
      "Iteration 689/1072, Loss: 5.2134\n",
      "Iteration 690/1072, Loss: 5.0332\n",
      "Iteration 691/1072, Loss: 4.9644\n",
      "Iteration 692/1072, Loss: 5.1035\n",
      "Iteration 693/1072, Loss: 5.3695\n",
      "Iteration 694/1072, Loss: 5.6272\n",
      "Iteration 695/1072, Loss: 5.4996\n",
      "Iteration 696/1072, Loss: 4.9857\n",
      "Iteration 697/1072, Loss: 4.7750\n",
      "Iteration 698/1072, Loss: 5.2904\n",
      "Iteration 699/1072, Loss: 5.3989\n",
      "Iteration 700/1072, Loss: 5.1039\n",
      "Iteration 701/1072, Loss: 5.6711\n",
      "Iteration 702/1072, Loss: 5.5697\n",
      "Iteration 703/1072, Loss: 4.9362\n",
      "Iteration 704/1072, Loss: 5.1660\n",
      "Iteration 705/1072, Loss: 5.4325\n",
      "Iteration 706/1072, Loss: 5.6617\n",
      "Iteration 707/1072, Loss: 5.1062\n",
      "Iteration 708/1072, Loss: 5.1523\n",
      "Iteration 709/1072, Loss: 4.9137\n",
      "Iteration 710/1072, Loss: 5.4777\n",
      "Iteration 711/1072, Loss: 5.3019\n",
      "Iteration 712/1072, Loss: 5.6462\n",
      "Iteration 713/1072, Loss: 4.8603\n",
      "Iteration 714/1072, Loss: 5.0829\n",
      "Iteration 715/1072, Loss: 5.4328\n",
      "Iteration 716/1072, Loss: 5.2489\n",
      "Iteration 717/1072, Loss: 4.9902\n",
      "Iteration 718/1072, Loss: 4.8220\n",
      "Iteration 719/1072, Loss: 5.6842\n",
      "Iteration 720/1072, Loss: 5.0845\n",
      "Iteration 721/1072, Loss: 5.3109\n",
      "Iteration 722/1072, Loss: 5.2819\n",
      "Iteration 723/1072, Loss: 4.8126\n",
      "Iteration 724/1072, Loss: 5.4462\n",
      "Iteration 725/1072, Loss: 5.3734\n",
      "Iteration 726/1072, Loss: 5.2917\n",
      "Iteration 727/1072, Loss: 4.9391\n",
      "Iteration 728/1072, Loss: 5.3085\n",
      "Iteration 729/1072, Loss: 5.5510\n",
      "Iteration 730/1072, Loss: 4.9180\n",
      "Iteration 731/1072, Loss: 5.4784\n",
      "Iteration 732/1072, Loss: 5.6796\n",
      "Iteration 733/1072, Loss: 5.2855\n",
      "Iteration 734/1072, Loss: 5.0239\n",
      "Iteration 735/1072, Loss: 5.3778\n",
      "Iteration 736/1072, Loss: 5.1406\n",
      "Iteration 737/1072, Loss: 4.9996\n",
      "Iteration 738/1072, Loss: 5.6054\n",
      "Iteration 739/1072, Loss: 4.9306\n",
      "Iteration 740/1072, Loss: 4.9385\n",
      "Iteration 741/1072, Loss: 5.1130\n",
      "Iteration 742/1072, Loss: 5.3739\n",
      "Iteration 743/1072, Loss: 5.4257\n",
      "Iteration 744/1072, Loss: 5.1633\n",
      "Iteration 745/1072, Loss: 5.0161\n",
      "Iteration 746/1072, Loss: 5.2735\n",
      "Iteration 747/1072, Loss: 5.3681\n",
      "Iteration 748/1072, Loss: 4.7940\n",
      "Iteration 749/1072, Loss: 5.1237\n",
      "Iteration 750/1072, Loss: 4.7206\n",
      "Iteration 751/1072, Loss: 4.7211\n",
      "Iteration 752/1072, Loss: 4.6828\n",
      "Iteration 753/1072, Loss: 5.1494\n",
      "Iteration 754/1072, Loss: 5.5032\n",
      "Iteration 755/1072, Loss: 4.9350\n",
      "Iteration 756/1072, Loss: 5.5270\n",
      "Iteration 757/1072, Loss: 5.4897\n",
      "Iteration 758/1072, Loss: 5.1165\n",
      "Iteration 759/1072, Loss: 4.7999\n",
      "Iteration 760/1072, Loss: 5.2584\n",
      "Iteration 761/1072, Loss: 4.6883\n",
      "Iteration 762/1072, Loss: 5.4248\n",
      "Iteration 763/1072, Loss: 5.2110\n",
      "Iteration 764/1072, Loss: 5.2037\n",
      "Iteration 765/1072, Loss: 4.7223\n",
      "Iteration 766/1072, Loss: 4.9108\n",
      "Iteration 767/1072, Loss: 5.5231\n",
      "Iteration 768/1072, Loss: 5.2997\n",
      "Iteration 769/1072, Loss: 5.1455\n",
      "Iteration 770/1072, Loss: 5.4666\n",
      "Iteration 771/1072, Loss: 5.5599\n",
      "Iteration 772/1072, Loss: 5.0333\n",
      "Iteration 773/1072, Loss: 5.5250\n",
      "Iteration 774/1072, Loss: 5.5441\n",
      "Iteration 775/1072, Loss: 5.8833\n",
      "Iteration 776/1072, Loss: 5.2663\n",
      "Iteration 777/1072, Loss: 5.4067\n",
      "Iteration 778/1072, Loss: 5.4057\n",
      "Iteration 779/1072, Loss: 4.6813\n",
      "Iteration 780/1072, Loss: 5.3899\n",
      "Iteration 781/1072, Loss: 5.2515\n",
      "Iteration 782/1072, Loss: 5.3123\n",
      "Iteration 783/1072, Loss: 5.0884\n",
      "Iteration 784/1072, Loss: 5.0999\n",
      "Iteration 785/1072, Loss: 4.8810\n",
      "Iteration 786/1072, Loss: 4.9724\n",
      "Iteration 787/1072, Loss: 5.1778\n",
      "Iteration 788/1072, Loss: 5.2845\n",
      "Iteration 789/1072, Loss: 5.3115\n",
      "Iteration 790/1072, Loss: 4.9884\n",
      "Iteration 791/1072, Loss: 4.9736\n",
      "Iteration 792/1072, Loss: 5.2272\n",
      "Iteration 793/1072, Loss: 5.4788\n",
      "Iteration 794/1072, Loss: 5.5951\n",
      "Iteration 795/1072, Loss: 5.4325\n",
      "Iteration 796/1072, Loss: 5.1618\n",
      "Iteration 797/1072, Loss: 4.7781\n",
      "Iteration 798/1072, Loss: 5.0105\n",
      "Iteration 799/1072, Loss: 4.9408\n",
      "Iteration 800/1072, Loss: 5.0427\n",
      "Iteration 801/1072, Loss: 5.3206\n",
      "Iteration 802/1072, Loss: 5.1385\n",
      "Iteration 803/1072, Loss: 4.8205\n",
      "Iteration 804/1072, Loss: 5.3833\n",
      "Iteration 805/1072, Loss: 4.7352\n",
      "Iteration 806/1072, Loss: 4.5243\n",
      "Iteration 807/1072, Loss: 5.2548\n",
      "Iteration 808/1072, Loss: 5.3376\n",
      "Iteration 809/1072, Loss: 5.1429\n",
      "Iteration 810/1072, Loss: 5.3457\n",
      "Iteration 811/1072, Loss: 5.3008\n",
      "Iteration 812/1072, Loss: 4.6829\n",
      "Iteration 813/1072, Loss: 5.2544\n",
      "Iteration 814/1072, Loss: 5.7154\n",
      "Iteration 815/1072, Loss: 4.8916\n",
      "Iteration 816/1072, Loss: 5.3989\n",
      "Iteration 817/1072, Loss: 5.1792\n",
      "Iteration 818/1072, Loss: 5.4432\n",
      "Iteration 819/1072, Loss: 5.2361\n",
      "Iteration 820/1072, Loss: 5.4606\n",
      "Iteration 821/1072, Loss: 5.4296\n",
      "Iteration 822/1072, Loss: 5.1632\n",
      "Iteration 823/1072, Loss: 5.2993\n",
      "Iteration 824/1072, Loss: 5.5931\n",
      "Iteration 825/1072, Loss: 4.6689\n",
      "Iteration 826/1072, Loss: 5.1663\n",
      "Iteration 827/1072, Loss: 5.1249\n",
      "Iteration 828/1072, Loss: 5.0311\n",
      "Iteration 829/1072, Loss: 4.6947\n",
      "Iteration 830/1072, Loss: 5.0459\n",
      "Iteration 831/1072, Loss: 5.2596\n",
      "Iteration 832/1072, Loss: 5.0109\n",
      "Iteration 833/1072, Loss: 5.2647\n",
      "Iteration 834/1072, Loss: 5.0641\n",
      "Iteration 835/1072, Loss: 5.1837\n",
      "Iteration 836/1072, Loss: 4.9454\n",
      "Iteration 837/1072, Loss: 5.0810\n",
      "Iteration 838/1072, Loss: 5.4544\n",
      "Iteration 839/1072, Loss: 5.1531\n",
      "Iteration 840/1072, Loss: 4.8120\n",
      "Iteration 841/1072, Loss: 4.9812\n",
      "Iteration 842/1072, Loss: 4.9955\n",
      "Iteration 843/1072, Loss: 4.8759\n",
      "Iteration 844/1072, Loss: 5.4900\n",
      "Iteration 845/1072, Loss: 4.9347\n",
      "Iteration 846/1072, Loss: 5.3089\n",
      "Iteration 847/1072, Loss: 5.0399\n",
      "Iteration 848/1072, Loss: 5.5118\n",
      "Iteration 849/1072, Loss: 4.4828\n",
      "Iteration 850/1072, Loss: 5.3365\n",
      "Iteration 851/1072, Loss: 4.8182\n",
      "Iteration 852/1072, Loss: 5.4398\n",
      "Iteration 853/1072, Loss: 5.2702\n",
      "Iteration 854/1072, Loss: 5.3320\n",
      "Iteration 855/1072, Loss: 5.3446\n",
      "Iteration 856/1072, Loss: 5.1970\n",
      "Iteration 857/1072, Loss: 4.9258\n",
      "Iteration 858/1072, Loss: 5.4154\n",
      "Iteration 859/1072, Loss: 5.4393\n",
      "Iteration 860/1072, Loss: 5.3536\n",
      "Iteration 861/1072, Loss: 5.0940\n",
      "Iteration 862/1072, Loss: 5.4685\n",
      "Iteration 863/1072, Loss: 4.7531\n",
      "Iteration 864/1072, Loss: 5.2340\n",
      "Iteration 865/1072, Loss: 5.8341\n",
      "Iteration 866/1072, Loss: 4.9765\n",
      "Iteration 867/1072, Loss: 5.2796\n",
      "Iteration 868/1072, Loss: 4.8204\n",
      "Iteration 869/1072, Loss: 5.2935\n",
      "Iteration 870/1072, Loss: 5.0137\n",
      "Iteration 871/1072, Loss: 5.4200\n",
      "Iteration 872/1072, Loss: 4.8546\n",
      "Iteration 873/1072, Loss: 5.0953\n",
      "Iteration 874/1072, Loss: 4.7186\n",
      "Iteration 875/1072, Loss: 4.7254\n",
      "Iteration 876/1072, Loss: 4.7903\n",
      "Iteration 877/1072, Loss: 4.9674\n",
      "Iteration 878/1072, Loss: 5.1869\n",
      "Iteration 879/1072, Loss: 4.7427\n",
      "Iteration 880/1072, Loss: 5.3253\n",
      "Iteration 881/1072, Loss: 4.7566\n",
      "Iteration 882/1072, Loss: 4.9974\n",
      "Iteration 883/1072, Loss: 5.1775\n",
      "Iteration 884/1072, Loss: 5.2314\n",
      "Iteration 885/1072, Loss: 5.3097\n",
      "Iteration 886/1072, Loss: 5.0758\n",
      "Iteration 887/1072, Loss: 4.9235\n",
      "Iteration 888/1072, Loss: 4.9320\n",
      "Iteration 889/1072, Loss: 5.6388\n",
      "Iteration 890/1072, Loss: 5.1142\n",
      "Iteration 891/1072, Loss: 5.3686\n",
      "Iteration 892/1072, Loss: 5.3268\n",
      "Iteration 893/1072, Loss: 5.0432\n",
      "Iteration 894/1072, Loss: 4.8015\n",
      "Iteration 895/1072, Loss: 5.3403\n",
      "Iteration 896/1072, Loss: 5.1606\n",
      "Iteration 897/1072, Loss: 4.8299\n",
      "Iteration 898/1072, Loss: 5.3840\n",
      "Iteration 899/1072, Loss: 4.7435\n",
      "Iteration 900/1072, Loss: 5.3124\n",
      "Iteration 901/1072, Loss: 5.2862\n",
      "Iteration 902/1072, Loss: 5.4380\n",
      "Iteration 903/1072, Loss: 4.9525\n",
      "Iteration 904/1072, Loss: 5.2979\n",
      "Iteration 905/1072, Loss: 5.6521\n",
      "Iteration 906/1072, Loss: 4.7757\n",
      "Iteration 907/1072, Loss: 5.1553\n",
      "Iteration 908/1072, Loss: 4.7213\n",
      "Iteration 909/1072, Loss: 5.7138\n",
      "Iteration 910/1072, Loss: 5.0793\n",
      "Iteration 911/1072, Loss: 4.9794\n",
      "Iteration 912/1072, Loss: 5.1157\n",
      "Iteration 913/1072, Loss: 4.9508\n",
      "Iteration 914/1072, Loss: 5.8217\n",
      "Iteration 915/1072, Loss: 5.5887\n",
      "Iteration 916/1072, Loss: 4.6737\n",
      "Iteration 917/1072, Loss: 5.1166\n",
      "Iteration 918/1072, Loss: 5.6090\n",
      "Iteration 919/1072, Loss: 4.9022\n",
      "Iteration 920/1072, Loss: 5.2628\n",
      "Iteration 921/1072, Loss: 4.9386\n",
      "Iteration 922/1072, Loss: 5.0119\n",
      "Iteration 923/1072, Loss: 5.0231\n",
      "Iteration 924/1072, Loss: 5.0585\n",
      "Iteration 925/1072, Loss: 4.8186\n",
      "Iteration 926/1072, Loss: 4.8193\n",
      "Iteration 927/1072, Loss: 5.3099\n",
      "Iteration 928/1072, Loss: 5.0481\n",
      "Iteration 929/1072, Loss: 5.3691\n",
      "Iteration 930/1072, Loss: 5.1614\n",
      "Iteration 931/1072, Loss: 5.6869\n",
      "Iteration 932/1072, Loss: 5.4699\n",
      "Iteration 933/1072, Loss: 4.9162\n",
      "Iteration 934/1072, Loss: 5.0778\n",
      "Iteration 935/1072, Loss: 5.3683\n",
      "Iteration 936/1072, Loss: 4.8674\n",
      "Iteration 937/1072, Loss: 4.6898\n",
      "Iteration 938/1072, Loss: 4.5534\n",
      "Iteration 939/1072, Loss: 5.3058\n",
      "Iteration 940/1072, Loss: 4.9737\n",
      "Iteration 941/1072, Loss: 4.7122\n",
      "Iteration 942/1072, Loss: 5.4732\n",
      "Iteration 943/1072, Loss: 5.7663\n",
      "Iteration 944/1072, Loss: 5.1192\n",
      "Iteration 945/1072, Loss: 5.2109\n",
      "Iteration 946/1072, Loss: 4.9086\n",
      "Iteration 947/1072, Loss: 5.1380\n",
      "Iteration 948/1072, Loss: 5.1155\n",
      "Iteration 949/1072, Loss: 5.2368\n",
      "Iteration 950/1072, Loss: 5.0968\n",
      "Iteration 951/1072, Loss: 5.0683\n",
      "Iteration 952/1072, Loss: 5.1010\n",
      "Iteration 953/1072, Loss: 4.9434\n",
      "Iteration 954/1072, Loss: 4.9172\n",
      "Iteration 955/1072, Loss: 5.0335\n",
      "Iteration 956/1072, Loss: 4.9416\n",
      "Iteration 957/1072, Loss: 5.3022\n",
      "Iteration 958/1072, Loss: 5.2001\n",
      "Iteration 959/1072, Loss: 5.1308\n",
      "Iteration 960/1072, Loss: 5.6566\n",
      "Iteration 961/1072, Loss: 4.8980\n",
      "Iteration 962/1072, Loss: 5.4808\n",
      "Iteration 963/1072, Loss: 5.1187\n",
      "Iteration 964/1072, Loss: 5.3879\n",
      "Iteration 965/1072, Loss: 4.6603\n",
      "Iteration 966/1072, Loss: 4.8557\n",
      "Iteration 967/1072, Loss: 5.0029\n",
      "Iteration 968/1072, Loss: 4.8912\n",
      "Iteration 969/1072, Loss: 4.8225\n",
      "Iteration 970/1072, Loss: 4.8360\n",
      "Iteration 971/1072, Loss: 5.3102\n",
      "Iteration 972/1072, Loss: 5.2249\n",
      "Iteration 973/1072, Loss: 5.1038\n",
      "Iteration 974/1072, Loss: 4.4239\n",
      "Iteration 975/1072, Loss: 5.4246\n",
      "Iteration 976/1072, Loss: 4.6380\n",
      "Iteration 977/1072, Loss: 5.2393\n",
      "Iteration 978/1072, Loss: 5.1875\n",
      "Iteration 979/1072, Loss: 5.2909\n",
      "Iteration 980/1072, Loss: 5.0305\n",
      "Iteration 981/1072, Loss: 5.0224\n",
      "Iteration 982/1072, Loss: 5.3613\n",
      "Iteration 983/1072, Loss: 5.0117\n",
      "Iteration 984/1072, Loss: 5.0391\n",
      "Iteration 985/1072, Loss: 5.1673\n",
      "Iteration 986/1072, Loss: 5.4410\n",
      "Iteration 987/1072, Loss: 5.1081\n",
      "Iteration 988/1072, Loss: 5.1068\n",
      "Iteration 989/1072, Loss: 5.2454\n",
      "Iteration 990/1072, Loss: 4.7332\n",
      "Iteration 991/1072, Loss: 4.9266\n",
      "Iteration 992/1072, Loss: 5.0162\n",
      "Iteration 993/1072, Loss: 5.0807\n",
      "Iteration 994/1072, Loss: 5.1083\n",
      "Iteration 995/1072, Loss: 5.0313\n",
      "Iteration 996/1072, Loss: 5.1671\n",
      "Iteration 997/1072, Loss: 5.0651\n",
      "Iteration 998/1072, Loss: 5.2801\n",
      "Iteration 999/1072, Loss: 5.0694\n",
      "Iteration 1000/1072, Loss: 4.8662\n",
      "Iteration 1001/1072, Loss: 5.1243\n",
      "Iteration 1002/1072, Loss: 5.2898\n",
      "Iteration 1003/1072, Loss: 5.4196\n",
      "Iteration 1004/1072, Loss: 5.3056\n",
      "Iteration 1005/1072, Loss: 5.2402\n",
      "Iteration 1006/1072, Loss: 5.3683\n",
      "Iteration 1007/1072, Loss: 5.0444\n",
      "Iteration 1008/1072, Loss: 5.6622\n",
      "Iteration 1009/1072, Loss: 5.2644\n",
      "Iteration 1010/1072, Loss: 5.1849\n",
      "Iteration 1011/1072, Loss: 5.2115\n",
      "Iteration 1012/1072, Loss: 4.6448\n",
      "Iteration 1013/1072, Loss: 5.2425\n",
      "Iteration 1014/1072, Loss: 4.7026\n",
      "Iteration 1015/1072, Loss: 5.1538\n",
      "Iteration 1016/1072, Loss: 5.1129\n",
      "Iteration 1017/1072, Loss: 5.2719\n",
      "Iteration 1018/1072, Loss: 5.0341\n",
      "Iteration 1019/1072, Loss: 4.8762\n",
      "Iteration 1020/1072, Loss: 4.7744\n",
      "Iteration 1021/1072, Loss: 5.1915\n",
      "Iteration 1022/1072, Loss: 4.7713\n",
      "Iteration 1023/1072, Loss: 5.2037\n",
      "Iteration 1024/1072, Loss: 4.6865\n",
      "Iteration 1025/1072, Loss: 4.5078\n",
      "Iteration 1026/1072, Loss: 4.9745\n",
      "Iteration 1027/1072, Loss: 4.5088\n",
      "Iteration 1028/1072, Loss: 4.5267\n",
      "Iteration 1029/1072, Loss: 4.8971\n",
      "Iteration 1030/1072, Loss: 4.7132\n",
      "Iteration 1031/1072, Loss: 5.0310\n",
      "Iteration 1032/1072, Loss: 4.9519\n",
      "Iteration 1033/1072, Loss: 5.0648\n",
      "Iteration 1034/1072, Loss: 4.9363\n",
      "Iteration 1035/1072, Loss: 5.0599\n",
      "Iteration 1036/1072, Loss: 5.1989\n",
      "Iteration 1037/1072, Loss: 4.9139\n",
      "Iteration 1038/1072, Loss: 5.1410\n",
      "Iteration 1039/1072, Loss: 5.1808\n",
      "Iteration 1040/1072, Loss: 4.9082\n",
      "Iteration 1041/1072, Loss: 4.7385\n",
      "Iteration 1042/1072, Loss: 5.7722\n",
      "Iteration 1043/1072, Loss: 4.9903\n",
      "Iteration 1044/1072, Loss: 5.1487\n",
      "Iteration 1045/1072, Loss: 5.3492\n",
      "Iteration 1046/1072, Loss: 5.2062\n",
      "Iteration 1047/1072, Loss: 5.4853\n",
      "Iteration 1048/1072, Loss: 4.6895\n",
      "Iteration 1049/1072, Loss: 4.7528\n",
      "Iteration 1050/1072, Loss: 4.4643\n",
      "Iteration 1051/1072, Loss: 5.3708\n",
      "Iteration 1052/1072, Loss: 5.0898\n",
      "Iteration 1053/1072, Loss: 4.8150\n",
      "Iteration 1054/1072, Loss: 4.6748\n",
      "Iteration 1055/1072, Loss: 5.1155\n",
      "Iteration 1056/1072, Loss: 4.4634\n",
      "Iteration 1057/1072, Loss: 5.1457\n",
      "Iteration 1058/1072, Loss: 4.6165\n",
      "Iteration 1059/1072, Loss: 5.0921\n",
      "Iteration 1060/1072, Loss: 4.9198\n",
      "Iteration 1061/1072, Loss: 4.9868\n",
      "Iteration 1062/1072, Loss: 5.5847\n",
      "Iteration 1063/1072, Loss: 5.2435\n",
      "Iteration 1064/1072, Loss: 5.0800\n",
      "Iteration 1065/1072, Loss: 4.9173\n",
      "Iteration 1066/1072, Loss: 5.2185\n",
      "Iteration 1067/1072, Loss: 5.3352\n",
      "Iteration 1068/1072, Loss: 4.8585\n",
      "Iteration 1069/1072, Loss: 4.6881\n",
      "Iteration 1070/1072, Loss: 4.8158\n",
      "Iteration 1071/1072, Loss: 5.3551\n",
      "Iteration 1072/1072, Loss: 6.4438\n",
      "Epoch 8/10, Loss: 5.2235\n",
      "Validation Accuracy: 21.57%\n",
      "Model checkpoint saved!\n",
      "Iteration 1/1072, Loss: 4.9393\n",
      "Iteration 2/1072, Loss: 4.8149\n",
      "Iteration 3/1072, Loss: 4.6611\n",
      "Iteration 4/1072, Loss: 4.9235\n",
      "Iteration 5/1072, Loss: 4.7017\n",
      "Iteration 6/1072, Loss: 4.7940\n",
      "Iteration 7/1072, Loss: 4.4319\n",
      "Iteration 8/1072, Loss: 4.8183\n",
      "Iteration 9/1072, Loss: 4.5512\n",
      "Iteration 10/1072, Loss: 4.9901\n",
      "Iteration 11/1072, Loss: 4.2819\n",
      "Iteration 12/1072, Loss: 4.8198\n",
      "Iteration 13/1072, Loss: 5.1892\n",
      "Iteration 14/1072, Loss: 5.0325\n",
      "Iteration 15/1072, Loss: 3.9273\n",
      "Iteration 16/1072, Loss: 4.6345\n",
      "Iteration 17/1072, Loss: 4.2104\n",
      "Iteration 18/1072, Loss: 5.3171\n",
      "Iteration 19/1072, Loss: 4.4902\n",
      "Iteration 20/1072, Loss: 5.2815\n",
      "Iteration 21/1072, Loss: 5.2022\n",
      "Iteration 22/1072, Loss: 4.4278\n",
      "Iteration 23/1072, Loss: 4.6325\n",
      "Iteration 24/1072, Loss: 4.8620\n",
      "Iteration 25/1072, Loss: 4.9240\n",
      "Iteration 26/1072, Loss: 4.7791\n",
      "Iteration 27/1072, Loss: 4.8961\n",
      "Iteration 28/1072, Loss: 5.0866\n",
      "Iteration 29/1072, Loss: 4.8075\n",
      "Iteration 30/1072, Loss: 4.3572\n",
      "Iteration 31/1072, Loss: 5.0014\n",
      "Iteration 32/1072, Loss: 4.5963\n",
      "Iteration 33/1072, Loss: 5.0333\n",
      "Iteration 34/1072, Loss: 4.7952\n",
      "Iteration 35/1072, Loss: 4.7459\n",
      "Iteration 36/1072, Loss: 4.9151\n",
      "Iteration 37/1072, Loss: 4.7186\n",
      "Iteration 38/1072, Loss: 4.5584\n",
      "Iteration 39/1072, Loss: 5.3277\n",
      "Iteration 40/1072, Loss: 4.8119\n",
      "Iteration 41/1072, Loss: 4.9164\n",
      "Iteration 42/1072, Loss: 4.5536\n",
      "Iteration 43/1072, Loss: 4.6059\n",
      "Iteration 44/1072, Loss: 4.9852\n",
      "Iteration 45/1072, Loss: 5.0465\n",
      "Iteration 46/1072, Loss: 5.2071\n",
      "Iteration 47/1072, Loss: 4.6261\n",
      "Iteration 48/1072, Loss: 4.7481\n",
      "Iteration 49/1072, Loss: 5.0723\n",
      "Iteration 50/1072, Loss: 4.6486\n",
      "Iteration 51/1072, Loss: 4.8571\n",
      "Iteration 52/1072, Loss: 4.9773\n",
      "Iteration 53/1072, Loss: 5.0814\n",
      "Iteration 54/1072, Loss: 4.8338\n",
      "Iteration 55/1072, Loss: 4.5421\n",
      "Iteration 56/1072, Loss: 5.3107\n",
      "Iteration 57/1072, Loss: 4.3503\n",
      "Iteration 58/1072, Loss: 4.5564\n",
      "Iteration 59/1072, Loss: 5.1761\n",
      "Iteration 60/1072, Loss: 5.2003\n",
      "Iteration 61/1072, Loss: 4.6071\n",
      "Iteration 62/1072, Loss: 4.9653\n",
      "Iteration 63/1072, Loss: 4.9131\n",
      "Iteration 64/1072, Loss: 4.3337\n",
      "Iteration 65/1072, Loss: 4.5905\n",
      "Iteration 66/1072, Loss: 4.9916\n",
      "Iteration 67/1072, Loss: 4.3922\n",
      "Iteration 68/1072, Loss: 5.2028\n",
      "Iteration 69/1072, Loss: 4.6439\n",
      "Iteration 70/1072, Loss: 4.9702\n",
      "Iteration 71/1072, Loss: 4.7854\n",
      "Iteration 72/1072, Loss: 4.7303\n",
      "Iteration 73/1072, Loss: 4.8450\n",
      "Iteration 74/1072, Loss: 4.8142\n",
      "Iteration 75/1072, Loss: 4.7024\n",
      "Iteration 76/1072, Loss: 4.9628\n",
      "Iteration 77/1072, Loss: 4.8846\n",
      "Iteration 78/1072, Loss: 4.0473\n",
      "Iteration 79/1072, Loss: 4.9776\n",
      "Iteration 80/1072, Loss: 4.7062\n",
      "Iteration 81/1072, Loss: 4.8282\n",
      "Iteration 82/1072, Loss: 5.1107\n",
      "Iteration 83/1072, Loss: 4.1907\n",
      "Iteration 84/1072, Loss: 4.5966\n",
      "Iteration 85/1072, Loss: 4.3440\n",
      "Iteration 86/1072, Loss: 4.5100\n",
      "Iteration 87/1072, Loss: 5.2326\n",
      "Iteration 88/1072, Loss: 4.7238\n",
      "Iteration 89/1072, Loss: 4.7534\n",
      "Iteration 90/1072, Loss: 4.4477\n",
      "Iteration 91/1072, Loss: 4.6422\n",
      "Iteration 92/1072, Loss: 4.7466\n",
      "Iteration 93/1072, Loss: 5.0317\n",
      "Iteration 94/1072, Loss: 4.7900\n",
      "Iteration 95/1072, Loss: 4.9163\n",
      "Iteration 96/1072, Loss: 5.0281\n",
      "Iteration 97/1072, Loss: 4.9475\n",
      "Iteration 98/1072, Loss: 4.8691\n",
      "Iteration 99/1072, Loss: 4.9890\n",
      "Iteration 100/1072, Loss: 4.4918\n",
      "Iteration 101/1072, Loss: 4.4774\n",
      "Iteration 102/1072, Loss: 4.7803\n",
      "Iteration 103/1072, Loss: 4.4585\n",
      "Iteration 104/1072, Loss: 4.9623\n",
      "Iteration 105/1072, Loss: 4.6796\n",
      "Iteration 106/1072, Loss: 4.5789\n",
      "Iteration 107/1072, Loss: 4.4970\n",
      "Iteration 108/1072, Loss: 4.7892\n",
      "Iteration 109/1072, Loss: 4.7093\n",
      "Iteration 110/1072, Loss: 4.4867\n",
      "Iteration 111/1072, Loss: 4.7360\n",
      "Iteration 112/1072, Loss: 4.8835\n",
      "Iteration 113/1072, Loss: 4.6773\n",
      "Iteration 114/1072, Loss: 4.6951\n",
      "Iteration 115/1072, Loss: 4.3920\n",
      "Iteration 116/1072, Loss: 4.5723\n",
      "Iteration 117/1072, Loss: 4.4223\n",
      "Iteration 118/1072, Loss: 5.3441\n",
      "Iteration 119/1072, Loss: 4.8709\n",
      "Iteration 120/1072, Loss: 5.1785\n",
      "Iteration 121/1072, Loss: 4.6551\n",
      "Iteration 122/1072, Loss: 4.3663\n",
      "Iteration 123/1072, Loss: 4.8314\n",
      "Iteration 124/1072, Loss: 5.0051\n",
      "Iteration 125/1072, Loss: 4.6673\n",
      "Iteration 126/1072, Loss: 4.9130\n",
      "Iteration 127/1072, Loss: 4.9017\n",
      "Iteration 128/1072, Loss: 4.6067\n",
      "Iteration 129/1072, Loss: 4.3340\n",
      "Iteration 130/1072, Loss: 4.2084\n",
      "Iteration 131/1072, Loss: 5.3862\n",
      "Iteration 132/1072, Loss: 4.7582\n",
      "Iteration 133/1072, Loss: 4.2081\n",
      "Iteration 134/1072, Loss: 4.7851\n",
      "Iteration 135/1072, Loss: 4.7026\n",
      "Iteration 136/1072, Loss: 4.6651\n",
      "Iteration 137/1072, Loss: 4.6861\n",
      "Iteration 138/1072, Loss: 4.9988\n",
      "Iteration 139/1072, Loss: 4.5832\n",
      "Iteration 140/1072, Loss: 4.3855\n",
      "Iteration 141/1072, Loss: 4.5945\n",
      "Iteration 142/1072, Loss: 4.7739\n",
      "Iteration 143/1072, Loss: 4.3557\n",
      "Iteration 144/1072, Loss: 5.2647\n",
      "Iteration 145/1072, Loss: 4.8647\n",
      "Iteration 146/1072, Loss: 5.2748\n",
      "Iteration 147/1072, Loss: 4.8849\n",
      "Iteration 148/1072, Loss: 4.6658\n",
      "Iteration 149/1072, Loss: 5.2913\n",
      "Iteration 150/1072, Loss: 5.1922\n",
      "Iteration 151/1072, Loss: 4.3211\n",
      "Iteration 152/1072, Loss: 4.9382\n",
      "Iteration 153/1072, Loss: 4.5954\n",
      "Iteration 154/1072, Loss: 4.6505\n",
      "Iteration 155/1072, Loss: 4.6090\n",
      "Iteration 156/1072, Loss: 5.1316\n",
      "Iteration 157/1072, Loss: 4.3224\n",
      "Iteration 158/1072, Loss: 4.4984\n",
      "Iteration 159/1072, Loss: 4.5773\n",
      "Iteration 160/1072, Loss: 4.5906\n",
      "Iteration 161/1072, Loss: 4.8895\n",
      "Iteration 162/1072, Loss: 4.3970\n",
      "Iteration 163/1072, Loss: 5.3816\n",
      "Iteration 164/1072, Loss: 4.4258\n",
      "Iteration 165/1072, Loss: 4.8759\n",
      "Iteration 166/1072, Loss: 5.0841\n",
      "Iteration 167/1072, Loss: 5.1371\n",
      "Iteration 168/1072, Loss: 5.0099\n",
      "Iteration 169/1072, Loss: 4.8460\n",
      "Iteration 170/1072, Loss: 5.3875\n",
      "Iteration 171/1072, Loss: 4.4351\n",
      "Iteration 172/1072, Loss: 4.6406\n",
      "Iteration 173/1072, Loss: 5.0064\n",
      "Iteration 174/1072, Loss: 4.7067\n",
      "Iteration 175/1072, Loss: 4.8651\n",
      "Iteration 176/1072, Loss: 4.5583\n",
      "Iteration 177/1072, Loss: 4.5326\n",
      "Iteration 178/1072, Loss: 5.1836\n",
      "Iteration 179/1072, Loss: 4.7492\n",
      "Iteration 180/1072, Loss: 4.8990\n",
      "Iteration 181/1072, Loss: 4.8753\n",
      "Iteration 182/1072, Loss: 4.9248\n",
      "Iteration 183/1072, Loss: 4.5610\n",
      "Iteration 184/1072, Loss: 4.6401\n",
      "Iteration 185/1072, Loss: 4.6993\n",
      "Iteration 186/1072, Loss: 4.9134\n",
      "Iteration 187/1072, Loss: 5.3816\n",
      "Iteration 188/1072, Loss: 4.7118\n",
      "Iteration 189/1072, Loss: 4.7907\n",
      "Iteration 190/1072, Loss: 4.3597\n",
      "Iteration 191/1072, Loss: 5.4537\n",
      "Iteration 192/1072, Loss: 4.7520\n",
      "Iteration 193/1072, Loss: 4.7564\n",
      "Iteration 194/1072, Loss: 4.9526\n",
      "Iteration 195/1072, Loss: 4.9739\n",
      "Iteration 196/1072, Loss: 4.7560\n",
      "Iteration 197/1072, Loss: 4.8812\n",
      "Iteration 198/1072, Loss: 4.3748\n",
      "Iteration 199/1072, Loss: 4.8620\n",
      "Iteration 200/1072, Loss: 4.6448\n",
      "Iteration 201/1072, Loss: 4.4982\n",
      "Iteration 202/1072, Loss: 4.5394\n",
      "Iteration 203/1072, Loss: 5.3887\n",
      "Iteration 204/1072, Loss: 5.0321\n",
      "Iteration 205/1072, Loss: 4.5465\n",
      "Iteration 206/1072, Loss: 4.5294\n",
      "Iteration 207/1072, Loss: 4.8254\n",
      "Iteration 208/1072, Loss: 4.2548\n",
      "Iteration 209/1072, Loss: 5.0785\n",
      "Iteration 210/1072, Loss: 4.8956\n",
      "Iteration 211/1072, Loss: 5.1711\n",
      "Iteration 212/1072, Loss: 4.8314\n",
      "Iteration 213/1072, Loss: 4.3243\n",
      "Iteration 214/1072, Loss: 4.6972\n",
      "Iteration 215/1072, Loss: 4.4885\n",
      "Iteration 216/1072, Loss: 5.1707\n",
      "Iteration 217/1072, Loss: 4.5259\n",
      "Iteration 218/1072, Loss: 4.8249\n",
      "Iteration 219/1072, Loss: 4.6184\n",
      "Iteration 220/1072, Loss: 5.0386\n",
      "Iteration 221/1072, Loss: 4.4136\n",
      "Iteration 222/1072, Loss: 4.5277\n",
      "Iteration 223/1072, Loss: 4.6221\n",
      "Iteration 224/1072, Loss: 4.8032\n",
      "Iteration 225/1072, Loss: 4.7061\n",
      "Iteration 226/1072, Loss: 4.7198\n",
      "Iteration 227/1072, Loss: 4.8701\n",
      "Iteration 228/1072, Loss: 4.1663\n",
      "Iteration 229/1072, Loss: 4.2388\n",
      "Iteration 230/1072, Loss: 5.0424\n",
      "Iteration 231/1072, Loss: 4.4716\n",
      "Iteration 232/1072, Loss: 4.6169\n",
      "Iteration 233/1072, Loss: 5.0973\n",
      "Iteration 234/1072, Loss: 4.6149\n",
      "Iteration 235/1072, Loss: 4.2322\n",
      "Iteration 236/1072, Loss: 4.9668\n",
      "Iteration 237/1072, Loss: 5.3420\n",
      "Iteration 238/1072, Loss: 4.2006\n",
      "Iteration 239/1072, Loss: 4.8190\n",
      "Iteration 240/1072, Loss: 5.2395\n",
      "Iteration 241/1072, Loss: 4.5392\n",
      "Iteration 242/1072, Loss: 4.8992\n",
      "Iteration 243/1072, Loss: 4.0544\n",
      "Iteration 244/1072, Loss: 5.3158\n",
      "Iteration 245/1072, Loss: 4.4534\n",
      "Iteration 246/1072, Loss: 4.9966\n",
      "Iteration 247/1072, Loss: 4.5674\n",
      "Iteration 248/1072, Loss: 4.1902\n",
      "Iteration 249/1072, Loss: 4.4047\n",
      "Iteration 250/1072, Loss: 4.8567\n",
      "Iteration 251/1072, Loss: 4.8313\n",
      "Iteration 252/1072, Loss: 4.6887\n",
      "Iteration 253/1072, Loss: 4.8365\n",
      "Iteration 254/1072, Loss: 4.8112\n",
      "Iteration 255/1072, Loss: 4.4848\n",
      "Iteration 256/1072, Loss: 4.6548\n",
      "Iteration 257/1072, Loss: 5.2003\n",
      "Iteration 258/1072, Loss: 4.7428\n",
      "Iteration 259/1072, Loss: 4.6987\n",
      "Iteration 260/1072, Loss: 4.0928\n",
      "Iteration 261/1072, Loss: 4.9513\n",
      "Iteration 262/1072, Loss: 4.5827\n",
      "Iteration 263/1072, Loss: 5.0900\n",
      "Iteration 264/1072, Loss: 4.6468\n",
      "Iteration 265/1072, Loss: 4.8519\n",
      "Iteration 266/1072, Loss: 5.2282\n",
      "Iteration 267/1072, Loss: 4.7821\n",
      "Iteration 268/1072, Loss: 4.8041\n",
      "Iteration 269/1072, Loss: 5.0733\n",
      "Iteration 270/1072, Loss: 4.5393\n",
      "Iteration 271/1072, Loss: 4.7862\n",
      "Iteration 272/1072, Loss: 4.4054\n",
      "Iteration 273/1072, Loss: 4.5938\n",
      "Iteration 274/1072, Loss: 4.6955\n",
      "Iteration 275/1072, Loss: 4.7008\n",
      "Iteration 276/1072, Loss: 4.4262\n",
      "Iteration 277/1072, Loss: 5.0867\n",
      "Iteration 278/1072, Loss: 4.9773\n",
      "Iteration 279/1072, Loss: 4.7030\n",
      "Iteration 280/1072, Loss: 5.0638\n",
      "Iteration 281/1072, Loss: 5.1867\n",
      "Iteration 282/1072, Loss: 4.2403\n",
      "Iteration 283/1072, Loss: 4.9411\n",
      "Iteration 284/1072, Loss: 4.6953\n",
      "Iteration 285/1072, Loss: 4.4975\n",
      "Iteration 286/1072, Loss: 4.9746\n",
      "Iteration 287/1072, Loss: 5.4354\n",
      "Iteration 288/1072, Loss: 4.5038\n",
      "Iteration 289/1072, Loss: 4.5430\n",
      "Iteration 290/1072, Loss: 4.3011\n",
      "Iteration 291/1072, Loss: 5.0959\n",
      "Iteration 292/1072, Loss: 4.6607\n",
      "Iteration 293/1072, Loss: 4.5917\n",
      "Iteration 294/1072, Loss: 4.6516\n",
      "Iteration 295/1072, Loss: 5.1306\n",
      "Iteration 296/1072, Loss: 4.8092\n",
      "Iteration 319/1072, Loss: 4.5184\n",
      "Iteration 320/1072, Loss: 4.4999\n",
      "Iteration 321/1072, Loss: 5.1102\n",
      "Iteration 322/1072, Loss: 4.6004\n",
      "Iteration 323/1072, Loss: 4.7958\n",
      "Iteration 324/1072, Loss: 5.0407\n",
      "Iteration 325/1072, Loss: 4.4502\n",
      "Iteration 326/1072, Loss: 4.8357\n",
      "Iteration 327/1072, Loss: 4.7583\n",
      "Iteration 328/1072, Loss: 4.9258\n",
      "Iteration 329/1072, Loss: 4.6865\n",
      "Iteration 330/1072, Loss: 4.5927\n",
      "Iteration 331/1072, Loss: 4.0511\n",
      "Iteration 332/1072, Loss: 4.4116\n",
      "Iteration 333/1072, Loss: 4.8497\n",
      "Iteration 334/1072, Loss: 4.8206\n",
      "Iteration 335/1072, Loss: 4.7786\n",
      "Iteration 336/1072, Loss: 4.7958\n",
      "Iteration 337/1072, Loss: 5.0224\n",
      "Iteration 338/1072, Loss: 4.2914\n",
      "Iteration 339/1072, Loss: 4.4619\n",
      "Iteration 340/1072, Loss: 4.1907\n",
      "Iteration 341/1072, Loss: 5.0192\n",
      "Iteration 342/1072, Loss: 4.8728\n",
      "Iteration 343/1072, Loss: 4.6716\n",
      "Iteration 344/1072, Loss: 4.6093\n",
      "Iteration 345/1072, Loss: 4.3020\n",
      "Iteration 346/1072, Loss: 4.9662\n",
      "Iteration 347/1072, Loss: 5.0186\n",
      "Iteration 348/1072, Loss: 5.1250\n",
      "Iteration 349/1072, Loss: 4.9766\n",
      "Iteration 350/1072, Loss: 4.2170\n",
      "Iteration 351/1072, Loss: 5.0300\n",
      "Iteration 352/1072, Loss: 4.7460\n",
      "Iteration 353/1072, Loss: 4.7149\n",
      "Iteration 354/1072, Loss: 4.5495\n",
      "Iteration 355/1072, Loss: 4.1954\n",
      "Iteration 356/1072, Loss: 4.5708\n",
      "Iteration 357/1072, Loss: 4.8439\n",
      "Iteration 358/1072, Loss: 4.5827\n",
      "Iteration 359/1072, Loss: 4.7572\n",
      "Iteration 360/1072, Loss: 4.2609\n",
      "Iteration 361/1072, Loss: 4.5663\n",
      "Iteration 362/1072, Loss: 4.8372\n",
      "Iteration 363/1072, Loss: 4.5742\n",
      "Iteration 364/1072, Loss: 4.5841\n",
      "Iteration 365/1072, Loss: 4.9761\n",
      "Iteration 366/1072, Loss: 4.7684\n",
      "Iteration 367/1072, Loss: 5.0514\n",
      "Iteration 368/1072, Loss: 5.0268\n",
      "Iteration 369/1072, Loss: 4.1756\n",
      "Iteration 370/1072, Loss: 5.1165\n",
      "Iteration 371/1072, Loss: 4.4925\n",
      "Iteration 372/1072, Loss: 4.8044\n",
      "Iteration 373/1072, Loss: 5.0279\n",
      "Iteration 374/1072, Loss: 4.2525\n",
      "Iteration 375/1072, Loss: 5.2516\n",
      "Iteration 376/1072, Loss: 4.5201\n",
      "Iteration 377/1072, Loss: 5.3694\n",
      "Iteration 378/1072, Loss: 4.6351\n",
      "Iteration 379/1072, Loss: 5.1960\n",
      "Iteration 380/1072, Loss: 4.8501\n",
      "Iteration 381/1072, Loss: 4.6307\n",
      "Iteration 382/1072, Loss: 5.0485\n",
      "Iteration 383/1072, Loss: 4.4979\n",
      "Iteration 384/1072, Loss: 4.9814\n",
      "Iteration 385/1072, Loss: 4.6934\n",
      "Iteration 386/1072, Loss: 4.7874\n",
      "Iteration 387/1072, Loss: 5.2612\n",
      "Iteration 388/1072, Loss: 4.4843\n",
      "Iteration 389/1072, Loss: 4.5295\n",
      "Iteration 390/1072, Loss: 4.6641\n",
      "Iteration 391/1072, Loss: 4.6843\n",
      "Iteration 392/1072, Loss: 4.8895\n",
      "Iteration 393/1072, Loss: 4.6265\n",
      "Iteration 394/1072, Loss: 4.1052\n",
      "Iteration 395/1072, Loss: 4.8195\n",
      "Iteration 396/1072, Loss: 4.4915\n",
      "Iteration 397/1072, Loss: 5.3417\n",
      "Iteration 398/1072, Loss: 4.3073\n",
      "Iteration 399/1072, Loss: 4.6836\n",
      "Iteration 400/1072, Loss: 4.4519\n",
      "Iteration 401/1072, Loss: 4.8729\n",
      "Iteration 402/1072, Loss: 5.2749\n",
      "Iteration 403/1072, Loss: 4.6278\n",
      "Iteration 404/1072, Loss: 4.8974\n",
      "Iteration 405/1072, Loss: 4.9373\n",
      "Iteration 406/1072, Loss: 4.4748\n",
      "Iteration 407/1072, Loss: 4.4711\n",
      "Iteration 408/1072, Loss: 4.4566\n",
      "Iteration 409/1072, Loss: 4.5233\n",
      "Iteration 410/1072, Loss: 4.6179\n",
      "Iteration 411/1072, Loss: 4.8154\n",
      "Iteration 412/1072, Loss: 4.7178\n",
      "Iteration 413/1072, Loss: 5.0284\n",
      "Iteration 414/1072, Loss: 4.7115\n",
      "Iteration 415/1072, Loss: 5.1450\n",
      "Iteration 416/1072, Loss: 4.5832\n",
      "Iteration 417/1072, Loss: 5.3009\n",
      "Iteration 418/1072, Loss: 4.6383\n",
      "Iteration 419/1072, Loss: 4.3489\n",
      "Iteration 420/1072, Loss: 4.7791\n",
      "Iteration 421/1072, Loss: 5.1281\n",
      "Iteration 422/1072, Loss: 5.0526\n",
      "Iteration 423/1072, Loss: 4.6856\n",
      "Iteration 424/1072, Loss: 4.4712\n",
      "Iteration 425/1072, Loss: 4.5354\n",
      "Iteration 426/1072, Loss: 4.6459\n",
      "Iteration 427/1072, Loss: 4.4117\n",
      "Iteration 428/1072, Loss: 4.6656\n",
      "Iteration 429/1072, Loss: 4.4325\n",
      "Iteration 430/1072, Loss: 4.7184\n",
      "Iteration 431/1072, Loss: 4.8457\n",
      "Iteration 432/1072, Loss: 3.9421\n",
      "Iteration 433/1072, Loss: 4.9257\n",
      "Iteration 434/1072, Loss: 4.9455\n",
      "Iteration 435/1072, Loss: 4.4390\n",
      "Iteration 436/1072, Loss: 4.4499\n",
      "Iteration 437/1072, Loss: 4.8551\n",
      "Iteration 438/1072, Loss: 5.0634\n",
      "Iteration 439/1072, Loss: 4.8096\n",
      "Iteration 440/1072, Loss: 4.7592\n",
      "Iteration 441/1072, Loss: 4.7526\n",
      "Iteration 442/1072, Loss: 4.6749\n",
      "Iteration 443/1072, Loss: 4.5988\n",
      "Iteration 444/1072, Loss: 4.3253\n",
      "Iteration 445/1072, Loss: 5.0639\n",
      "Iteration 446/1072, Loss: 4.5845\n",
      "Iteration 447/1072, Loss: 4.6219\n",
      "Iteration 448/1072, Loss: 4.8726\n",
      "Iteration 449/1072, Loss: 4.8990\n",
      "Iteration 450/1072, Loss: 4.6496\n",
      "Iteration 451/1072, Loss: 4.4536\n",
      "Iteration 452/1072, Loss: 4.5683\n",
      "Iteration 453/1072, Loss: 5.1076\n",
      "Iteration 454/1072, Loss: 4.4174\n",
      "Iteration 455/1072, Loss: 5.0550\n",
      "Iteration 456/1072, Loss: 5.0657\n",
      "Iteration 457/1072, Loss: 4.4718\n",
      "Iteration 458/1072, Loss: 4.8994\n",
      "Iteration 459/1072, Loss: 4.4012\n",
      "Iteration 460/1072, Loss: 4.7574\n",
      "Iteration 461/1072, Loss: 4.5646\n",
      "Iteration 462/1072, Loss: 5.1998\n",
      "Iteration 463/1072, Loss: 4.7850\n",
      "Iteration 464/1072, Loss: 4.6047\n",
      "Iteration 465/1072, Loss: 4.4783\n",
      "Iteration 466/1072, Loss: 4.3809\n",
      "Iteration 467/1072, Loss: 5.0418\n",
      "Iteration 468/1072, Loss: 5.2619\n",
      "Iteration 469/1072, Loss: 4.4763\n",
      "Iteration 470/1072, Loss: 4.8035\n",
      "Iteration 471/1072, Loss: 5.1701\n",
      "Iteration 472/1072, Loss: 4.7178\n",
      "Iteration 473/1072, Loss: 4.8633\n",
      "Iteration 474/1072, Loss: 4.4075\n",
      "Iteration 475/1072, Loss: 4.6823\n",
      "Iteration 476/1072, Loss: 4.3410\n",
      "Iteration 477/1072, Loss: 4.4606\n",
      "Iteration 478/1072, Loss: 5.0142\n",
      "Iteration 479/1072, Loss: 5.2273\n",
      "Iteration 480/1072, Loss: 5.1924\n",
      "Iteration 481/1072, Loss: 4.9682\n",
      "Iteration 482/1072, Loss: 4.2208\n",
      "Iteration 483/1072, Loss: 4.5597\n",
      "Iteration 484/1072, Loss: 4.7542\n",
      "Iteration 485/1072, Loss: 4.2300\n",
      "Iteration 486/1072, Loss: 4.4933\n",
      "Iteration 487/1072, Loss: 4.9538\n",
      "Iteration 488/1072, Loss: 4.8350\n",
      "Iteration 489/1072, Loss: 4.7656\n",
      "Iteration 490/1072, Loss: 4.8755\n",
      "Iteration 491/1072, Loss: 4.8808\n",
      "Iteration 492/1072, Loss: 4.4201\n",
      "Iteration 493/1072, Loss: 4.9214\n",
      "Iteration 494/1072, Loss: 4.9769\n",
      "Iteration 495/1072, Loss: 4.5683\n",
      "Iteration 496/1072, Loss: 4.6686\n",
      "Iteration 497/1072, Loss: 4.6039\n",
      "Iteration 498/1072, Loss: 4.6087\n",
      "Iteration 499/1072, Loss: 4.4620\n",
      "Iteration 500/1072, Loss: 4.2485\n",
      "Iteration 501/1072, Loss: 4.7634\n",
      "Iteration 502/1072, Loss: 5.4759\n",
      "Iteration 503/1072, Loss: 4.6746\n",
      "Iteration 504/1072, Loss: 4.7189\n",
      "Iteration 505/1072, Loss: 4.8187\n",
      "Iteration 506/1072, Loss: 4.7056\n",
      "Iteration 507/1072, Loss: 4.1231\n",
      "Iteration 508/1072, Loss: 4.5050\n",
      "Iteration 509/1072, Loss: 4.9986\n",
      "Iteration 510/1072, Loss: 4.3687\n",
      "Iteration 511/1072, Loss: 4.0075\n",
      "Iteration 512/1072, Loss: 4.9470\n",
      "Iteration 513/1072, Loss: 4.7502\n",
      "Iteration 514/1072, Loss: 4.7310\n",
      "Iteration 515/1072, Loss: 4.6693\n",
      "Iteration 516/1072, Loss: 4.7421\n",
      "Iteration 517/1072, Loss: 5.8114\n",
      "Iteration 518/1072, Loss: 5.6567\n",
      "Iteration 519/1072, Loss: 4.9703\n",
      "Iteration 520/1072, Loss: 4.8773\n",
      "Iteration 521/1072, Loss: 4.8741\n",
      "Iteration 522/1072, Loss: 4.9748\n",
      "Iteration 523/1072, Loss: 4.6250\n",
      "Iteration 524/1072, Loss: 4.9984\n",
      "Iteration 525/1072, Loss: 4.9315\n",
      "Iteration 526/1072, Loss: 4.7543\n",
      "Iteration 527/1072, Loss: 4.7985\n",
      "Iteration 528/1072, Loss: 4.5805\n",
      "Iteration 529/1072, Loss: 4.8952\n",
      "Iteration 530/1072, Loss: 5.2125\n",
      "Iteration 531/1072, Loss: 5.1730\n",
      "Iteration 532/1072, Loss: 4.8324\n",
      "Iteration 533/1072, Loss: 4.7908\n",
      "Iteration 534/1072, Loss: 4.7276\n",
      "Iteration 535/1072, Loss: 4.9376\n",
      "Iteration 536/1072, Loss: 4.8657\n",
      "Iteration 537/1072, Loss: 4.7155\n",
      "Iteration 538/1072, Loss: 5.1262\n",
      "Iteration 539/1072, Loss: 5.1575\n",
      "Iteration 540/1072, Loss: 4.6475\n",
      "Iteration 541/1072, Loss: 4.6645\n",
      "Iteration 542/1072, Loss: 5.0808\n",
      "Iteration 543/1072, Loss: 5.1420\n",
      "Iteration 544/1072, Loss: 5.3308\n",
      "Iteration 545/1072, Loss: 4.9623\n",
      "Iteration 546/1072, Loss: 4.4805\n",
      "Iteration 547/1072, Loss: 3.8745\n",
      "Iteration 548/1072, Loss: 4.8110\n",
      "Iteration 549/1072, Loss: 5.4747\n",
      "Iteration 550/1072, Loss: 4.6085\n",
      "Iteration 551/1072, Loss: 4.2682\n",
      "Iteration 552/1072, Loss: 4.7389\n",
      "Iteration 553/1072, Loss: 4.6585\n",
      "Iteration 554/1072, Loss: 4.4185\n",
      "Iteration 555/1072, Loss: 4.4687\n",
      "Iteration 556/1072, Loss: 4.6196\n",
      "Iteration 557/1072, Loss: 4.6788\n",
      "Iteration 558/1072, Loss: 4.4188\n",
      "Iteration 559/1072, Loss: 4.5596\n",
      "Iteration 560/1072, Loss: 4.5509\n",
      "Iteration 561/1072, Loss: 5.0849\n",
      "Iteration 562/1072, Loss: 4.8644\n",
      "Iteration 563/1072, Loss: 4.7409\n",
      "Iteration 564/1072, Loss: 4.5121\n",
      "Iteration 565/1072, Loss: 4.4785\n",
      "Iteration 566/1072, Loss: 5.1052\n",
      "Iteration 567/1072, Loss: 4.4880\n",
      "Iteration 568/1072, Loss: 4.7672\n",
      "Iteration 569/1072, Loss: 4.7807\n",
      "Iteration 570/1072, Loss: 4.6313\n",
      "Iteration 571/1072, Loss: 5.0288\n",
      "Iteration 572/1072, Loss: 4.7066\n",
      "Iteration 573/1072, Loss: 4.7898\n",
      "Iteration 574/1072, Loss: 4.6724\n",
      "Iteration 575/1072, Loss: 5.1752\n",
      "Iteration 576/1072, Loss: 4.3584\n",
      "Iteration 577/1072, Loss: 5.4553\n",
      "Iteration 578/1072, Loss: 4.9730\n",
      "Iteration 579/1072, Loss: 4.3981\n",
      "Iteration 580/1072, Loss: 4.7636\n",
      "Iteration 581/1072, Loss: 4.9248\n",
      "Iteration 582/1072, Loss: 4.5933\n",
      "Iteration 583/1072, Loss: 4.6435\n",
      "Iteration 584/1072, Loss: 5.1513\n",
      "Iteration 585/1072, Loss: 4.1366\n",
      "Iteration 586/1072, Loss: 4.3409\n",
      "Iteration 587/1072, Loss: 5.1357\n",
      "Iteration 588/1072, Loss: 4.7292\n",
      "Iteration 589/1072, Loss: 4.7609\n",
      "Iteration 590/1072, Loss: 4.9884\n",
      "Iteration 591/1072, Loss: 5.1736\n",
      "Iteration 592/1072, Loss: 4.5959\n",
      "Iteration 593/1072, Loss: 4.6867\n",
      "Iteration 594/1072, Loss: 4.4838\n",
      "Iteration 595/1072, Loss: 5.0902\n",
      "Iteration 596/1072, Loss: 4.4599\n",
      "Iteration 597/1072, Loss: 4.8613\n",
      "Iteration 598/1072, Loss: 4.6265\n",
      "Iteration 599/1072, Loss: 5.2373\n",
      "Iteration 600/1072, Loss: 4.2830\n",
      "Iteration 601/1072, Loss: 4.8018\n",
      "Iteration 602/1072, Loss: 4.4655\n",
      "Iteration 603/1072, Loss: 4.9227\n",
      "Iteration 604/1072, Loss: 4.2312\n",
      "Iteration 605/1072, Loss: 4.7977\n",
      "Iteration 606/1072, Loss: 4.6526\n",
      "Iteration 607/1072, Loss: 4.5916\n",
      "Iteration 608/1072, Loss: 4.5655\n",
      "Iteration 609/1072, Loss: 3.9418\n",
      "Iteration 610/1072, Loss: 4.4391\n",
      "Iteration 611/1072, Loss: 4.3781\n",
      "Iteration 612/1072, Loss: 4.8000\n",
      "Iteration 613/1072, Loss: 4.8617\n",
      "Iteration 614/1072, Loss: 4.0313\n",
      "Iteration 615/1072, Loss: 4.7372\n",
      "Iteration 616/1072, Loss: 4.7051\n",
      "Iteration 617/1072, Loss: 4.7553\n",
      "Iteration 618/1072, Loss: 4.4240\n",
      "Iteration 619/1072, Loss: 5.2407\n",
      "Iteration 620/1072, Loss: 4.3108\n",
      "Iteration 621/1072, Loss: 4.7100\n",
      "Iteration 622/1072, Loss: 4.2936\n",
      "Iteration 623/1072, Loss: 4.7484\n",
      "Iteration 624/1072, Loss: 4.7638\n",
      "Iteration 625/1072, Loss: 4.2757\n",
      "Iteration 626/1072, Loss: 4.8943\n",
      "Iteration 627/1072, Loss: 4.7852\n",
      "Iteration 628/1072, Loss: 4.7593\n",
      "Iteration 629/1072, Loss: 4.1579\n",
      "Iteration 630/1072, Loss: 4.6550\n",
      "Iteration 631/1072, Loss: 5.2487\n",
      "Iteration 632/1072, Loss: 5.0230\n",
      "Iteration 633/1072, Loss: 4.6649\n",
      "Iteration 634/1072, Loss: 4.2972\n",
      "Iteration 635/1072, Loss: 4.3621\n",
      "Iteration 636/1072, Loss: 4.6886\n",
      "Iteration 637/1072, Loss: 5.0823\n",
      "Iteration 638/1072, Loss: 4.4430\n",
      "Iteration 639/1072, Loss: 4.2838\n",
      "Iteration 640/1072, Loss: 4.7950\n",
      "Iteration 641/1072, Loss: 4.4289\n",
      "Iteration 642/1072, Loss: 4.6019\n",
      "Iteration 643/1072, Loss: 4.7223\n",
      "Iteration 644/1072, Loss: 5.2389\n",
      "Iteration 645/1072, Loss: 4.0533\n",
      "Iteration 646/1072, Loss: 4.7612\n",
      "Iteration 647/1072, Loss: 5.1617\n",
      "Iteration 648/1072, Loss: 4.9158\n",
      "Iteration 649/1072, Loss: 4.0952\n",
      "Iteration 650/1072, Loss: 5.0133\n",
      "Iteration 651/1072, Loss: 4.3607\n",
      "Iteration 652/1072, Loss: 4.6244\n",
      "Iteration 653/1072, Loss: 4.4529\n",
      "Iteration 654/1072, Loss: 5.2255\n",
      "Iteration 655/1072, Loss: 4.6200\n",
      "Iteration 656/1072, Loss: 4.8856\n",
      "Iteration 657/1072, Loss: 4.7410\n",
      "Iteration 658/1072, Loss: 4.8376\n",
      "Iteration 659/1072, Loss: 4.6228\n",
      "Iteration 660/1072, Loss: 5.0455\n",
      "Iteration 661/1072, Loss: 5.1996\n",
      "Iteration 662/1072, Loss: 5.0036\n",
      "Iteration 663/1072, Loss: 4.2736\n",
      "Iteration 664/1072, Loss: 4.4677\n",
      "Iteration 665/1072, Loss: 4.6205\n",
      "Iteration 666/1072, Loss: 4.7803\n",
      "Iteration 667/1072, Loss: 5.0958\n",
      "Iteration 668/1072, Loss: 4.8014\n",
      "Iteration 669/1072, Loss: 4.2901\n",
      "Iteration 670/1072, Loss: 4.3269\n",
      "Iteration 671/1072, Loss: 4.7611\n",
      "Iteration 672/1072, Loss: 4.2995\n",
      "Iteration 673/1072, Loss: 4.9096\n",
      "Iteration 674/1072, Loss: 4.6230\n",
      "Iteration 675/1072, Loss: 4.7400\n",
      "Iteration 676/1072, Loss: 4.6462\n",
      "Iteration 677/1072, Loss: 4.9908\n",
      "Iteration 678/1072, Loss: 4.5733\n",
      "Iteration 679/1072, Loss: 4.5442\n",
      "Iteration 680/1072, Loss: 4.5754\n",
      "Iteration 681/1072, Loss: 4.7437\n",
      "Iteration 682/1072, Loss: 4.7065\n",
      "Iteration 683/1072, Loss: 4.8708\n",
      "Iteration 684/1072, Loss: 4.7240\n",
      "Iteration 685/1072, Loss: 4.6458\n",
      "Iteration 686/1072, Loss: 3.9974\n",
      "Iteration 687/1072, Loss: 4.4823\n",
      "Iteration 688/1072, Loss: 4.3136\n",
      "Iteration 689/1072, Loss: 4.4031\n",
      "Iteration 690/1072, Loss: 4.7319\n",
      "Iteration 691/1072, Loss: 4.5064\n",
      "Iteration 692/1072, Loss: 4.7640\n",
      "Iteration 693/1072, Loss: 4.4432\n",
      "Iteration 694/1072, Loss: 4.6858\n",
      "Iteration 695/1072, Loss: 4.5386\n",
      "Iteration 696/1072, Loss: 4.5653\n",
      "Iteration 697/1072, Loss: 4.7001\n",
      "Iteration 698/1072, Loss: 4.7996\n",
      "Iteration 699/1072, Loss: 4.6613\n",
      "Iteration 700/1072, Loss: 4.5988\n",
      "Iteration 701/1072, Loss: 5.2584\n",
      "Iteration 702/1072, Loss: 4.5888\n",
      "Iteration 703/1072, Loss: 4.9178\n",
      "Iteration 704/1072, Loss: 4.5827\n",
      "Iteration 705/1072, Loss: 4.5899\n",
      "Iteration 706/1072, Loss: 4.1968\n",
      "Iteration 707/1072, Loss: 4.8337\n",
      "Iteration 708/1072, Loss: 4.8097\n",
      "Iteration 709/1072, Loss: 4.6379\n",
      "Iteration 710/1072, Loss: 4.4772\n",
      "Iteration 711/1072, Loss: 4.4215\n",
      "Iteration 712/1072, Loss: 5.0942\n",
      "Iteration 713/1072, Loss: 4.0361\n",
      "Iteration 714/1072, Loss: 4.7399\n",
      "Iteration 715/1072, Loss: 4.2870\n",
      "Iteration 716/1072, Loss: 4.9032\n",
      "Iteration 717/1072, Loss: 4.4698\n",
      "Iteration 718/1072, Loss: 4.6011\n",
      "Iteration 719/1072, Loss: 4.4733\n",
      "Iteration 720/1072, Loss: 4.8651\n",
      "Iteration 721/1072, Loss: 4.2828\n",
      "Iteration 722/1072, Loss: 3.8803\n",
      "Iteration 723/1072, Loss: 4.8602\n",
      "Iteration 724/1072, Loss: 4.5664\n",
      "Iteration 725/1072, Loss: 4.7346\n",
      "Iteration 726/1072, Loss: 4.7805\n",
      "Iteration 727/1072, Loss: 4.9291\n",
      "Iteration 728/1072, Loss: 4.5510\n",
      "Iteration 729/1072, Loss: 4.4738\n",
      "Iteration 730/1072, Loss: 4.6271\n",
      "Iteration 731/1072, Loss: 4.7463\n",
      "Iteration 732/1072, Loss: 4.6763\n",
      "Iteration 733/1072, Loss: 4.5255\n",
      "Iteration 734/1072, Loss: 5.2999\n",
      "Iteration 735/1072, Loss: 4.5329\n",
      "Iteration 736/1072, Loss: 4.4585\n",
      "Iteration 737/1072, Loss: 5.0138\n",
      "Iteration 738/1072, Loss: 4.5282\n",
      "Iteration 739/1072, Loss: 4.9426\n",
      "Iteration 740/1072, Loss: 4.6495\n",
      "Iteration 741/1072, Loss: 4.4402\n",
      "Iteration 742/1072, Loss: 5.1274\n",
      "Iteration 743/1072, Loss: 5.0502\n",
      "Iteration 744/1072, Loss: 4.6893\n",
      "Iteration 745/1072, Loss: 5.0078\n",
      "Iteration 746/1072, Loss: 4.5179\n",
      "Iteration 747/1072, Loss: 4.9609\n",
      "Iteration 748/1072, Loss: 4.3862\n",
      "Iteration 749/1072, Loss: 4.4275\n",
      "Iteration 750/1072, Loss: 4.8580\n",
      "Iteration 751/1072, Loss: 4.8581\n",
      "Iteration 752/1072, Loss: 5.2027\n",
      "Iteration 753/1072, Loss: 4.9898\n",
      "Iteration 754/1072, Loss: 4.7179\n",
      "Iteration 755/1072, Loss: 4.9206\n",
      "Iteration 756/1072, Loss: 4.5136\n",
      "Iteration 757/1072, Loss: 4.6768\n",
      "Iteration 758/1072, Loss: 5.2866\n",
      "Iteration 759/1072, Loss: 4.7378\n",
      "Iteration 760/1072, Loss: 5.1354\n",
      "Iteration 761/1072, Loss: 4.3677\n",
      "Iteration 762/1072, Loss: 4.7690\n",
      "Iteration 763/1072, Loss: 4.7193\n",
      "Iteration 764/1072, Loss: 5.1046\n",
      "Iteration 765/1072, Loss: 4.4296\n",
      "Iteration 766/1072, Loss: 4.9321\n",
      "Iteration 767/1072, Loss: 4.5348\n",
      "Iteration 768/1072, Loss: 4.8925\n",
      "Iteration 769/1072, Loss: 4.8280\n",
      "Iteration 770/1072, Loss: 4.4597\n",
      "Iteration 771/1072, Loss: 4.8879\n",
      "Iteration 772/1072, Loss: 4.8775\n",
      "Iteration 773/1072, Loss: 5.1473\n",
      "Iteration 774/1072, Loss: 4.5181\n",
      "Iteration 775/1072, Loss: 4.6653\n",
      "Iteration 776/1072, Loss: 4.4555\n",
      "Iteration 777/1072, Loss: 4.2343\n",
      "Iteration 778/1072, Loss: 4.6158\n",
      "Iteration 779/1072, Loss: 4.4232\n",
      "Iteration 780/1072, Loss: 4.5218\n",
      "Iteration 781/1072, Loss: 4.4192\n",
      "Iteration 782/1072, Loss: 4.7815\n",
      "Iteration 783/1072, Loss: 4.8481\n",
      "Iteration 784/1072, Loss: 4.8678\n",
      "Iteration 785/1072, Loss: 4.4129\n",
      "Iteration 786/1072, Loss: 4.5613\n",
      "Iteration 787/1072, Loss: 4.6161\n",
      "Iteration 788/1072, Loss: 4.6102\n",
      "Iteration 789/1072, Loss: 5.1127\n",
      "Iteration 790/1072, Loss: 4.7346\n",
      "Iteration 791/1072, Loss: 4.0406\n",
      "Iteration 792/1072, Loss: 5.0610\n",
      "Iteration 793/1072, Loss: 5.4208\n",
      "Iteration 794/1072, Loss: 4.8828\n",
      "Iteration 795/1072, Loss: 5.8822\n",
      "Iteration 796/1072, Loss: 4.7072\n",
      "Iteration 797/1072, Loss: 4.3489\n",
      "Iteration 798/1072, Loss: 4.2942\n",
      "Iteration 799/1072, Loss: 4.2359\n",
      "Iteration 800/1072, Loss: 4.4472\n",
      "Iteration 801/1072, Loss: 4.5316\n",
      "Iteration 802/1072, Loss: 5.1877\n",
      "Iteration 803/1072, Loss: 4.4803\n",
      "Iteration 804/1072, Loss: 4.7208\n",
      "Iteration 805/1072, Loss: 4.5903\n",
      "Iteration 806/1072, Loss: 4.8163\n",
      "Iteration 807/1072, Loss: 4.9202\n",
      "Iteration 808/1072, Loss: 4.9314\n",
      "Iteration 809/1072, Loss: 4.7139\n",
      "Iteration 810/1072, Loss: 4.4617\n",
      "Iteration 811/1072, Loss: 5.0185\n",
      "Iteration 812/1072, Loss: 4.5793\n",
      "Iteration 813/1072, Loss: 4.7908\n",
      "Iteration 814/1072, Loss: 4.7777\n",
      "Iteration 815/1072, Loss: 4.5083\n",
      "Iteration 816/1072, Loss: 4.4945\n",
      "Iteration 817/1072, Loss: 3.9042\n",
      "Iteration 818/1072, Loss: 4.7113\n",
      "Iteration 819/1072, Loss: 4.6930\n",
      "Iteration 820/1072, Loss: 4.5889\n",
      "Iteration 821/1072, Loss: 4.7250\n",
      "Iteration 822/1072, Loss: 4.4406\n",
      "Iteration 823/1072, Loss: 4.5788\n",
      "Iteration 824/1072, Loss: 4.5502\n",
      "Iteration 825/1072, Loss: 4.7735\n",
      "Iteration 826/1072, Loss: 4.5649\n",
      "Iteration 827/1072, Loss: 4.8153\n",
      "Iteration 828/1072, Loss: 4.8378\n",
      "Iteration 829/1072, Loss: 4.4840\n",
      "Iteration 830/1072, Loss: 4.7913\n",
      "Iteration 831/1072, Loss: 4.1475\n",
      "Iteration 832/1072, Loss: 4.3643\n",
      "Iteration 833/1072, Loss: 4.6267\n",
      "Iteration 834/1072, Loss: 4.7205\n",
      "Iteration 835/1072, Loss: 4.5865\n",
      "Iteration 836/1072, Loss: 4.8387\n",
      "Iteration 837/1072, Loss: 5.1639\n",
      "Iteration 838/1072, Loss: 4.4844\n",
      "Iteration 839/1072, Loss: 5.0802\n",
      "Iteration 840/1072, Loss: 4.3939\n",
      "Iteration 841/1072, Loss: 4.6284\n",
      "Iteration 842/1072, Loss: 4.4960\n",
      "Iteration 843/1072, Loss: 3.8308\n",
      "Iteration 844/1072, Loss: 5.0818\n",
      "Iteration 845/1072, Loss: 4.3921\n",
      "Iteration 846/1072, Loss: 3.8434\n",
      "Iteration 847/1072, Loss: 4.2964\n",
      "Iteration 848/1072, Loss: 4.7334\n",
      "Iteration 849/1072, Loss: 4.1730\n",
      "Iteration 850/1072, Loss: 4.6814\n",
      "Iteration 851/1072, Loss: 3.9520\n",
      "Iteration 852/1072, Loss: 4.5148\n",
      "Iteration 853/1072, Loss: 4.8961\n",
      "Iteration 854/1072, Loss: 5.0950\n",
      "Iteration 855/1072, Loss: 4.2855\n",
      "Iteration 856/1072, Loss: 5.0301\n",
      "Iteration 857/1072, Loss: 4.0866\n",
      "Iteration 858/1072, Loss: 4.8164\n",
      "Iteration 859/1072, Loss: 4.9486\n",
      "Iteration 860/1072, Loss: 4.2222\n",
      "Iteration 861/1072, Loss: 4.8207\n",
      "Iteration 862/1072, Loss: 4.3762\n",
      "Iteration 863/1072, Loss: 4.3382\n",
      "Iteration 864/1072, Loss: 4.9321\n",
      "Iteration 865/1072, Loss: 5.0627\n",
      "Iteration 866/1072, Loss: 4.2120\n",
      "Iteration 867/1072, Loss: 4.5341\n",
      "Iteration 868/1072, Loss: 4.4573\n",
      "Iteration 869/1072, Loss: 4.6377\n",
      "Iteration 870/1072, Loss: 4.8331\n",
      "Iteration 871/1072, Loss: 4.6961\n",
      "Iteration 872/1072, Loss: 4.7449\n",
      "Iteration 873/1072, Loss: 4.4203\n",
      "Iteration 874/1072, Loss: 5.0673\n",
      "Iteration 875/1072, Loss: 4.1421\n",
      "Iteration 876/1072, Loss: 4.3342\n",
      "Iteration 877/1072, Loss: 4.7682\n",
      "Iteration 878/1072, Loss: 4.2279\n",
      "Iteration 879/1072, Loss: 4.5864\n",
      "Iteration 880/1072, Loss: 4.5782\n",
      "Iteration 881/1072, Loss: 4.4287\n",
      "Iteration 882/1072, Loss: 4.6811\n",
      "Iteration 883/1072, Loss: 4.4763\n",
      "Iteration 884/1072, Loss: 4.5272\n",
      "Iteration 885/1072, Loss: 4.9468\n",
      "Iteration 886/1072, Loss: 4.8096\n",
      "Iteration 887/1072, Loss: 4.7766\n",
      "Iteration 888/1072, Loss: 4.8831\n",
      "Iteration 889/1072, Loss: 4.5763\n",
      "Iteration 890/1072, Loss: 4.8071\n",
      "Iteration 891/1072, Loss: 4.5644\n",
      "Iteration 892/1072, Loss: 4.8735\n",
      "Iteration 893/1072, Loss: 4.3686\n",
      "Iteration 894/1072, Loss: 4.8576\n",
      "Iteration 895/1072, Loss: 4.7945\n",
      "Iteration 896/1072, Loss: 4.3065\n",
      "Iteration 897/1072, Loss: 4.6502\n",
      "Iteration 898/1072, Loss: 4.4792\n",
      "Iteration 899/1072, Loss: 4.1519\n",
      "Iteration 900/1072, Loss: 4.6608\n",
      "Iteration 901/1072, Loss: 4.4305\n",
      "Iteration 902/1072, Loss: 4.0061\n",
      "Iteration 903/1072, Loss: 4.2843\n",
      "Iteration 904/1072, Loss: 4.0401\n",
      "Iteration 905/1072, Loss: 4.3329\n",
      "Iteration 906/1072, Loss: 4.9395\n",
      "Iteration 907/1072, Loss: 4.5377\n",
      "Iteration 908/1072, Loss: 4.8274\n",
      "Iteration 909/1072, Loss: 4.7387\n",
      "Iteration 910/1072, Loss: 4.8185\n",
      "Iteration 911/1072, Loss: 4.2606\n",
      "Iteration 912/1072, Loss: 4.2398\n",
      "Iteration 913/1072, Loss: 4.5564\n",
      "Iteration 914/1072, Loss: 4.7137\n",
      "Iteration 915/1072, Loss: 4.2800\n",
      "Iteration 916/1072, Loss: 4.6129\n",
      "Iteration 917/1072, Loss: 4.2975\n",
      "Iteration 918/1072, Loss: 4.7168\n",
      "Iteration 919/1072, Loss: 4.8839\n",
      "Iteration 920/1072, Loss: 4.2765\n",
      "Iteration 921/1072, Loss: 4.8597\n",
      "Iteration 922/1072, Loss: 4.0186\n",
      "Iteration 923/1072, Loss: 4.7486\n",
      "Iteration 924/1072, Loss: 4.8550\n",
      "Iteration 925/1072, Loss: 4.6912\n",
      "Iteration 926/1072, Loss: 4.6866\n",
      "Iteration 927/1072, Loss: 5.0000\n",
      "Iteration 928/1072, Loss: 4.8620\n",
      "Iteration 929/1072, Loss: 4.6879\n",
      "Iteration 930/1072, Loss: 4.0602\n",
      "Iteration 931/1072, Loss: 4.5244\n",
      "Iteration 932/1072, Loss: 4.8150\n",
      "Iteration 933/1072, Loss: 4.6663\n",
      "Iteration 934/1072, Loss: 4.8313\n",
      "Iteration 935/1072, Loss: 4.3860\n",
      "Iteration 936/1072, Loss: 4.5803\n",
      "Iteration 937/1072, Loss: 5.1339\n",
      "Iteration 938/1072, Loss: 4.3850\n",
      "Iteration 939/1072, Loss: 4.2989\n",
      "Iteration 940/1072, Loss: 4.6633\n",
      "Iteration 941/1072, Loss: 5.0454\n",
      "Iteration 942/1072, Loss: 4.3139\n",
      "Iteration 943/1072, Loss: 5.0003\n",
      "Iteration 944/1072, Loss: 4.7996\n",
      "Iteration 945/1072, Loss: 4.6649\n",
      "Iteration 946/1072, Loss: 4.8033\n",
      "Iteration 947/1072, Loss: 4.4930\n",
      "Iteration 948/1072, Loss: 4.6749\n",
      "Iteration 949/1072, Loss: 5.1860\n",
      "Iteration 950/1072, Loss: 5.0021\n",
      "Iteration 951/1072, Loss: 4.3112\n",
      "Iteration 952/1072, Loss: 4.2395\n",
      "Iteration 953/1072, Loss: 4.8477\n",
      "Iteration 954/1072, Loss: 4.6885\n",
      "Iteration 955/1072, Loss: 5.1107\n",
      "Iteration 956/1072, Loss: 4.6138\n",
      "Iteration 957/1072, Loss: 4.4612\n",
      "Iteration 958/1072, Loss: 4.9140\n",
      "Iteration 959/1072, Loss: 4.8903\n",
      "Iteration 960/1072, Loss: 4.4018\n",
      "Iteration 961/1072, Loss: 4.8047\n",
      "Iteration 962/1072, Loss: 4.6897\n",
      "Iteration 963/1072, Loss: 4.2013\n",
      "Iteration 964/1072, Loss: 4.6090\n",
      "Iteration 965/1072, Loss: 5.0181\n",
      "Iteration 966/1072, Loss: 4.8379\n",
      "Iteration 967/1072, Loss: 4.5653\n",
      "Iteration 968/1072, Loss: 4.3880\n",
      "Iteration 969/1072, Loss: 4.8119\n",
      "Iteration 970/1072, Loss: 4.5889\n",
      "Iteration 971/1072, Loss: 4.8197\n",
      "Iteration 972/1072, Loss: 5.3065\n",
      "Iteration 973/1072, Loss: 3.9989\n",
      "Iteration 974/1072, Loss: 4.4488\n",
      "Iteration 975/1072, Loss: 4.9937\n",
      "Iteration 976/1072, Loss: 3.8618\n",
      "Iteration 977/1072, Loss: 4.5051\n",
      "Iteration 978/1072, Loss: 4.5676\n",
      "Iteration 979/1072, Loss: 4.1652\n",
      "Iteration 980/1072, Loss: 4.4454\n",
      "Iteration 981/1072, Loss: 5.0415\n",
      "Iteration 982/1072, Loss: 4.5220\n",
      "Iteration 983/1072, Loss: 4.4838\n",
      "Iteration 984/1072, Loss: 4.9239\n",
      "Iteration 985/1072, Loss: 4.6561\n",
      "Iteration 986/1072, Loss: 4.2880\n",
      "Iteration 987/1072, Loss: 4.8181\n",
      "Iteration 988/1072, Loss: 4.7835\n",
      "Iteration 989/1072, Loss: 4.4384\n",
      "Iteration 991/1072, Loss: 4.9718\n",
      "Iteration 32/1072, Loss: 4.8353\n",
      "Iteration 121/1072, Loss: 4.3347\n",
      "Iteration 122/1072, Loss: 4.4890\n",
      "Iteration 123/1072, Loss: 4.4949\n",
      "Iteration 124/1072, Loss: 4.1253\n",
      "Iteration 125/1072, Loss: 3.6135\n",
      "Iteration 126/1072, Loss: 4.0421\n",
      "Iteration 127/1072, Loss: 3.9105\n",
      "Iteration 128/1072, Loss: 4.4237\n",
      "Iteration 129/1072, Loss: 4.4768\n",
      "Iteration 130/1072, Loss: 4.2417\n",
      "Iteration 131/1072, Loss: 4.4664\n",
      "Iteration 132/1072, Loss: 4.0681\n",
      "Iteration 133/1072, Loss: 4.4064\n",
      "Iteration 134/1072, Loss: 4.0769\n",
      "Iteration 135/1072, Loss: 4.5154\n",
      "Iteration 136/1072, Loss: 4.2075\n",
      "Iteration 137/1072, Loss: 3.8837\n",
      "Iteration 138/1072, Loss: 4.1810\n",
      "Iteration 139/1072, Loss: 4.9086\n",
      "Iteration 140/1072, Loss: 4.5882\n",
      "Iteration 141/1072, Loss: 3.9160\n",
      "Iteration 142/1072, Loss: 3.8415\n",
      "Iteration 143/1072, Loss: 3.9967\n",
      "Iteration 144/1072, Loss: 4.4127\n",
      "Iteration 145/1072, Loss: 4.1611\n",
      "Iteration 146/1072, Loss: 3.8064\n",
      "Iteration 147/1072, Loss: 4.6365\n",
      "Iteration 148/1072, Loss: 4.5154\n",
      "Iteration 149/1072, Loss: 4.0447\n",
      "Iteration 150/1072, Loss: 4.5124\n",
      "Iteration 151/1072, Loss: 4.4718\n",
      "Iteration 152/1072, Loss: 4.1885\n",
      "Iteration 153/1072, Loss: 4.7977\n",
      "Iteration 154/1072, Loss: 3.9970\n",
      "Iteration 155/1072, Loss: 4.0906\n",
      "Iteration 156/1072, Loss: 4.9608\n",
      "Iteration 157/1072, Loss: 4.1784\n",
      "Iteration 158/1072, Loss: 4.1547\n",
      "Iteration 159/1072, Loss: 4.5543\n",
      "Iteration 160/1072, Loss: 4.4517\n",
      "Iteration 161/1072, Loss: 4.2526\n",
      "Iteration 162/1072, Loss: 4.5559\n",
      "Iteration 163/1072, Loss: 3.6540\n",
      "Iteration 164/1072, Loss: 5.1942\n",
      "Iteration 165/1072, Loss: 4.5248\n",
      "Iteration 166/1072, Loss: 3.9280\n",
      "Iteration 167/1072, Loss: 4.0233\n",
      "Iteration 168/1072, Loss: 4.4461\n",
      "Iteration 169/1072, Loss: 4.1421\n",
      "Iteration 170/1072, Loss: 4.7527\n",
      "Iteration 171/1072, Loss: 4.0477\n",
      "Iteration 172/1072, Loss: 3.8750\n",
      "Iteration 173/1072, Loss: 4.1639\n",
      "Iteration 174/1072, Loss: 4.2802\n",
      "Iteration 175/1072, Loss: 3.9400\n",
      "Iteration 176/1072, Loss: 3.8786\n",
      "Iteration 177/1072, Loss: 4.7837\n",
      "Iteration 178/1072, Loss: 4.1606\n",
      "Iteration 179/1072, Loss: 3.8233\n",
      "Iteration 180/1072, Loss: 4.5747\n",
      "Iteration 181/1072, Loss: 4.9067\n",
      "Iteration 182/1072, Loss: 4.3663\n",
      "Iteration 183/1072, Loss: 4.3411\n",
      "Iteration 184/1072, Loss: 4.3575\n",
      "Iteration 185/1072, Loss: 3.8915\n",
      "Iteration 186/1072, Loss: 4.3219\n",
      "Iteration 187/1072, Loss: 3.8347\n",
      "Iteration 188/1072, Loss: 4.0899\n",
      "Iteration 189/1072, Loss: 4.4681\n",
      "Iteration 190/1072, Loss: 3.9085\n",
      "Iteration 191/1072, Loss: 4.5304\n",
      "Iteration 192/1072, Loss: 4.5228\n",
      "Iteration 193/1072, Loss: 4.2070\n",
      "Iteration 194/1072, Loss: 3.9417\n",
      "Iteration 195/1072, Loss: 4.5299\n",
      "Iteration 196/1072, Loss: 3.9298\n",
      "Iteration 197/1072, Loss: 4.5175\n",
      "Iteration 198/1072, Loss: 4.1555\n",
      "Iteration 199/1072, Loss: 4.2767\n",
      "Iteration 200/1072, Loss: 4.9461\n",
      "Iteration 201/1072, Loss: 4.0232\n",
      "Iteration 202/1072, Loss: 4.0625\n",
      "Iteration 203/1072, Loss: 4.4885\n",
      "Iteration 204/1072, Loss: 4.1113\n",
      "Iteration 205/1072, Loss: 4.3578\n",
      "Iteration 206/1072, Loss: 4.0276\n",
      "Iteration 207/1072, Loss: 3.9809\n",
      "Iteration 208/1072, Loss: 4.7153\n",
      "Iteration 209/1072, Loss: 4.4890\n",
      "Iteration 210/1072, Loss: 4.5537\n",
      "Iteration 211/1072, Loss: 4.3994\n",
      "Iteration 212/1072, Loss: 4.4376\n",
      "Iteration 213/1072, Loss: 3.4780\n",
      "Iteration 214/1072, Loss: 4.4093\n",
      "Iteration 215/1072, Loss: 3.8336\n",
      "Iteration 216/1072, Loss: 4.3789\n",
      "Iteration 217/1072, Loss: 4.2324\n",
      "Iteration 218/1072, Loss: 4.5342\n",
      "Iteration 219/1072, Loss: 4.6844\n",
      "Iteration 220/1072, Loss: 3.7858\n",
      "Iteration 221/1072, Loss: 4.5402\n",
      "Iteration 222/1072, Loss: 4.1757\n",
      "Iteration 223/1072, Loss: 4.5062\n",
      "Iteration 224/1072, Loss: 4.3228\n",
      "Iteration 225/1072, Loss: 4.8144\n",
      "Iteration 226/1072, Loss: 3.9624\n",
      "Iteration 227/1072, Loss: 3.5915\n",
      "Iteration 228/1072, Loss: 3.7118\n",
      "Iteration 229/1072, Loss: 4.1285\n",
      "Iteration 230/1072, Loss: 4.3603\n",
      "Iteration 231/1072, Loss: 4.2853\n",
      "Iteration 232/1072, Loss: 3.9799\n",
      "Iteration 233/1072, Loss: 4.2332\n",
      "Iteration 234/1072, Loss: 4.0043\n",
      "Iteration 235/1072, Loss: 5.0276\n",
      "Iteration 236/1072, Loss: 4.3720\n",
      "Iteration 237/1072, Loss: 3.8478\n",
      "Iteration 238/1072, Loss: 4.1823\n",
      "Iteration 239/1072, Loss: 4.6095\n",
      "Iteration 240/1072, Loss: 4.0355\n",
      "Iteration 241/1072, Loss: 3.9627\n",
      "Iteration 242/1072, Loss: 4.2622\n",
      "Iteration 243/1072, Loss: 4.2087\n",
      "Iteration 244/1072, Loss: 4.5035\n",
      "Iteration 245/1072, Loss: 3.7396\n",
      "Iteration 246/1072, Loss: 4.5219\n",
      "Iteration 247/1072, Loss: 4.2045\n",
      "Iteration 248/1072, Loss: 4.5440\n",
      "Iteration 249/1072, Loss: 4.0090\n",
      "Iteration 250/1072, Loss: 4.0729\n",
      "Iteration 251/1072, Loss: 4.6019\n",
      "Iteration 252/1072, Loss: 4.2817\n",
      "Iteration 253/1072, Loss: 4.4578\n",
      "Iteration 254/1072, Loss: 4.2666\n",
      "Iteration 255/1072, Loss: 4.3886\n",
      "Iteration 256/1072, Loss: 4.0870\n",
      "Iteration 257/1072, Loss: 4.0489\n",
      "Iteration 258/1072, Loss: 5.0658\n",
      "Iteration 259/1072, Loss: 3.6345\n",
      "Iteration 260/1072, Loss: 4.7542\n",
      "Iteration 261/1072, Loss: 4.0558\n",
      "Iteration 262/1072, Loss: 4.0076\n",
      "Iteration 263/1072, Loss: 4.1004\n",
      "Iteration 264/1072, Loss: 3.8901\n",
      "Iteration 265/1072, Loss: 4.0163\n",
      "Iteration 266/1072, Loss: 3.9165\n",
      "Iteration 267/1072, Loss: 4.0875\n",
      "Iteration 268/1072, Loss: 4.3776\n",
      "Iteration 269/1072, Loss: 4.7196\n",
      "Iteration 270/1072, Loss: 4.2598\n",
      "Iteration 271/1072, Loss: 4.1535\n",
      "Iteration 272/1072, Loss: 4.3943\n",
      "Iteration 273/1072, Loss: 4.1489\n",
      "Iteration 274/1072, Loss: 3.9812\n",
      "Iteration 275/1072, Loss: 4.2370\n",
      "Iteration 276/1072, Loss: 4.3179\n",
      "Iteration 277/1072, Loss: 4.3266\n",
      "Iteration 278/1072, Loss: 4.2910\n",
      "Iteration 279/1072, Loss: 4.0048\n",
      "Iteration 280/1072, Loss: 4.0814\n",
      "Iteration 281/1072, Loss: 3.9218\n",
      "Iteration 282/1072, Loss: 4.0873\n",
      "Iteration 283/1072, Loss: 4.6074\n",
      "Iteration 284/1072, Loss: 4.3175\n",
      "Iteration 285/1072, Loss: 4.4496\n",
      "Iteration 286/1072, Loss: 4.3152\n",
      "Iteration 287/1072, Loss: 4.1799\n",
      "Iteration 288/1072, Loss: 4.2793\n",
      "Iteration 289/1072, Loss: 4.2243\n",
      "Iteration 290/1072, Loss: 3.9238\n",
      "Iteration 291/1072, Loss: 3.8555\n",
      "Iteration 292/1072, Loss: 4.0891\n",
      "Iteration 293/1072, Loss: 3.9513\n",
      "Iteration 294/1072, Loss: 4.5311\n",
      "Iteration 295/1072, Loss: 4.0437\n",
      "Iteration 296/1072, Loss: 4.0627\n",
      "Iteration 297/1072, Loss: 4.6988\n",
      "Iteration 298/1072, Loss: 4.1026\n",
      "Iteration 299/1072, Loss: 4.2322\n",
      "Iteration 300/1072, Loss: 4.2294\n",
      "Iteration 301/1072, Loss: 3.7603\n",
      "Iteration 302/1072, Loss: 4.2525\n",
      "Iteration 303/1072, Loss: 3.6396\n",
      "Iteration 304/1072, Loss: 4.1875\n",
      "Iteration 305/1072, Loss: 4.7977\n",
      "Iteration 306/1072, Loss: 4.9835\n",
      "Iteration 307/1072, Loss: 4.3150\n",
      "Iteration 308/1072, Loss: 4.0606\n",
      "Iteration 309/1072, Loss: 4.0509\n",
      "Iteration 310/1072, Loss: 4.0787\n",
      "Iteration 311/1072, Loss: 4.1648\n",
      "Iteration 312/1072, Loss: 3.7984\n",
      "Iteration 313/1072, Loss: 4.3545\n",
      "Iteration 314/1072, Loss: 4.5415\n",
      "Iteration 315/1072, Loss: 3.9979\n",
      "Iteration 316/1072, Loss: 4.3247\n",
      "Iteration 317/1072, Loss: 4.4044\n",
      "Iteration 318/1072, Loss: 4.3974\n",
      "Iteration 319/1072, Loss: 3.9477\n",
      "Iteration 320/1072, Loss: 4.3815\n",
      "Iteration 321/1072, Loss: 4.2176\n",
      "Iteration 322/1072, Loss: 4.6188\n",
      "Iteration 323/1072, Loss: 4.1414\n",
      "Iteration 324/1072, Loss: 3.5952\n",
      "Iteration 325/1072, Loss: 4.3240\n",
      "Iteration 326/1072, Loss: 4.2449\n",
      "Iteration 327/1072, Loss: 4.4521\n",
      "Iteration 328/1072, Loss: 4.2523\n",
      "Iteration 329/1072, Loss: 4.3563\n",
      "Iteration 330/1072, Loss: 4.4535\n",
      "Iteration 331/1072, Loss: 4.0387\n",
      "Iteration 332/1072, Loss: 4.3436\n",
      "Iteration 333/1072, Loss: 4.2786\n",
      "Iteration 334/1072, Loss: 4.3910\n",
      "Iteration 335/1072, Loss: 4.4904\n",
      "Iteration 336/1072, Loss: 4.0725\n",
      "Iteration 337/1072, Loss: 4.2395\n",
      "Iteration 338/1072, Loss: 4.9519\n",
      "Iteration 339/1072, Loss: 4.2327\n",
      "Iteration 340/1072, Loss: 3.2981\n",
      "Iteration 341/1072, Loss: 3.6408\n",
      "Iteration 342/1072, Loss: 4.6101\n",
      "Iteration 343/1072, Loss: 3.9705\n",
      "Iteration 344/1072, Loss: 4.5914\n",
      "Iteration 345/1072, Loss: 4.3879\n",
      "Iteration 346/1072, Loss: 4.2203\n",
      "Iteration 347/1072, Loss: 4.2756\n",
      "Iteration 348/1072, Loss: 4.0187\n",
      "Iteration 349/1072, Loss: 3.5445\n",
      "Iteration 350/1072, Loss: 4.0914\n",
      "Iteration 351/1072, Loss: 4.1886\n",
      "Iteration 352/1072, Loss: 4.0461\n",
      "Iteration 353/1072, Loss: 3.9392\n",
      "Iteration 354/1072, Loss: 3.6695\n",
      "Iteration 355/1072, Loss: 3.9178\n",
      "Iteration 356/1072, Loss: 4.0468\n",
      "Iteration 357/1072, Loss: 4.2322\n",
      "Iteration 358/1072, Loss: 4.4310\n",
      "Iteration 359/1072, Loss: 4.7026\n",
      "Iteration 360/1072, Loss: 4.4723\n",
      "Iteration 361/1072, Loss: 4.2707\n",
      "Iteration 362/1072, Loss: 3.2991\n",
      "Iteration 363/1072, Loss: 4.0589\n",
      "Iteration 364/1072, Loss: 4.0194\n",
      "Iteration 365/1072, Loss: 3.6991\n",
      "Iteration 366/1072, Loss: 4.2395\n",
      "Iteration 367/1072, Loss: 4.8494\n",
      "Iteration 368/1072, Loss: 4.2414\n",
      "Iteration 369/1072, Loss: 3.9988\n",
      "Iteration 370/1072, Loss: 4.2880\n",
      "Iteration 371/1072, Loss: 4.1559\n",
      "Iteration 372/1072, Loss: 4.5563\n",
      "Iteration 373/1072, Loss: 4.1002\n",
      "Iteration 374/1072, Loss: 4.4765\n",
      "Iteration 375/1072, Loss: 4.2893\n",
      "Iteration 376/1072, Loss: 4.8642\n",
      "Iteration 377/1072, Loss: 4.2188\n",
      "Iteration 378/1072, Loss: 4.1483\n",
      "Iteration 379/1072, Loss: 4.1982\n",
      "Iteration 380/1072, Loss: 4.8001\n",
      "Iteration 381/1072, Loss: 3.9112\n",
      "Iteration 382/1072, Loss: 3.6011\n",
      "Iteration 383/1072, Loss: 4.0406\n",
      "Iteration 384/1072, Loss: 4.1679\n",
      "Iteration 385/1072, Loss: 4.4023\n",
      "Iteration 386/1072, Loss: 4.0469\n",
      "Iteration 387/1072, Loss: 4.0170\n",
      "Iteration 388/1072, Loss: 4.0789\n",
      "Iteration 389/1072, Loss: 3.9244\n",
      "Iteration 390/1072, Loss: 3.7080\n",
      "Iteration 391/1072, Loss: 4.1339\n",
      "Iteration 392/1072, Loss: 3.9274\n",
      "Iteration 393/1072, Loss: 3.9536\n",
      "Iteration 394/1072, Loss: 3.8643\n",
      "Iteration 395/1072, Loss: 4.7376\n",
      "Iteration 396/1072, Loss: 3.8492\n",
      "Iteration 397/1072, Loss: 3.6992\n",
      "Iteration 398/1072, Loss: 3.8862\n",
      "Iteration 399/1072, Loss: 3.4087\n",
      "Iteration 400/1072, Loss: 4.3360\n",
      "Iteration 401/1072, Loss: 3.7751\n",
      "Iteration 402/1072, Loss: 3.9068\n",
      "Iteration 403/1072, Loss: 4.2338\n",
      "Iteration 404/1072, Loss: 4.7523\n",
      "Iteration 405/1072, Loss: 3.8510\n",
      "Iteration 406/1072, Loss: 4.6952\n",
      "Iteration 407/1072, Loss: 4.1727\n",
      "Iteration 408/1072, Loss: 4.2237\n",
      "Iteration 409/1072, Loss: 4.5161\n",
      "Iteration 410/1072, Loss: 4.2692\n",
      "Iteration 411/1072, Loss: 4.5485\n",
      "Iteration 412/1072, Loss: 4.0836\n",
      "Iteration 413/1072, Loss: 4.0388\n",
      "Iteration 414/1072, Loss: 4.8989\n",
      "Iteration 415/1072, Loss: 4.4114\n",
      "Iteration 416/1072, Loss: 4.0599\n",
      "Iteration 417/1072, Loss: 4.6261\n",
      "Iteration 418/1072, Loss: 4.1330\n",
      "Iteration 419/1072, Loss: 3.9487\n",
      "Iteration 420/1072, Loss: 3.8048\n",
      "Iteration 421/1072, Loss: 4.0339\n",
      "Iteration 422/1072, Loss: 4.1953\n",
      "Iteration 423/1072, Loss: 4.5927\n",
      "Iteration 424/1072, Loss: 3.9494\n",
      "Iteration 425/1072, Loss: 4.8222\n",
      "Iteration 426/1072, Loss: 3.9847\n",
      "Iteration 427/1072, Loss: 3.5380\n",
      "Iteration 428/1072, Loss: 4.3393\n",
      "Iteration 429/1072, Loss: 4.6957\n",
      "Iteration 430/1072, Loss: 4.0697\n",
      "Iteration 431/1072, Loss: 4.0925\n",
      "Iteration 432/1072, Loss: 3.7402\n",
      "Iteration 433/1072, Loss: 3.9699\n",
      "Iteration 434/1072, Loss: 4.1344\n",
      "Iteration 435/1072, Loss: 4.6886\n",
      "Iteration 436/1072, Loss: 3.8495\n",
      "Iteration 437/1072, Loss: 3.9798\n",
      "Iteration 438/1072, Loss: 4.9643\n",
      "Iteration 439/1072, Loss: 3.9562\n",
      "Iteration 440/1072, Loss: 4.3414\n",
      "Iteration 441/1072, Loss: 4.2304\n",
      "Iteration 442/1072, Loss: 4.6174\n",
      "Iteration 443/1072, Loss: 4.4747\n",
      "Iteration 444/1072, Loss: 4.2158\n",
      "Iteration 445/1072, Loss: 4.0176\n",
      "Iteration 446/1072, Loss: 4.7430\n",
      "Iteration 447/1072, Loss: 4.4256\n",
      "Iteration 448/1072, Loss: 4.4342\n",
      "Iteration 449/1072, Loss: 4.7429\n",
      "Iteration 450/1072, Loss: 4.2917\n",
      "Iteration 451/1072, Loss: 5.0763\n",
      "Iteration 452/1072, Loss: 4.2012\n",
      "Iteration 453/1072, Loss: 3.9233\n",
      "Iteration 454/1072, Loss: 4.1810\n",
      "Iteration 455/1072, Loss: 3.8486\n",
      "Iteration 456/1072, Loss: 4.1871\n",
      "Iteration 457/1072, Loss: 3.7868\n",
      "Iteration 458/1072, Loss: 3.9218\n",
      "Iteration 459/1072, Loss: 4.1931\n",
      "Iteration 460/1072, Loss: 4.3425\n",
      "Iteration 461/1072, Loss: 4.7348\n",
      "Iteration 462/1072, Loss: 4.6715\n",
      "Iteration 463/1072, Loss: 4.3651\n",
      "Iteration 464/1072, Loss: 3.9832\n",
      "Iteration 465/1072, Loss: 3.9757\n",
      "Iteration 466/1072, Loss: 4.3852\n",
      "Iteration 467/1072, Loss: 4.5518\n",
      "Iteration 468/1072, Loss: 3.4797\n",
      "Iteration 469/1072, Loss: 4.1262\n",
      "Iteration 470/1072, Loss: 3.8299\n",
      "Iteration 471/1072, Loss: 4.7986\n",
      "Iteration 472/1072, Loss: 4.1788\n",
      "Iteration 473/1072, Loss: 4.0198\n",
      "Iteration 474/1072, Loss: 4.4316\n",
      "Iteration 475/1072, Loss: 3.9386\n",
      "Iteration 476/1072, Loss: 3.9076\n",
      "Iteration 477/1072, Loss: 4.0657\n",
      "Iteration 478/1072, Loss: 4.0365\n",
      "Iteration 479/1072, Loss: 4.3114\n",
      "Iteration 480/1072, Loss: 4.1867\n",
      "Iteration 481/1072, Loss: 4.5303\n",
      "Iteration 482/1072, Loss: 4.0294\n",
      "Iteration 483/1072, Loss: 4.2210\n",
      "Iteration 484/1072, Loss: 4.3545\n",
      "Iteration 485/1072, Loss: 4.3403\n",
      "Iteration 486/1072, Loss: 4.4060\n",
      "Iteration 487/1072, Loss: 4.1364\n",
      "Iteration 488/1072, Loss: 4.5326\n",
      "Iteration 489/1072, Loss: 4.0912\n",
      "Iteration 490/1072, Loss: 4.1371\n",
      "Iteration 491/1072, Loss: 3.9091\n",
      "Iteration 492/1072, Loss: 4.1038\n",
      "Iteration 493/1072, Loss: 4.2515\n",
      "Iteration 494/1072, Loss: 4.2146\n",
      "Iteration 495/1072, Loss: 4.6195\n",
      "Iteration 496/1072, Loss: 4.1824\n",
      "Iteration 497/1072, Loss: 4.1875\n",
      "Iteration 498/1072, Loss: 4.2095\n",
      "Iteration 499/1072, Loss: 4.4738\n",
      "Iteration 500/1072, Loss: 4.4600\n",
      "Iteration 501/1072, Loss: 3.8389\n",
      "Iteration 502/1072, Loss: 4.4435\n",
      "Iteration 503/1072, Loss: 4.6844\n",
      "Iteration 504/1072, Loss: 4.0131\n",
      "Iteration 505/1072, Loss: 4.4086\n",
      "Iteration 506/1072, Loss: 3.9739\n",
      "Iteration 507/1072, Loss: 3.7954\n",
      "Iteration 508/1072, Loss: 4.1587\n",
      "Iteration 509/1072, Loss: 3.8310\n",
      "Iteration 510/1072, Loss: 4.1258\n",
      "Iteration 511/1072, Loss: 3.8980\n",
      "Iteration 512/1072, Loss: 4.5926\n",
      "Iteration 513/1072, Loss: 3.6183\n",
      "Iteration 514/1072, Loss: 3.5700\n",
      "Iteration 515/1072, Loss: 4.1093\n",
      "Iteration 516/1072, Loss: 4.0774\n",
      "Iteration 517/1072, Loss: 4.3713\n",
      "Iteration 518/1072, Loss: 4.2708\n",
      "Iteration 519/1072, Loss: 4.5897\n",
      "Iteration 520/1072, Loss: 4.6791\n",
      "Iteration 521/1072, Loss: 4.7905\n",
      "Iteration 522/1072, Loss: 4.1314\n",
      "Iteration 523/1072, Loss: 4.0635\n",
      "Iteration 524/1072, Loss: 4.4252\n",
      "Iteration 525/1072, Loss: 4.0126\n",
      "Iteration 526/1072, Loss: 4.2859\n",
      "Iteration 527/1072, Loss: 4.2535\n",
      "Iteration 528/1072, Loss: 3.6675\n",
      "Iteration 529/1072, Loss: 4.5585\n",
      "Iteration 530/1072, Loss: 4.4725\n",
      "Iteration 531/1072, Loss: 4.4502\n",
      "Iteration 532/1072, Loss: 4.4038\n",
      "Iteration 533/1072, Loss: 3.5491\n",
      "Iteration 534/1072, Loss: 4.2838\n",
      "Iteration 535/1072, Loss: 3.9085\n",
      "Iteration 536/1072, Loss: 4.0219\n",
      "Iteration 537/1072, Loss: 4.1625\n",
      "Iteration 538/1072, Loss: 4.3789\n",
      "Iteration 539/1072, Loss: 4.3433\n",
      "Iteration 540/1072, Loss: 3.6273\n",
      "Iteration 541/1072, Loss: 4.1628\n",
      "Iteration 542/1072, Loss: 4.1549\n",
      "Iteration 543/1072, Loss: 3.9213\n",
      "Iteration 544/1072, Loss: 4.2704\n",
      "Iteration 545/1072, Loss: 4.0675\n",
      "Iteration 546/1072, Loss: 4.7707\n",
      "Iteration 547/1072, Loss: 4.5923\n",
      "Iteration 548/1072, Loss: 4.3612\n",
      "Iteration 549/1072, Loss: 3.8883\n",
      "Iteration 550/1072, Loss: 4.1901\n",
      "Iteration 551/1072, Loss: 4.0681\n",
      "Iteration 552/1072, Loss: 4.5540\n",
      "Iteration 553/1072, Loss: 4.3406\n",
      "Iteration 554/1072, Loss: 4.1963\n",
      "Iteration 555/1072, Loss: 4.4169\n",
      "Iteration 556/1072, Loss: 4.0689\n",
      "Iteration 557/1072, Loss: 4.6411\n",
      "Iteration 558/1072, Loss: 3.9166\n",
      "Iteration 559/1072, Loss: 4.1108\n",
      "Iteration 560/1072, Loss: 4.6463\n",
      "Iteration 561/1072, Loss: 4.2469\n",
      "Iteration 562/1072, Loss: 3.8348\n",
      "Iteration 563/1072, Loss: 3.9840\n",
      "Iteration 564/1072, Loss: 4.0564\n",
      "Iteration 565/1072, Loss: 4.5529\n",
      "Iteration 566/1072, Loss: 4.3395\n",
      "Iteration 567/1072, Loss: 4.5423\n",
      "Iteration 568/1072, Loss: 3.9987\n",
      "Iteration 569/1072, Loss: 4.5616\n",
      "Iteration 570/1072, Loss: 4.3231\n",
      "Iteration 571/1072, Loss: 4.3484\n",
      "Iteration 572/1072, Loss: 4.1802\n",
      "Iteration 573/1072, Loss: 4.1818\n",
      "Iteration 574/1072, Loss: 3.6895\n",
      "Iteration 575/1072, Loss: 4.5428\n",
      "Iteration 576/1072, Loss: 4.7072\n",
      "Iteration 577/1072, Loss: 4.1296\n",
      "Iteration 578/1072, Loss: 4.9445\n",
      "Iteration 579/1072, Loss: 4.1211\n",
      "Iteration 580/1072, Loss: 4.2538\n",
      "Iteration 581/1072, Loss: 4.7927\n",
      "Iteration 582/1072, Loss: 4.3950\n",
      "Iteration 583/1072, Loss: 4.6570\n",
      "Iteration 584/1072, Loss: 4.3585\n",
      "Iteration 585/1072, Loss: 4.3115\n",
      "Iteration 586/1072, Loss: 4.1569\n",
      "Iteration 587/1072, Loss: 4.0886\n",
      "Iteration 588/1072, Loss: 3.9247\n",
      "Iteration 589/1072, Loss: 4.1662\n",
      "Iteration 590/1072, Loss: 3.8905\n",
      "Iteration 591/1072, Loss: 4.2411\n",
      "Iteration 592/1072, Loss: 4.7411\n",
      "Iteration 593/1072, Loss: 4.0877\n",
      "Iteration 594/1072, Loss: 4.1928\n",
      "Iteration 595/1072, Loss: 4.3985\n",
      "Iteration 596/1072, Loss: 4.2734\n",
      "Iteration 597/1072, Loss: 4.1880\n",
      "Iteration 598/1072, Loss: 4.4178\n",
      "Iteration 599/1072, Loss: 4.2584\n",
      "Iteration 600/1072, Loss: 4.4486\n",
      "Iteration 601/1072, Loss: 4.3930\n",
      "Iteration 602/1072, Loss: 4.3389\n",
      "Iteration 603/1072, Loss: 4.2860\n",
      "Iteration 604/1072, Loss: 4.0878\n",
      "Iteration 605/1072, Loss: 4.1595\n",
      "Iteration 606/1072, Loss: 4.5467\n",
      "Iteration 607/1072, Loss: 3.9896\n",
      "Iteration 608/1072, Loss: 4.6840\n",
      "Iteration 609/1072, Loss: 3.8394\n",
      "Iteration 610/1072, Loss: 4.3654\n",
      "Iteration 611/1072, Loss: 4.8068\n",
      "Iteration 612/1072, Loss: 4.2330\n",
      "Iteration 613/1072, Loss: 4.8397\n",
      "Iteration 614/1072, Loss: 3.9285\n",
      "Iteration 615/1072, Loss: 3.8400\n",
      "Iteration 616/1072, Loss: 4.5579\n",
      "Iteration 617/1072, Loss: 4.1783\n",
      "Iteration 618/1072, Loss: 4.7109\n",
      "Iteration 619/1072, Loss: 5.0164\n",
      "Iteration 620/1072, Loss: 4.3563\n",
      "Iteration 621/1072, Loss: 3.5237\n",
      "Iteration 622/1072, Loss: 4.5879\n",
      "Iteration 623/1072, Loss: 4.2187\n",
      "Iteration 624/1072, Loss: 3.4784\n",
      "Iteration 625/1072, Loss: 3.4649\n",
      "Iteration 626/1072, Loss: 4.5794\n",
      "Iteration 627/1072, Loss: 4.5176\n",
      "Iteration 628/1072, Loss: 4.1803\n",
      "Iteration 629/1072, Loss: 4.0148\n",
      "Iteration 630/1072, Loss: 3.9687\n",
      "Iteration 631/1072, Loss: 3.6863\n",
      "Iteration 632/1072, Loss: 4.1488\n",
      "Iteration 633/1072, Loss: 4.1570\n",
      "Iteration 634/1072, Loss: 4.1567\n",
      "Iteration 635/1072, Loss: 4.1078\n",
      "Iteration 636/1072, Loss: 4.4270\n",
      "Iteration 637/1072, Loss: 4.0413\n",
      "Iteration 638/1072, Loss: 4.7655\n",
      "Iteration 639/1072, Loss: 4.1581\n",
      "Iteration 640/1072, Loss: 4.2913\n",
      "Iteration 641/1072, Loss: 5.0788\n",
      "Iteration 642/1072, Loss: 4.4828\n",
      "Iteration 643/1072, Loss: 4.0395\n",
      "Iteration 644/1072, Loss: 4.2030\n",
      "Iteration 645/1072, Loss: 4.1465\n",
      "Iteration 646/1072, Loss: 4.5799\n",
      "Iteration 647/1072, Loss: 3.9268\n",
      "Iteration 648/1072, Loss: 4.4973\n",
      "Iteration 649/1072, Loss: 4.3267\n",
      "Iteration 650/1072, Loss: 4.1463\n",
      "Iteration 651/1072, Loss: 4.0969\n",
      "Iteration 652/1072, Loss: 4.2590\n",
      "Iteration 653/1072, Loss: 4.8949\n",
      "Iteration 654/1072, Loss: 4.1426\n",
      "Iteration 655/1072, Loss: 3.8631\n",
      "Iteration 656/1072, Loss: 4.3881\n",
      "Iteration 657/1072, Loss: 3.9759\n",
      "Iteration 658/1072, Loss: 3.6027\n",
      "Iteration 659/1072, Loss: 3.9967\n",
      "Iteration 660/1072, Loss: 4.4273\n",
      "Iteration 661/1072, Loss: 4.0191\n",
      "Iteration 662/1072, Loss: 3.9268\n",
      "Iteration 663/1072, Loss: 4.4316\n",
      "Iteration 664/1072, Loss: 4.2821\n",
      "Iteration 665/1072, Loss: 4.1919\n",
      "Iteration 666/1072, Loss: 4.2092\n",
      "Iteration 667/1072, Loss: 3.9247\n",
      "Iteration 668/1072, Loss: 4.6463\n",
      "Iteration 669/1072, Loss: 3.6874\n",
      "Iteration 670/1072, Loss: 4.1328\n",
      "Iteration 671/1072, Loss: 3.6852\n",
      "Iteration 672/1072, Loss: 3.6531\n",
      "Iteration 673/1072, Loss: 4.1505\n",
      "Iteration 674/1072, Loss: 4.0627\n",
      "Iteration 675/1072, Loss: 4.3365\n",
      "Iteration 676/1072, Loss: 3.9710\n",
      "Iteration 677/1072, Loss: 4.2898\n",
      "Iteration 678/1072, Loss: 3.9710\n",
      "Iteration 679/1072, Loss: 3.9279\n",
      "Iteration 680/1072, Loss: 4.0051\n",
      "Iteration 681/1072, Loss: 4.3576\n",
      "Iteration 682/1072, Loss: 4.6416\n",
      "Iteration 683/1072, Loss: 3.8507\n",
      "Iteration 684/1072, Loss: 3.8772\n",
      "Iteration 685/1072, Loss: 4.1772\n",
      "Iteration 686/1072, Loss: 3.9409\n",
      "Iteration 687/1072, Loss: 4.5879\n",
      "Iteration 688/1072, Loss: 3.8631\n",
      "Iteration 689/1072, Loss: 4.2977\n",
      "Iteration 690/1072, Loss: 4.4375\n",
      "Iteration 691/1072, Loss: 4.2724\n",
      "Iteration 692/1072, Loss: 3.9513\n",
      "Iteration 693/1072, Loss: 3.7438\n",
      "Iteration 694/1072, Loss: 4.1392\n",
      "Iteration 695/1072, Loss: 4.4846\n",
      "Iteration 696/1072, Loss: 4.4622\n",
      "Iteration 697/1072, Loss: 4.0616\n",
      "Iteration 698/1072, Loss: 4.6124\n",
      "Iteration 699/1072, Loss: 3.7132\n",
      "Iteration 700/1072, Loss: 4.2816\n",
      "Iteration 701/1072, Loss: 3.8473\n",
      "Iteration 702/1072, Loss: 4.0436\n",
      "Iteration 703/1072, Loss: 4.3818\n",
      "Iteration 704/1072, Loss: 4.2767\n",
      "Iteration 705/1072, Loss: 4.4297\n",
      "Iteration 706/1072, Loss: 4.4939\n",
      "Iteration 707/1072, Loss: 3.8002\n",
      "Iteration 708/1072, Loss: 3.9263\n",
      "Iteration 709/1072, Loss: 4.2828\n",
      "Iteration 710/1072, Loss: 3.9215\n",
      "Iteration 711/1072, Loss: 4.3595\n",
      "Iteration 712/1072, Loss: 4.5983\n",
      "Iteration 713/1072, Loss: 4.0161\n",
      "Iteration 714/1072, Loss: 4.7601\n",
      "Iteration 715/1072, Loss: 4.5629\n",
      "Iteration 716/1072, Loss: 4.1467\n",
      "Iteration 717/1072, Loss: 3.6187\n",
      "Iteration 718/1072, Loss: 4.3384\n",
      "Iteration 719/1072, Loss: 3.8672\n",
      "Iteration 720/1072, Loss: 3.9947\n",
      "Iteration 721/1072, Loss: 3.8573\n",
      "Iteration 722/1072, Loss: 3.9939\n",
      "Iteration 723/1072, Loss: 4.2592\n",
      "Iteration 724/1072, Loss: 3.9236\n",
      "Iteration 725/1072, Loss: 3.5230\n",
      "Iteration 726/1072, Loss: 3.7505\n",
      "Iteration 727/1072, Loss: 4.2125\n",
      "Iteration 728/1072, Loss: 4.0595\n",
      "Iteration 729/1072, Loss: 3.8399\n",
      "Iteration 730/1072, Loss: 4.3450\n",
      "Iteration 731/1072, Loss: 4.1124\n",
      "Iteration 732/1072, Loss: 4.5473\n",
      "Iteration 733/1072, Loss: 4.3962\n",
      "Iteration 734/1072, Loss: 4.8325\n",
      "Iteration 735/1072, Loss: 4.2906\n",
      "Iteration 736/1072, Loss: 4.6921\n",
      "Iteration 737/1072, Loss: 3.8772\n",
      "Iteration 738/1072, Loss: 4.5556\n",
      "Iteration 739/1072, Loss: 4.2239\n",
      "Iteration 740/1072, Loss: 4.1870\n",
      "Iteration 741/1072, Loss: 4.8525\n",
      "Iteration 742/1072, Loss: 4.4528\n",
      "Iteration 743/1072, Loss: 4.5940\n",
      "Iteration 744/1072, Loss: 4.4634\n",
      "Iteration 745/1072, Loss: 4.6134\n",
      "Iteration 746/1072, Loss: 3.8105\n",
      "Iteration 747/1072, Loss: 4.3071\n",
      "Iteration 748/1072, Loss: 3.7219\n",
      "Iteration 749/1072, Loss: 4.3754\n",
      "Iteration 750/1072, Loss: 4.3764\n",
      "Iteration 751/1072, Loss: 4.5982\n",
      "Iteration 752/1072, Loss: 4.1055\n",
      "Iteration 753/1072, Loss: 4.4859\n",
      "Iteration 754/1072, Loss: 3.5773\n",
      "Iteration 755/1072, Loss: 3.7264\n",
      "Iteration 756/1072, Loss: 4.7018\n",
      "Iteration 757/1072, Loss: 4.5588\n",
      "Iteration 758/1072, Loss: 3.8806\n",
      "Iteration 759/1072, Loss: 4.0735\n",
      "Iteration 760/1072, Loss: 4.1883\n",
      "Iteration 761/1072, Loss: 3.6135\n",
      "Iteration 762/1072, Loss: 4.4081\n",
      "Iteration 763/1072, Loss: 3.9987\n",
      "Iteration 764/1072, Loss: 3.9728\n",
      "Iteration 765/1072, Loss: 4.2957\n",
      "Iteration 766/1072, Loss: 4.3569\n",
      "Iteration 767/1072, Loss: 4.7552\n",
      "Iteration 768/1072, Loss: 4.4079\n",
      "Iteration 769/1072, Loss: 3.7734\n",
      "Iteration 770/1072, Loss: 3.9810\n",
      "Iteration 771/1072, Loss: 4.5282\n",
      "Iteration 772/1072, Loss: 3.8112\n",
      "Iteration 773/1072, Loss: 4.5999\n",
      "Iteration 774/1072, Loss: 3.9221\n",
      "Iteration 775/1072, Loss: 3.9630\n",
      "Iteration 776/1072, Loss: 4.2477\n",
      "Iteration 777/1072, Loss: 4.0067\n",
      "Iteration 778/1072, Loss: 4.9211\n",
      "Iteration 779/1072, Loss: 4.3868\n",
      "Iteration 780/1072, Loss: 3.7427\n",
      "Iteration 781/1072, Loss: 3.9057\n",
      "Iteration 782/1072, Loss: 4.0941\n",
      "Iteration 783/1072, Loss: 4.0942\n",
      "Iteration 784/1072, Loss: 3.9623\n",
      "Iteration 785/1072, Loss: 4.5841\n",
      "Iteration 786/1072, Loss: 3.9485\n",
      "Iteration 787/1072, Loss: 4.1695\n",
      "Iteration 788/1072, Loss: 4.1710\n",
      "Iteration 789/1072, Loss: 4.1935\n",
      "Iteration 790/1072, Loss: 4.4271\n",
      "Iteration 791/1072, Loss: 4.2581\n",
      "Iteration 792/1072, Loss: 3.5914\n",
      "Iteration 793/1072, Loss: 4.4081\n",
      "Iteration 794/1072, Loss: 3.3232\n",
      "Iteration 795/1072, Loss: 4.1063\n",
      "Iteration 796/1072, Loss: 4.2070\n",
      "Iteration 797/1072, Loss: 3.9158\n",
      "Iteration 798/1072, Loss: 4.1790\n",
      "Iteration 799/1072, Loss: 4.5766\n",
      "Iteration 800/1072, Loss: 4.1495\n",
      "Iteration 801/1072, Loss: 4.1497\n",
      "Iteration 802/1072, Loss: 4.3038\n",
      "Iteration 803/1072, Loss: 4.5580\n",
      "Iteration 804/1072, Loss: 3.8761\n",
      "Iteration 805/1072, Loss: 4.7251\n",
      "Iteration 806/1072, Loss: 3.6489\n",
      "Iteration 807/1072, Loss: 4.6925\n",
      "Iteration 808/1072, Loss: 4.0737\n",
      "Iteration 809/1072, Loss: 3.9188\n",
      "Iteration 810/1072, Loss: 4.3299\n",
      "Iteration 811/1072, Loss: 4.3174\n",
      "Iteration 812/1072, Loss: 4.1211\n",
      "Iteration 813/1072, Loss: 4.0394\n",
      "Iteration 814/1072, Loss: 4.0068\n",
      "Iteration 815/1072, Loss: 4.1665\n",
      "Iteration 816/1072, Loss: 4.2873\n",
      "Iteration 817/1072, Loss: 4.8985\n",
      "Iteration 818/1072, Loss: 4.1926\n",
      "Iteration 819/1072, Loss: 4.1055\n",
      "Iteration 820/1072, Loss: 4.5746\n",
      "Iteration 821/1072, Loss: 4.1118\n",
      "Iteration 822/1072, Loss: 4.0377\n",
      "Iteration 823/1072, Loss: 4.3945\n",
      "Iteration 824/1072, Loss: 4.2030\n",
      "Iteration 825/1072, Loss: 4.5975\n",
      "Iteration 826/1072, Loss: 4.9198\n",
      "Iteration 827/1072, Loss: 4.0186\n",
      "Iteration 828/1072, Loss: 4.4114\n",
      "Iteration 829/1072, Loss: 3.6491\n",
      "Iteration 830/1072, Loss: 3.8294\n",
      "Iteration 831/1072, Loss: 4.1668\n",
      "Iteration 832/1072, Loss: 3.9910\n",
      "Iteration 833/1072, Loss: 3.6014\n",
      "Iteration 834/1072, Loss: 3.9888\n",
      "Iteration 835/1072, Loss: 4.0457\n",
      "Iteration 836/1072, Loss: 4.1654\n",
      "Iteration 837/1072, Loss: 4.5924\n",
      "Iteration 838/1072, Loss: 3.6325\n",
      "Iteration 839/1072, Loss: 3.9309\n",
      "Iteration 840/1072, Loss: 4.0752\n",
      "Iteration 841/1072, Loss: 4.1654\n",
      "Iteration 842/1072, Loss: 4.2249\n",
      "Iteration 843/1072, Loss: 3.9864\n",
      "Iteration 844/1072, Loss: 3.7907\n",
      "Iteration 845/1072, Loss: 3.8726\n",
      "Iteration 846/1072, Loss: 4.3254\n",
      "Iteration 847/1072, Loss: 4.2630\n",
      "Iteration 848/1072, Loss: 4.3863\n",
      "Iteration 849/1072, Loss: 4.5228\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_accuracy = 0.0  # Initialize best accuracy\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        print(f\"Iteration {i+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(epoch_loss)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # Save model checkpoint if validation accuracy improves\n",
    "    val_dataset = CustomDataset('validation.csv', 'final/validation_images', transform=data_transforms)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f\"Validation Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "        # Save the model checkpoint if accuracy improves\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            torch.save(model.state_dict(), 'resnet_checkpoint/best_model_checkpoint.pth')\n",
    "            print(\"Model checkpoint saved!\")\n",
    "\n",
    "# Plotting the train and validation loss curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train and Validation Loss Curves')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Done done done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0123e699-d821-4270-957b-96a75ed63192",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030865f0-025f-4957-bbf0-0ff64b548ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-13.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m108"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

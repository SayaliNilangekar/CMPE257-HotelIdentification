{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d078e5d1-57df-4c73-88ba-3d88e415615b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install efficientnet_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eeb1878d-d01a-49b5-ab43-653f9a5a7558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from efficientnet_pytorch import EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94c6a783-ee5f-4577-9a36-1d6569414ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b811925-88b1-4ac4-aee1-efe144c82b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.classes = self.data['hotel_id'].unique().tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, str(self.data.iloc[idx, 1]), str(self.data.iloc[idx, 0]))\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        label = self.classes.index(self.data.iloc[idx, 1])\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5baab643-992b-427c-b711-e96873f4612a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b4\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CustomDataset('train.csv', 'final/train_images', transform=data_transforms)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "model = EfficientNet.from_pretrained('efficientnet-b4')\n",
    "num_features = model._fc.in_features\n",
    "model._fc = nn.Linear(num_features, len(train_dataset.classes))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee19ca87-e580-4ec4-a03b-be0def0f19b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (_conv_stem): Conv2dStaticSamePadding(\n",
       "    3, 48, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
       "    (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
       "  )\n",
       "  (_bn0): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (_blocks): ModuleList(\n",
       "    (0): MBConvBlock(\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        48, 48, kernel_size=(3, 3), stride=[1, 1], groups=48, bias=False\n",
       "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        48, 12, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        12, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (1): MBConvBlock(\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        24, 24, kernel_size=(3, 3), stride=(1, 1), groups=24, bias=False\n",
       "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        24, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 24, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (2): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(3, 3), stride=[2, 2], groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (3): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        192, 192, kernel_size=(3, 3), stride=(1, 1), groups=192, bias=False\n",
       "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        192, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        8, 192, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (4): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        192, 192, kernel_size=(3, 3), stride=(1, 1), groups=192, bias=False\n",
       "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        192, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        8, 192, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (5): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        192, 192, kernel_size=(3, 3), stride=(1, 1), groups=192, bias=False\n",
       "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        192, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        8, 192, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (6): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        192, 192, kernel_size=(5, 5), stride=[2, 2], groups=192, bias=False\n",
       "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        192, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        8, 192, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        192, 56, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(56, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (7): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        336, 336, kernel_size=(5, 5), stride=(1, 1), groups=336, bias=False\n",
       "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        336, 14, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        14, 336, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(56, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (8): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        336, 336, kernel_size=(5, 5), stride=(1, 1), groups=336, bias=False\n",
       "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        336, 14, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        14, 336, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(56, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (9): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        336, 336, kernel_size=(5, 5), stride=(1, 1), groups=336, bias=False\n",
       "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        336, 14, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        14, 336, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(56, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (10): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        336, 336, kernel_size=(3, 3), stride=[2, 2], groups=336, bias=False\n",
       "        (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        336, 14, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        14, 336, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        336, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (11): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(3, 3), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (12): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(3, 3), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (13): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(3, 3), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (14): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(3, 3), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (15): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(3, 3), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (16): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=[1, 1], groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (17): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False\n",
       "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (18): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False\n",
       "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (19): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False\n",
       "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (20): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False\n",
       "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (21): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False\n",
       "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (22): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        960, 960, kernel_size=(5, 5), stride=[2, 2], groups=960, bias=False\n",
       "        (static_padding): ZeroPad2d((1, 2, 1, 2))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        960, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (23): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
       "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (24): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
       "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (25): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
       "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (26): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
       "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (27): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
       "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (28): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
       "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (29): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
       "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (30): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1632, 1632, kernel_size=(3, 3), stride=[1, 1], groups=1632, bias=False\n",
       "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1632, 448, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(448, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (31): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        448, 2688, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(2688, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        2688, 2688, kernel_size=(3, 3), stride=(1, 1), groups=2688, bias=False\n",
       "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(2688, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        2688, 112, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        112, 2688, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        2688, 448, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(448, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "  )\n",
       "  (_conv_head): Conv2dStaticSamePadding(\n",
       "    448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (static_padding): Identity()\n",
       "  )\n",
       "  (_bn1): BatchNorm2d(1792, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
       "  (_dropout): Dropout(p=0.4, inplace=False)\n",
       "  (_fc): Linear(in_features=1792, out_features=3116, bias=True)\n",
       "  (_swish): MemoryEfficientSwish()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50cf434b-ab44-4cde-ab40-fc870fdfa534",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "best_accuracy = 0.0  # Track the best validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "532c6adc-3b70-4166-b80a-8930ee62996c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []  \n",
    "val_losses = []  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9a6ba92-7f60-4b83-815d-0db634d0e89d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/1072, Loss: 8.0877\n",
      "Iteration 2/1072, Loss: 8.0821\n",
      "Iteration 3/1072, Loss: 8.0637\n",
      "Iteration 4/1072, Loss: 8.0095\n",
      "Iteration 5/1072, Loss: 8.0881\n",
      "Iteration 6/1072, Loss: 8.1100\n",
      "Iteration 7/1072, Loss: 8.0555\n",
      "Iteration 8/1072, Loss: 8.0661\n",
      "Iteration 9/1072, Loss: 8.0112\n",
      "Iteration 10/1072, Loss: 8.0671\n",
      "Iteration 11/1072, Loss: 8.0379\n",
      "Iteration 12/1072, Loss: 8.1222\n",
      "Iteration 13/1072, Loss: 8.0518\n",
      "Iteration 14/1072, Loss: 7.9983\n",
      "Iteration 15/1072, Loss: 8.0871\n",
      "Iteration 16/1072, Loss: 8.1005\n",
      "Iteration 17/1072, Loss: 8.0673\n",
      "Iteration 18/1072, Loss: 8.1220\n",
      "Iteration 19/1072, Loss: 8.0675\n",
      "Iteration 20/1072, Loss: 7.9916\n",
      "Iteration 21/1072, Loss: 8.1142\n",
      "Iteration 22/1072, Loss: 8.0897\n",
      "Iteration 23/1072, Loss: 8.0994\n",
      "Iteration 24/1072, Loss: 8.0604\n",
      "Iteration 25/1072, Loss: 8.0715\n",
      "Iteration 26/1072, Loss: 8.0858\n",
      "Iteration 27/1072, Loss: 8.0838\n",
      "Iteration 28/1072, Loss: 8.0735\n",
      "Iteration 29/1072, Loss: 8.0424\n",
      "Iteration 30/1072, Loss: 8.0190\n",
      "Iteration 31/1072, Loss: 8.1322\n",
      "Iteration 32/1072, Loss: 8.0872\n",
      "Iteration 33/1072, Loss: 8.0772\n",
      "Iteration 34/1072, Loss: 8.0873\n",
      "Iteration 35/1072, Loss: 8.0290\n",
      "Iteration 36/1072, Loss: 8.0608\n",
      "Iteration 37/1072, Loss: 8.0431\n",
      "Iteration 38/1072, Loss: 7.9969\n",
      "Iteration 39/1072, Loss: 8.0750\n",
      "Iteration 40/1072, Loss: 8.0365\n",
      "Iteration 41/1072, Loss: 8.0207\n",
      "Iteration 42/1072, Loss: 8.0662\n",
      "Iteration 43/1072, Loss: 8.0923\n",
      "Iteration 44/1072, Loss: 8.0638\n",
      "Iteration 45/1072, Loss: 8.0689\n",
      "Iteration 46/1072, Loss: 8.0446\n",
      "Iteration 47/1072, Loss: 8.0306\n",
      "Iteration 48/1072, Loss: 8.0522\n",
      "Iteration 49/1072, Loss: 8.0793\n",
      "Iteration 50/1072, Loss: 8.0553\n",
      "Iteration 51/1072, Loss: 8.0611\n",
      "Iteration 52/1072, Loss: 8.0751\n",
      "Iteration 53/1072, Loss: 8.1088\n",
      "Iteration 54/1072, Loss: 8.0683\n",
      "Iteration 55/1072, Loss: 8.0765\n",
      "Iteration 56/1072, Loss: 8.1605\n",
      "Iteration 57/1072, Loss: 8.0363\n",
      "Iteration 58/1072, Loss: 8.0158\n",
      "Iteration 59/1072, Loss: 8.0461\n",
      "Iteration 60/1072, Loss: 8.0255\n",
      "Iteration 61/1072, Loss: 8.0715\n",
      "Iteration 62/1072, Loss: 8.0246\n",
      "Iteration 63/1072, Loss: 8.0605\n",
      "Iteration 64/1072, Loss: 8.1514\n",
      "Iteration 65/1072, Loss: 8.0255\n",
      "Iteration 66/1072, Loss: 8.0789\n",
      "Iteration 67/1072, Loss: 8.0286\n",
      "Iteration 68/1072, Loss: 7.9896\n",
      "Iteration 69/1072, Loss: 8.0392\n",
      "Iteration 70/1072, Loss: 8.0775\n",
      "Iteration 71/1072, Loss: 8.0196\n",
      "Iteration 72/1072, Loss: 8.0581\n",
      "Iteration 73/1072, Loss: 8.0812\n",
      "Iteration 74/1072, Loss: 8.0463\n",
      "Iteration 75/1072, Loss: 8.0368\n",
      "Iteration 76/1072, Loss: 8.0653\n",
      "Iteration 77/1072, Loss: 8.0698\n",
      "Iteration 78/1072, Loss: 8.0476\n",
      "Iteration 79/1072, Loss: 8.0910\n",
      "Iteration 80/1072, Loss: 8.0586\n",
      "Iteration 81/1072, Loss: 8.0562\n",
      "Iteration 82/1072, Loss: 8.0083\n",
      "Iteration 83/1072, Loss: 8.0853\n",
      "Iteration 84/1072, Loss: 8.1021\n",
      "Iteration 85/1072, Loss: 8.0951\n",
      "Iteration 86/1072, Loss: 8.0186\n",
      "Iteration 87/1072, Loss: 8.0359\n",
      "Iteration 88/1072, Loss: 8.1050\n",
      "Iteration 89/1072, Loss: 8.1036\n",
      "Iteration 90/1072, Loss: 8.0474\n",
      "Iteration 91/1072, Loss: 8.1092\n",
      "Iteration 92/1072, Loss: 8.1251\n",
      "Iteration 93/1072, Loss: 8.0907\n",
      "Iteration 94/1072, Loss: 8.0374\n",
      "Iteration 95/1072, Loss: 8.0570\n",
      "Iteration 96/1072, Loss: 8.0500\n",
      "Iteration 97/1072, Loss: 8.0713\n",
      "Iteration 98/1072, Loss: 8.0259\n",
      "Iteration 99/1072, Loss: 8.0719\n",
      "Iteration 100/1072, Loss: 8.0402\n",
      "Iteration 101/1072, Loss: 8.0184\n",
      "Iteration 102/1072, Loss: 8.0271\n",
      "Iteration 103/1072, Loss: 8.0546\n",
      "Iteration 104/1072, Loss: 8.0952\n",
      "Iteration 105/1072, Loss: 8.0936\n",
      "Iteration 106/1072, Loss: 8.0724\n",
      "Iteration 107/1072, Loss: 8.0939\n",
      "Iteration 108/1072, Loss: 8.0544\n",
      "Iteration 109/1072, Loss: 8.0887\n",
      "Iteration 110/1072, Loss: 8.0310\n",
      "Iteration 111/1072, Loss: 8.1021\n",
      "Iteration 112/1072, Loss: 8.0832\n",
      "Iteration 113/1072, Loss: 8.0068\n",
      "Iteration 114/1072, Loss: 8.0709\n",
      "Iteration 115/1072, Loss: 8.0806\n",
      "Iteration 116/1072, Loss: 8.0533\n",
      "Iteration 117/1072, Loss: 8.0315\n",
      "Iteration 118/1072, Loss: 8.0569\n",
      "Iteration 119/1072, Loss: 8.1116\n",
      "Iteration 120/1072, Loss: 8.0305\n",
      "Iteration 121/1072, Loss: 8.0405\n",
      "Iteration 122/1072, Loss: 8.0155\n",
      "Iteration 123/1072, Loss: 8.0437\n",
      "Iteration 124/1072, Loss: 8.0406\n",
      "Iteration 125/1072, Loss: 8.0525\n",
      "Iteration 126/1072, Loss: 8.0540\n",
      "Iteration 127/1072, Loss: 8.0350\n",
      "Iteration 128/1072, Loss: 8.0815\n",
      "Iteration 129/1072, Loss: 8.0288\n",
      "Iteration 130/1072, Loss: 8.0368\n",
      "Iteration 131/1072, Loss: 8.0733\n",
      "Iteration 132/1072, Loss: 8.0954\n",
      "Iteration 133/1072, Loss: 8.0590\n",
      "Iteration 134/1072, Loss: 8.0986\n",
      "Iteration 135/1072, Loss: 8.0351\n",
      "Iteration 136/1072, Loss: 8.0463\n",
      "Iteration 137/1072, Loss: 7.9776\n",
      "Iteration 138/1072, Loss: 8.0903\n",
      "Iteration 139/1072, Loss: 8.0759\n",
      "Iteration 140/1072, Loss: 8.0432\n",
      "Iteration 141/1072, Loss: 8.0332\n",
      "Iteration 142/1072, Loss: 7.9737\n",
      "Iteration 143/1072, Loss: 8.0407\n",
      "Iteration 144/1072, Loss: 8.0231\n",
      "Iteration 145/1072, Loss: 8.0372\n",
      "Iteration 146/1072, Loss: 8.0406\n",
      "Iteration 147/1072, Loss: 8.0264\n",
      "Iteration 148/1072, Loss: 8.0966\n",
      "Iteration 149/1072, Loss: 8.0489\n",
      "Iteration 150/1072, Loss: 8.0106\n",
      "Iteration 151/1072, Loss: 8.0688\n",
      "Iteration 152/1072, Loss: 8.0287\n",
      "Iteration 153/1072, Loss: 8.0544\n",
      "Iteration 154/1072, Loss: 8.0706\n",
      "Iteration 155/1072, Loss: 8.0474\n",
      "Iteration 156/1072, Loss: 8.0351\n",
      "Iteration 157/1072, Loss: 8.0385\n",
      "Iteration 158/1072, Loss: 8.0421\n",
      "Iteration 159/1072, Loss: 8.0146\n",
      "Iteration 160/1072, Loss: 8.0644\n",
      "Iteration 161/1072, Loss: 8.0508\n",
      "Iteration 162/1072, Loss: 8.0791\n",
      "Iteration 163/1072, Loss: 7.9987\n",
      "Iteration 164/1072, Loss: 8.1315\n",
      "Iteration 165/1072, Loss: 8.1126\n",
      "Iteration 166/1072, Loss: 8.1177\n",
      "Iteration 167/1072, Loss: 8.0301\n",
      "Iteration 168/1072, Loss: 8.0817\n",
      "Iteration 169/1072, Loss: 8.0573\n",
      "Iteration 170/1072, Loss: 8.1293\n",
      "Iteration 171/1072, Loss: 8.1073\n",
      "Iteration 172/1072, Loss: 8.0868\n",
      "Iteration 173/1072, Loss: 8.0529\n",
      "Iteration 174/1072, Loss: 8.0213\n",
      "Iteration 175/1072, Loss: 8.0383\n",
      "Iteration 176/1072, Loss: 8.0701\n",
      "Iteration 177/1072, Loss: 8.0557\n",
      "Iteration 178/1072, Loss: 8.0851\n",
      "Iteration 179/1072, Loss: 8.0738\n",
      "Iteration 180/1072, Loss: 8.0820\n",
      "Iteration 181/1072, Loss: 8.0300\n",
      "Iteration 182/1072, Loss: 8.1069\n",
      "Iteration 183/1072, Loss: 8.0686\n",
      "Iteration 184/1072, Loss: 8.0462\n",
      "Iteration 185/1072, Loss: 8.1135\n",
      "Iteration 186/1072, Loss: 8.0101\n",
      "Iteration 187/1072, Loss: 8.0415\n",
      "Iteration 188/1072, Loss: 8.0707\n",
      "Iteration 189/1072, Loss: 7.9812\n",
      "Iteration 190/1072, Loss: 8.0696\n",
      "Iteration 191/1072, Loss: 8.0755\n",
      "Iteration 192/1072, Loss: 8.0883\n",
      "Iteration 193/1072, Loss: 8.0996\n",
      "Iteration 194/1072, Loss: 8.0901\n",
      "Iteration 195/1072, Loss: 8.0238\n",
      "Iteration 196/1072, Loss: 8.0928\n",
      "Iteration 197/1072, Loss: 8.0675\n",
      "Iteration 198/1072, Loss: 8.0463\n",
      "Iteration 199/1072, Loss: 8.0255\n",
      "Iteration 200/1072, Loss: 8.0340\n",
      "Iteration 201/1072, Loss: 8.0317\n",
      "Iteration 202/1072, Loss: 8.1015\n",
      "Iteration 203/1072, Loss: 8.0737\n",
      "Iteration 204/1072, Loss: 7.9812\n",
      "Iteration 205/1072, Loss: 7.9913\n",
      "Iteration 206/1072, Loss: 8.0163\n",
      "Iteration 207/1072, Loss: 8.0516\n",
      "Iteration 208/1072, Loss: 8.0433\n",
      "Iteration 209/1072, Loss: 8.0363\n",
      "Iteration 210/1072, Loss: 8.0585\n",
      "Iteration 211/1072, Loss: 8.0423\n",
      "Iteration 212/1072, Loss: 8.0320\n",
      "Iteration 213/1072, Loss: 8.0609\n",
      "Iteration 214/1072, Loss: 8.0869\n",
      "Iteration 215/1072, Loss: 8.0487\n",
      "Iteration 216/1072, Loss: 8.1128\n",
      "Iteration 217/1072, Loss: 7.9964\n",
      "Iteration 218/1072, Loss: 8.0437\n",
      "Iteration 219/1072, Loss: 8.0302\n",
      "Iteration 220/1072, Loss: 8.0664\n",
      "Iteration 221/1072, Loss: 8.0309\n",
      "Iteration 222/1072, Loss: 8.0523\n",
      "Iteration 223/1072, Loss: 8.0769\n",
      "Iteration 224/1072, Loss: 8.0475\n",
      "Iteration 225/1072, Loss: 8.1056\n",
      "Iteration 226/1072, Loss: 8.0997\n",
      "Iteration 227/1072, Loss: 8.0742\n",
      "Iteration 228/1072, Loss: 8.0688\n",
      "Iteration 229/1072, Loss: 8.0647\n",
      "Iteration 230/1072, Loss: 8.0321\n",
      "Iteration 231/1072, Loss: 8.0573\n",
      "Iteration 232/1072, Loss: 8.1003\n",
      "Iteration 233/1072, Loss: 8.0707\n",
      "Iteration 234/1072, Loss: 8.0973\n",
      "Iteration 235/1072, Loss: 8.0851\n",
      "Iteration 236/1072, Loss: 8.0711\n",
      "Iteration 237/1072, Loss: 8.1091\n",
      "Iteration 238/1072, Loss: 8.0454\n",
      "Iteration 239/1072, Loss: 8.0378\n",
      "Iteration 240/1072, Loss: 8.0545\n",
      "Iteration 241/1072, Loss: 8.0668\n",
      "Iteration 242/1072, Loss: 8.0469\n",
      "Iteration 243/1072, Loss: 8.0529\n",
      "Iteration 244/1072, Loss: 8.0337\n",
      "Iteration 245/1072, Loss: 8.0594\n",
      "Iteration 246/1072, Loss: 8.0416\n",
      "Iteration 247/1072, Loss: 8.0441\n",
      "Iteration 248/1072, Loss: 8.0345\n",
      "Iteration 249/1072, Loss: 8.0772\n",
      "Iteration 250/1072, Loss: 8.0169\n",
      "Iteration 251/1072, Loss: 8.0342\n",
      "Iteration 252/1072, Loss: 8.1086\n",
      "Iteration 253/1072, Loss: 8.0113\n",
      "Iteration 254/1072, Loss: 8.0327\n",
      "Iteration 255/1072, Loss: 8.0604\n",
      "Iteration 256/1072, Loss: 8.0868\n",
      "Iteration 257/1072, Loss: 8.0449\n",
      "Iteration 258/1072, Loss: 8.0494\n",
      "Iteration 259/1072, Loss: 8.0812\n",
      "Iteration 260/1072, Loss: 8.0135\n",
      "Iteration 261/1072, Loss: 8.0691\n",
      "Iteration 262/1072, Loss: 7.9819\n",
      "Iteration 263/1072, Loss: 8.0079\n",
      "Iteration 264/1072, Loss: 8.0512\n",
      "Iteration 265/1072, Loss: 8.0641\n",
      "Iteration 266/1072, Loss: 8.0028\n",
      "Iteration 311/1072, Loss: 8.0660\n",
      "Iteration 312/1072, Loss: 8.0621\n",
      "Iteration 313/1072, Loss: 8.0473\n",
      "Iteration 314/1072, Loss: 8.0203\n",
      "Iteration 315/1072, Loss: 8.0107\n",
      "Iteration 316/1072, Loss: 8.0436\n",
      "Iteration 317/1072, Loss: 8.0324\n",
      "Iteration 318/1072, Loss: 8.1034\n",
      "Iteration 319/1072, Loss: 8.0519\n",
      "Iteration 320/1072, Loss: 8.0672\n",
      "Iteration 321/1072, Loss: 8.0336\n",
      "Iteration 322/1072, Loss: 8.0356\n",
      "Iteration 323/1072, Loss: 8.0869\n",
      "Iteration 324/1072, Loss: 8.0715\n",
      "Iteration 325/1072, Loss: 8.0895\n",
      "Iteration 326/1072, Loss: 8.0313\n",
      "Iteration 327/1072, Loss: 8.0677\n",
      "Iteration 328/1072, Loss: 8.0464\n",
      "Iteration 329/1072, Loss: 8.0778\n",
      "Iteration 330/1072, Loss: 8.0770\n",
      "Iteration 331/1072, Loss: 8.0737\n",
      "Iteration 332/1072, Loss: 8.0154\n",
      "Iteration 333/1072, Loss: 8.0930\n",
      "Iteration 334/1072, Loss: 8.0805\n",
      "Iteration 335/1072, Loss: 8.0595\n",
      "Iteration 336/1072, Loss: 8.0564\n",
      "Iteration 337/1072, Loss: 8.0787\n",
      "Iteration 338/1072, Loss: 8.0423\n",
      "Iteration 339/1072, Loss: 8.0264\n",
      "Iteration 340/1072, Loss: 8.1057\n",
      "Iteration 341/1072, Loss: 8.0399\n",
      "Iteration 342/1072, Loss: 8.0758\n",
      "Iteration 343/1072, Loss: 8.0403\n",
      "Iteration 344/1072, Loss: 8.0391\n",
      "Iteration 345/1072, Loss: 8.0347\n",
      "Iteration 346/1072, Loss: 7.9719\n",
      "Iteration 347/1072, Loss: 8.0630\n",
      "Iteration 348/1072, Loss: 8.0192\n",
      "Iteration 349/1072, Loss: 8.0696\n",
      "Iteration 350/1072, Loss: 8.0544\n",
      "Iteration 351/1072, Loss: 8.0474\n",
      "Iteration 352/1072, Loss: 8.0598\n",
      "Iteration 353/1072, Loss: 8.0821\n",
      "Iteration 354/1072, Loss: 8.0135\n",
      "Iteration 355/1072, Loss: 8.0860\n",
      "Iteration 356/1072, Loss: 8.1053\n",
      "Iteration 357/1072, Loss: 8.0183\n",
      "Iteration 358/1072, Loss: 8.0425\n",
      "Iteration 359/1072, Loss: 8.0246\n",
      "Iteration 360/1072, Loss: 8.0486\n",
      "Iteration 361/1072, Loss: 8.0629\n",
      "Iteration 362/1072, Loss: 8.0919\n",
      "Iteration 363/1072, Loss: 8.0987\n",
      "Iteration 364/1072, Loss: 8.1319\n",
      "Iteration 365/1072, Loss: 8.0858\n",
      "Iteration 366/1072, Loss: 8.0551\n",
      "Iteration 367/1072, Loss: 8.1156\n",
      "Iteration 368/1072, Loss: 8.0338\n",
      "Iteration 369/1072, Loss: 8.0715\n",
      "Iteration 370/1072, Loss: 8.0735\n",
      "Iteration 371/1072, Loss: 8.0827\n",
      "Iteration 372/1072, Loss: 8.1053\n",
      "Iteration 373/1072, Loss: 8.0415\n",
      "Iteration 374/1072, Loss: 8.0403\n",
      "Iteration 375/1072, Loss: 8.0935\n",
      "Iteration 376/1072, Loss: 8.0386\n",
      "Iteration 377/1072, Loss: 8.0692\n",
      "Iteration 378/1072, Loss: 8.0857\n",
      "Iteration 379/1072, Loss: 8.0352\n",
      "Iteration 380/1072, Loss: 8.0838\n",
      "Iteration 381/1072, Loss: 8.1001\n",
      "Iteration 382/1072, Loss: 8.0743\n",
      "Iteration 383/1072, Loss: 8.0473\n",
      "Iteration 384/1072, Loss: 8.0693\n",
      "Iteration 385/1072, Loss: 7.9910\n",
      "Iteration 386/1072, Loss: 8.0386\n",
      "Iteration 387/1072, Loss: 8.0165\n",
      "Iteration 388/1072, Loss: 8.0281\n",
      "Iteration 389/1072, Loss: 8.0193\n",
      "Iteration 390/1072, Loss: 8.0333\n",
      "Iteration 391/1072, Loss: 8.0588\n",
      "Iteration 392/1072, Loss: 8.0646\n",
      "Iteration 393/1072, Loss: 8.0794\n",
      "Iteration 394/1072, Loss: 8.0657\n",
      "Iteration 395/1072, Loss: 8.0498\n",
      "Iteration 396/1072, Loss: 8.0149\n",
      "Iteration 397/1072, Loss: 8.1012\n",
      "Iteration 398/1072, Loss: 8.0139\n",
      "Iteration 399/1072, Loss: 8.0289\n",
      "Iteration 400/1072, Loss: 8.0463\n",
      "Iteration 401/1072, Loss: 8.0266\n",
      "Iteration 402/1072, Loss: 8.0314\n",
      "Iteration 403/1072, Loss: 8.0340\n",
      "Iteration 404/1072, Loss: 8.0509\n",
      "Iteration 405/1072, Loss: 8.0573\n",
      "Iteration 406/1072, Loss: 8.0425\n",
      "Iteration 407/1072, Loss: 8.0574\n",
      "Iteration 408/1072, Loss: 8.0476\n",
      "Iteration 409/1072, Loss: 8.0249\n",
      "Iteration 410/1072, Loss: 8.0004\n",
      "Iteration 411/1072, Loss: 8.0345\n",
      "Iteration 412/1072, Loss: 8.0458\n",
      "Iteration 413/1072, Loss: 8.0672\n",
      "Iteration 414/1072, Loss: 8.0560\n",
      "Iteration 415/1072, Loss: 8.1079\n",
      "Iteration 416/1072, Loss: 8.1047\n",
      "Iteration 417/1072, Loss: 8.0567\n",
      "Iteration 418/1072, Loss: 8.0253\n",
      "Iteration 419/1072, Loss: 8.0886\n",
      "Iteration 420/1072, Loss: 8.1296\n",
      "Iteration 421/1072, Loss: 8.1062\n",
      "Iteration 422/1072, Loss: 8.0058\n",
      "Iteration 423/1072, Loss: 8.0757\n",
      "Iteration 424/1072, Loss: 7.9996\n",
      "Iteration 425/1072, Loss: 8.0327\n",
      "Iteration 426/1072, Loss: 8.0324\n",
      "Iteration 427/1072, Loss: 7.9821\n",
      "Iteration 428/1072, Loss: 8.0781\n",
      "Iteration 429/1072, Loss: 8.0042\n",
      "Iteration 430/1072, Loss: 8.0104\n",
      "Iteration 431/1072, Loss: 8.0788\n",
      "Iteration 432/1072, Loss: 8.0582\n",
      "Iteration 433/1072, Loss: 8.0425\n",
      "Iteration 434/1072, Loss: 8.0562\n",
      "Iteration 435/1072, Loss: 8.0992\n",
      "Iteration 436/1072, Loss: 8.0138\n",
      "Iteration 437/1072, Loss: 8.0217\n",
      "Iteration 438/1072, Loss: 8.0271\n",
      "Iteration 439/1072, Loss: 7.9920\n",
      "Iteration 440/1072, Loss: 8.0016\n",
      "Iteration 441/1072, Loss: 8.0824\n",
      "Iteration 442/1072, Loss: 8.0788\n",
      "Iteration 443/1072, Loss: 8.0147\n",
      "Iteration 444/1072, Loss: 8.0505\n",
      "Iteration 445/1072, Loss: 8.0580\n",
      "Iteration 446/1072, Loss: 7.9872\n",
      "Iteration 447/1072, Loss: 8.0684\n",
      "Iteration 448/1072, Loss: 8.0820\n",
      "Iteration 449/1072, Loss: 8.0248\n",
      "Iteration 450/1072, Loss: 8.0633\n",
      "Iteration 451/1072, Loss: 8.0287\n",
      "Iteration 452/1072, Loss: 8.1095\n",
      "Iteration 453/1072, Loss: 8.0755\n",
      "Iteration 454/1072, Loss: 8.0487\n",
      "Iteration 455/1072, Loss: 8.1014\n",
      "Iteration 456/1072, Loss: 8.0282\n",
      "Iteration 457/1072, Loss: 8.0577\n",
      "Iteration 458/1072, Loss: 8.0285\n",
      "Iteration 459/1072, Loss: 8.0211\n",
      "Iteration 460/1072, Loss: 8.0156\n",
      "Iteration 461/1072, Loss: 8.0447\n",
      "Iteration 462/1072, Loss: 8.0649\n",
      "Iteration 463/1072, Loss: 8.0460\n",
      "Iteration 464/1072, Loss: 8.0172\n",
      "Iteration 465/1072, Loss: 8.0279\n",
      "Iteration 466/1072, Loss: 8.0819\n",
      "Iteration 467/1072, Loss: 8.1200\n",
      "Iteration 468/1072, Loss: 8.0660\n",
      "Iteration 469/1072, Loss: 8.0533\n",
      "Iteration 470/1072, Loss: 8.0601\n",
      "Iteration 471/1072, Loss: 8.0366\n",
      "Iteration 472/1072, Loss: 8.0411\n",
      "Iteration 473/1072, Loss: 8.0263\n",
      "Iteration 474/1072, Loss: 8.0166\n",
      "Iteration 475/1072, Loss: 8.0902\n",
      "Iteration 476/1072, Loss: 8.0367\n",
      "Iteration 477/1072, Loss: 8.0609\n",
      "Iteration 478/1072, Loss: 8.0673\n",
      "Iteration 479/1072, Loss: 8.1356\n",
      "Iteration 480/1072, Loss: 8.0523\n",
      "Iteration 481/1072, Loss: 8.0318\n",
      "Iteration 482/1072, Loss: 8.0790\n",
      "Iteration 483/1072, Loss: 8.0541\n",
      "Iteration 484/1072, Loss: 8.0447\n",
      "Iteration 485/1072, Loss: 8.0352\n",
      "Iteration 486/1072, Loss: 8.0434\n",
      "Iteration 487/1072, Loss: 8.0361\n",
      "Iteration 488/1072, Loss: 8.0235\n",
      "Iteration 489/1072, Loss: 8.0561\n",
      "Iteration 490/1072, Loss: 8.0628\n",
      "Iteration 491/1072, Loss: 8.0364\n",
      "Iteration 492/1072, Loss: 8.0899\n",
      "Iteration 493/1072, Loss: 8.0685\n",
      "Iteration 494/1072, Loss: 8.0225\n",
      "Iteration 495/1072, Loss: 8.0093\n",
      "Iteration 496/1072, Loss: 8.0975\n",
      "Iteration 497/1072, Loss: 8.0365\n",
      "Iteration 498/1072, Loss: 8.1136\n",
      "Iteration 499/1072, Loss: 8.0295\n",
      "Iteration 500/1072, Loss: 8.0443\n",
      "Iteration 501/1072, Loss: 7.9821\n",
      "Iteration 502/1072, Loss: 8.0744\n",
      "Iteration 503/1072, Loss: 8.1063\n",
      "Iteration 504/1072, Loss: 8.0101\n",
      "Iteration 505/1072, Loss: 8.0356\n",
      "Iteration 506/1072, Loss: 8.0845\n",
      "Iteration 507/1072, Loss: 8.0775\n",
      "Iteration 508/1072, Loss: 8.0650\n",
      "Iteration 509/1072, Loss: 8.0877\n",
      "Iteration 510/1072, Loss: 8.0565\n",
      "Iteration 511/1072, Loss: 8.0077\n",
      "Iteration 512/1072, Loss: 8.0536\n",
      "Iteration 513/1072, Loss: 8.0721\n",
      "Iteration 514/1072, Loss: 8.0461\n",
      "Iteration 515/1072, Loss: 8.0754\n",
      "Iteration 516/1072, Loss: 8.0277\n",
      "Iteration 517/1072, Loss: 8.0747\n",
      "Iteration 518/1072, Loss: 8.0866\n",
      "Iteration 519/1072, Loss: 8.1157\n",
      "Iteration 520/1072, Loss: 8.0094\n",
      "Iteration 521/1072, Loss: 8.0968\n",
      "Iteration 522/1072, Loss: 8.0836\n",
      "Iteration 523/1072, Loss: 8.0547\n",
      "Iteration 524/1072, Loss: 8.0603\n",
      "Iteration 525/1072, Loss: 8.0595\n",
      "Iteration 526/1072, Loss: 8.0181\n",
      "Iteration 527/1072, Loss: 8.0378\n",
      "Iteration 528/1072, Loss: 8.0661\n",
      "Iteration 529/1072, Loss: 8.0625\n",
      "Iteration 530/1072, Loss: 8.0642\n",
      "Iteration 531/1072, Loss: 8.0669\n",
      "Iteration 532/1072, Loss: 8.0596\n",
      "Iteration 533/1072, Loss: 8.0351\n",
      "Iteration 534/1072, Loss: 8.0702\n",
      "Iteration 535/1072, Loss: 7.9766\n",
      "Iteration 536/1072, Loss: 7.9911\n",
      "Iteration 537/1072, Loss: 8.0518\n",
      "Iteration 538/1072, Loss: 8.0550\n",
      "Iteration 539/1072, Loss: 8.0530\n",
      "Iteration 540/1072, Loss: 8.0179\n",
      "Iteration 541/1072, Loss: 8.0044\n",
      "Iteration 542/1072, Loss: 8.0307\n",
      "Iteration 543/1072, Loss: 8.0392\n",
      "Iteration 544/1072, Loss: 8.0513\n",
      "Iteration 545/1072, Loss: 8.0692\n",
      "Iteration 546/1072, Loss: 8.0379\n",
      "Iteration 547/1072, Loss: 8.0426\n",
      "Iteration 548/1072, Loss: 8.1033\n",
      "Iteration 549/1072, Loss: 8.0305\n",
      "Iteration 550/1072, Loss: 8.0264\n",
      "Iteration 551/1072, Loss: 8.0537\n",
      "Iteration 552/1072, Loss: 8.0237\n",
      "Iteration 553/1072, Loss: 8.0207\n",
      "Iteration 554/1072, Loss: 8.0728\n",
      "Iteration 555/1072, Loss: 8.0199\n",
      "Iteration 556/1072, Loss: 8.0331\n",
      "Iteration 557/1072, Loss: 8.0973\n",
      "Iteration 558/1072, Loss: 8.0602\n",
      "Iteration 559/1072, Loss: 8.0735\n",
      "Iteration 560/1072, Loss: 8.1042\n",
      "Iteration 561/1072, Loss: 8.0022\n",
      "Iteration 562/1072, Loss: 8.0575\n",
      "Iteration 563/1072, Loss: 8.0935\n",
      "Iteration 564/1072, Loss: 8.0744\n",
      "Iteration 565/1072, Loss: 8.0563\n",
      "Iteration 566/1072, Loss: 8.0247\n",
      "Iteration 567/1072, Loss: 8.0470\n",
      "Iteration 568/1072, Loss: 8.0712\n",
      "Iteration 569/1072, Loss: 8.0644\n",
      "Iteration 570/1072, Loss: 7.9772\n",
      "Iteration 571/1072, Loss: 8.0829\n",
      "Iteration 572/1072, Loss: 8.0246\n",
      "Iteration 573/1072, Loss: 8.1002\n",
      "Iteration 574/1072, Loss: 8.0483\n",
      "Iteration 575/1072, Loss: 8.0617\n",
      "Iteration 576/1072, Loss: 8.0714\n",
      "Iteration 577/1072, Loss: 8.0477\n",
      "Iteration 578/1072, Loss: 8.0533\n",
      "Iteration 579/1072, Loss: 8.0223\n",
      "Iteration 580/1072, Loss: 8.0459\n",
      "Iteration 581/1072, Loss: 8.0813\n",
      "Iteration 582/1072, Loss: 8.0919\n",
      "Iteration 583/1072, Loss: 8.0942\n",
      "Iteration 584/1072, Loss: 8.0315\n",
      "Iteration 585/1072, Loss: 7.9965\n",
      "Iteration 586/1072, Loss: 8.0622\n",
      "Iteration 587/1072, Loss: 7.9869\n",
      "Iteration 588/1072, Loss: 8.0261\n",
      "Iteration 589/1072, Loss: 8.0050\n",
      "Iteration 590/1072, Loss: 8.0265\n",
      "Iteration 591/1072, Loss: 8.0434\n",
      "Iteration 592/1072, Loss: 8.0074\n",
      "Iteration 593/1072, Loss: 8.0327\n",
      "Iteration 594/1072, Loss: 8.0500\n",
      "Iteration 595/1072, Loss: 8.0979\n",
      "Iteration 596/1072, Loss: 8.0067\n",
      "Iteration 597/1072, Loss: 8.0458\n",
      "Iteration 598/1072, Loss: 8.0683\n",
      "Iteration 599/1072, Loss: 8.0434\n",
      "Iteration 600/1072, Loss: 8.0844\n",
      "Iteration 601/1072, Loss: 8.0082\n",
      "Iteration 602/1072, Loss: 8.0169\n",
      "Iteration 603/1072, Loss: 8.0746\n",
      "Iteration 604/1072, Loss: 8.0453\n",
      "Iteration 605/1072, Loss: 8.0699\n",
      "Iteration 606/1072, Loss: 8.0419\n",
      "Iteration 607/1072, Loss: 8.0745\n",
      "Iteration 608/1072, Loss: 8.0117\n",
      "Iteration 609/1072, Loss: 8.0425\n",
      "Iteration 610/1072, Loss: 8.0176\n",
      "Iteration 611/1072, Loss: 8.0319\n",
      "Iteration 612/1072, Loss: 8.0332\n",
      "Iteration 613/1072, Loss: 8.0370\n",
      "Iteration 614/1072, Loss: 8.0218\n",
      "Iteration 615/1072, Loss: 8.0401\n",
      "Iteration 616/1072, Loss: 8.0121\n",
      "Iteration 617/1072, Loss: 8.0568\n",
      "Iteration 618/1072, Loss: 7.9846\n",
      "Iteration 619/1072, Loss: 8.0149\n",
      "Iteration 620/1072, Loss: 8.0316\n",
      "Iteration 621/1072, Loss: 8.0182\n",
      "Iteration 622/1072, Loss: 8.0778\n",
      "Iteration 623/1072, Loss: 8.0844\n",
      "Iteration 624/1072, Loss: 8.0553\n",
      "Iteration 625/1072, Loss: 8.0567\n",
      "Iteration 626/1072, Loss: 8.0728\n",
      "Iteration 627/1072, Loss: 8.0394\n",
      "Iteration 628/1072, Loss: 8.1152\n",
      "Iteration 629/1072, Loss: 8.0860\n",
      "Iteration 630/1072, Loss: 8.0863\n",
      "Iteration 631/1072, Loss: 8.0438\n",
      "Iteration 632/1072, Loss: 8.0086\n",
      "Iteration 633/1072, Loss: 8.0565\n",
      "Iteration 634/1072, Loss: 8.0255\n",
      "Iteration 635/1072, Loss: 8.0431\n",
      "Iteration 636/1072, Loss: 8.0715\n",
      "Iteration 637/1072, Loss: 8.0435\n",
      "Iteration 638/1072, Loss: 8.0512\n",
      "Iteration 639/1072, Loss: 8.0436\n",
      "Iteration 640/1072, Loss: 8.0326\n",
      "Iteration 641/1072, Loss: 8.0064\n",
      "Iteration 642/1072, Loss: 7.9938\n",
      "Iteration 643/1072, Loss: 8.0415\n",
      "Iteration 644/1072, Loss: 8.0492\n",
      "Iteration 645/1072, Loss: 8.0024\n",
      "Iteration 646/1072, Loss: 7.9903\n",
      "Iteration 647/1072, Loss: 8.1290\n",
      "Iteration 648/1072, Loss: 8.0660\n",
      "Iteration 649/1072, Loss: 8.0222\n",
      "Iteration 650/1072, Loss: 8.0557\n",
      "Iteration 651/1072, Loss: 8.0555\n",
      "Iteration 652/1072, Loss: 8.0315\n",
      "Iteration 653/1072, Loss: 8.0291\n",
      "Iteration 654/1072, Loss: 8.0208\n",
      "Iteration 655/1072, Loss: 8.0010\n",
      "Iteration 656/1072, Loss: 8.0517\n",
      "Iteration 657/1072, Loss: 8.0584\n",
      "Iteration 658/1072, Loss: 8.0888\n",
      "Iteration 659/1072, Loss: 8.0644\n",
      "Iteration 660/1072, Loss: 8.0159\n",
      "Iteration 661/1072, Loss: 8.0981\n",
      "Iteration 662/1072, Loss: 8.0060\n",
      "Iteration 663/1072, Loss: 8.0092\n",
      "Iteration 664/1072, Loss: 7.9801\n",
      "Iteration 665/1072, Loss: 8.0451\n",
      "Iteration 666/1072, Loss: 8.0455\n",
      "Iteration 667/1072, Loss: 8.0514\n",
      "Iteration 668/1072, Loss: 7.9955\n",
      "Iteration 669/1072, Loss: 8.0400\n",
      "Iteration 670/1072, Loss: 8.0352\n",
      "Iteration 671/1072, Loss: 8.0065\n",
      "Iteration 672/1072, Loss: 8.0840\n",
      "Iteration 673/1072, Loss: 8.0472\n",
      "Iteration 674/1072, Loss: 8.0732\n",
      "Iteration 675/1072, Loss: 8.0672\n",
      "Iteration 676/1072, Loss: 8.0027\n",
      "Iteration 677/1072, Loss: 8.0235\n",
      "Iteration 678/1072, Loss: 7.9907\n",
      "Iteration 679/1072, Loss: 8.0738\n",
      "Iteration 680/1072, Loss: 8.0435\n",
      "Iteration 681/1072, Loss: 8.0632\n",
      "Iteration 682/1072, Loss: 8.0247\n",
      "Iteration 683/1072, Loss: 8.0738\n",
      "Iteration 684/1072, Loss: 8.0131\n",
      "Iteration 685/1072, Loss: 8.0988\n",
      "Iteration 686/1072, Loss: 8.0518\n",
      "Iteration 687/1072, Loss: 8.0450\n",
      "Iteration 688/1072, Loss: 7.9496\n",
      "Iteration 689/1072, Loss: 8.0809\n",
      "Iteration 690/1072, Loss: 8.0508\n",
      "Iteration 691/1072, Loss: 8.0370\n",
      "Iteration 692/1072, Loss: 8.0928\n",
      "Iteration 693/1072, Loss: 8.0565\n",
      "Iteration 694/1072, Loss: 8.0747\n",
      "Iteration 695/1072, Loss: 8.0642\n",
      "Iteration 696/1072, Loss: 8.0720\n",
      "Iteration 697/1072, Loss: 8.0613\n",
      "Iteration 698/1072, Loss: 8.0938\n",
      "Iteration 699/1072, Loss: 8.0352\n",
      "Iteration 700/1072, Loss: 8.0454\n",
      "Iteration 701/1072, Loss: 8.0534\n",
      "Iteration 702/1072, Loss: 8.0372\n",
      "Iteration 703/1072, Loss: 8.0517\n",
      "Iteration 704/1072, Loss: 8.0597\n",
      "Iteration 705/1072, Loss: 8.0971\n",
      "Iteration 706/1072, Loss: 8.0127\n",
      "Iteration 707/1072, Loss: 8.0437\n",
      "Iteration 708/1072, Loss: 8.0572\n",
      "Iteration 709/1072, Loss: 8.0761\n",
      "Iteration 710/1072, Loss: 8.0539\n",
      "Iteration 711/1072, Loss: 8.0888\n",
      "Iteration 712/1072, Loss: 8.0662\n",
      "Iteration 713/1072, Loss: 8.0406\n",
      "Iteration 714/1072, Loss: 8.0719\n",
      "Iteration 715/1072, Loss: 8.0358\n",
      "Iteration 716/1072, Loss: 8.0437\n",
      "Iteration 717/1072, Loss: 8.0435\n",
      "Iteration 718/1072, Loss: 8.0631\n",
      "Iteration 719/1072, Loss: 8.0408\n",
      "Iteration 720/1072, Loss: 8.0378\n",
      "Iteration 721/1072, Loss: 8.0237\n",
      "Iteration 722/1072, Loss: 8.0512\n",
      "Iteration 723/1072, Loss: 8.1006\n",
      "Iteration 724/1072, Loss: 8.0517\n",
      "Iteration 725/1072, Loss: 8.0316\n",
      "Iteration 726/1072, Loss: 8.0509\n",
      "Iteration 727/1072, Loss: 8.0336\n",
      "Iteration 728/1072, Loss: 8.1003\n",
      "Iteration 729/1072, Loss: 8.0633\n",
      "Iteration 730/1072, Loss: 8.0599\n",
      "Iteration 731/1072, Loss: 8.1135\n",
      "Iteration 732/1072, Loss: 8.0429\n",
      "Iteration 733/1072, Loss: 8.0258\n",
      "Iteration 734/1072, Loss: 8.0297\n",
      "Iteration 735/1072, Loss: 8.0155\n",
      "Iteration 736/1072, Loss: 8.0197\n",
      "Iteration 737/1072, Loss: 8.0296\n",
      "Iteration 738/1072, Loss: 8.0102\n",
      "Iteration 739/1072, Loss: 8.0465\n",
      "Iteration 740/1072, Loss: 8.0260\n",
      "Iteration 741/1072, Loss: 8.0902\n",
      "Iteration 742/1072, Loss: 8.0590\n",
      "Iteration 743/1072, Loss: 8.0086\n",
      "Iteration 744/1072, Loss: 8.0378\n",
      "Iteration 745/1072, Loss: 8.0500\n",
      "Iteration 746/1072, Loss: 8.0536\n",
      "Iteration 747/1072, Loss: 8.0404\n",
      "Iteration 748/1072, Loss: 8.1039\n",
      "Iteration 749/1072, Loss: 8.0158\n",
      "Iteration 750/1072, Loss: 8.0659\n",
      "Iteration 751/1072, Loss: 8.0656\n",
      "Iteration 752/1072, Loss: 8.0339\n",
      "Iteration 753/1072, Loss: 8.0830\n",
      "Iteration 754/1072, Loss: 8.0092\n",
      "Iteration 755/1072, Loss: 8.0608\n",
      "Iteration 756/1072, Loss: 8.0113\n",
      "Iteration 757/1072, Loss: 8.0770\n",
      "Iteration 758/1072, Loss: 8.0485\n",
      "Iteration 759/1072, Loss: 8.0770\n",
      "Iteration 760/1072, Loss: 8.0532\n",
      "Iteration 761/1072, Loss: 8.0478\n",
      "Iteration 762/1072, Loss: 8.0652\n",
      "Iteration 763/1072, Loss: 7.9951\n",
      "Iteration 764/1072, Loss: 8.0233\n",
      "Iteration 765/1072, Loss: 7.9905\n",
      "Iteration 766/1072, Loss: 8.0506\n",
      "Iteration 767/1072, Loss: 7.9670\n",
      "Iteration 768/1072, Loss: 8.0509\n",
      "Iteration 769/1072, Loss: 8.0587\n",
      "Iteration 770/1072, Loss: 8.0768\n",
      "Iteration 771/1072, Loss: 8.0522\n",
      "Iteration 772/1072, Loss: 8.0199\n",
      "Iteration 773/1072, Loss: 8.0339\n",
      "Iteration 774/1072, Loss: 8.0125\n",
      "Iteration 775/1072, Loss: 8.0074\n",
      "Iteration 776/1072, Loss: 8.0483\n",
      "Iteration 777/1072, Loss: 8.1022\n",
      "Iteration 778/1072, Loss: 8.0167\n",
      "Iteration 779/1072, Loss: 8.0668\n",
      "Iteration 780/1072, Loss: 8.0864\n",
      "Iteration 781/1072, Loss: 8.0410\n",
      "Iteration 782/1072, Loss: 8.0098\n",
      "Iteration 783/1072, Loss: 8.0558\n",
      "Iteration 784/1072, Loss: 8.0407\n",
      "Iteration 785/1072, Loss: 8.0538\n",
      "Iteration 786/1072, Loss: 8.0021\n",
      "Iteration 787/1072, Loss: 8.0732\n",
      "Iteration 788/1072, Loss: 8.0882\n",
      "Iteration 789/1072, Loss: 8.0527\n",
      "Iteration 790/1072, Loss: 8.0468\n",
      "Iteration 791/1072, Loss: 8.0173\n",
      "Iteration 792/1072, Loss: 8.0471\n",
      "Iteration 793/1072, Loss: 8.0097\n",
      "Iteration 794/1072, Loss: 7.9841\n",
      "Iteration 795/1072, Loss: 8.0856\n",
      "Iteration 796/1072, Loss: 8.0359\n",
      "Iteration 797/1072, Loss: 8.0701\n",
      "Iteration 798/1072, Loss: 8.0624\n",
      "Iteration 799/1072, Loss: 7.9973\n",
      "Iteration 800/1072, Loss: 8.0902\n",
      "Iteration 801/1072, Loss: 8.1295\n",
      "Iteration 802/1072, Loss: 7.9865\n",
      "Iteration 803/1072, Loss: 8.0226\n",
      "Iteration 804/1072, Loss: 8.0347\n",
      "Iteration 805/1072, Loss: 8.0488\n",
      "Iteration 806/1072, Loss: 8.0599\n",
      "Iteration 807/1072, Loss: 8.0250\n",
      "Iteration 808/1072, Loss: 8.0291\n",
      "Iteration 809/1072, Loss: 8.0650\n",
      "Iteration 810/1072, Loss: 8.0702\n",
      "Iteration 811/1072, Loss: 8.0972\n",
      "Iteration 812/1072, Loss: 8.0551\n",
      "Iteration 813/1072, Loss: 8.0715\n",
      "Iteration 814/1072, Loss: 8.0323\n",
      "Iteration 815/1072, Loss: 8.0282\n",
      "Iteration 816/1072, Loss: 8.0811\n",
      "Iteration 817/1072, Loss: 8.0239\n",
      "Iteration 818/1072, Loss: 8.0542\n",
      "Iteration 819/1072, Loss: 8.0495\n",
      "Iteration 820/1072, Loss: 8.0019\n",
      "Iteration 821/1072, Loss: 8.0372\n",
      "Iteration 822/1072, Loss: 8.0250\n",
      "Iteration 823/1072, Loss: 8.0702\n",
      "Iteration 824/1072, Loss: 8.0383\n",
      "Iteration 825/1072, Loss: 8.0171\n",
      "Iteration 826/1072, Loss: 8.0397\n",
      "Iteration 827/1072, Loss: 8.0501\n",
      "Iteration 828/1072, Loss: 8.0190\n",
      "Iteration 829/1072, Loss: 8.0234\n",
      "Iteration 830/1072, Loss: 8.0350\n",
      "Iteration 831/1072, Loss: 8.0131\n",
      "Iteration 832/1072, Loss: 8.0295\n",
      "Iteration 833/1072, Loss: 8.0694\n",
      "Iteration 834/1072, Loss: 8.0527\n",
      "Iteration 835/1072, Loss: 8.0428\n",
      "Iteration 836/1072, Loss: 8.0972\n",
      "Iteration 837/1072, Loss: 8.0178\n",
      "Iteration 838/1072, Loss: 8.0276\n",
      "Iteration 839/1072, Loss: 8.0317\n",
      "Iteration 840/1072, Loss: 8.0263\n",
      "Iteration 841/1072, Loss: 8.0747\n",
      "Iteration 842/1072, Loss: 8.0054\n",
      "Iteration 843/1072, Loss: 8.0460\n",
      "Iteration 844/1072, Loss: 8.0258\n",
      "Iteration 845/1072, Loss: 8.0595\n",
      "Iteration 846/1072, Loss: 8.0622\n",
      "Iteration 847/1072, Loss: 8.0949\n",
      "Iteration 848/1072, Loss: 7.9605\n",
      "Iteration 849/1072, Loss: 8.0636\n",
      "Iteration 850/1072, Loss: 8.0160\n",
      "Iteration 851/1072, Loss: 8.0305\n",
      "Iteration 852/1072, Loss: 8.0298\n",
      "Iteration 853/1072, Loss: 8.1150\n",
      "Iteration 854/1072, Loss: 8.0426\n",
      "Iteration 855/1072, Loss: 8.0758\n",
      "Iteration 856/1072, Loss: 8.0098\n",
      "Iteration 857/1072, Loss: 8.0808\n",
      "Iteration 858/1072, Loss: 7.9604\n",
      "Iteration 859/1072, Loss: 8.0543\n",
      "Iteration 860/1072, Loss: 8.0535\n",
      "Iteration 861/1072, Loss: 8.0278\n",
      "Iteration 862/1072, Loss: 8.0394\n",
      "Iteration 863/1072, Loss: 8.0506\n",
      "Iteration 864/1072, Loss: 8.0061\n",
      "Iteration 865/1072, Loss: 8.0414\n",
      "Iteration 866/1072, Loss: 8.0506\n",
      "Iteration 867/1072, Loss: 8.0565\n",
      "Iteration 868/1072, Loss: 8.0755\n",
      "Iteration 869/1072, Loss: 8.0490\n",
      "Iteration 870/1072, Loss: 8.0274\n",
      "Iteration 871/1072, Loss: 8.0094\n",
      "Iteration 872/1072, Loss: 8.1534\n",
      "Iteration 873/1072, Loss: 8.0164\n",
      "Iteration 874/1072, Loss: 7.9952\n",
      "Iteration 875/1072, Loss: 8.0984\n",
      "Iteration 876/1072, Loss: 8.0255\n",
      "Iteration 877/1072, Loss: 8.0530\n",
      "Iteration 878/1072, Loss: 8.0732\n",
      "Iteration 879/1072, Loss: 7.9827\n",
      "Iteration 880/1072, Loss: 8.0681\n",
      "Iteration 881/1072, Loss: 8.0582\n",
      "Iteration 882/1072, Loss: 8.0767\n",
      "Iteration 883/1072, Loss: 8.0496\n",
      "Iteration 884/1072, Loss: 8.0260\n",
      "Iteration 885/1072, Loss: 7.9749\n",
      "Iteration 886/1072, Loss: 8.0295\n",
      "Iteration 887/1072, Loss: 8.0482\n",
      "Iteration 888/1072, Loss: 8.0926\n",
      "Iteration 889/1072, Loss: 8.0992\n",
      "Iteration 890/1072, Loss: 8.0816\n",
      "Iteration 891/1072, Loss: 7.9975\n",
      "Iteration 892/1072, Loss: 8.0695\n",
      "Iteration 893/1072, Loss: 8.0155\n",
      "Iteration 894/1072, Loss: 8.0426\n",
      "Iteration 895/1072, Loss: 8.0184\n",
      "Iteration 896/1072, Loss: 8.0390\n",
      "Iteration 897/1072, Loss: 8.0445\n",
      "Iteration 898/1072, Loss: 7.9994\n",
      "Iteration 899/1072, Loss: 8.0174\n",
      "Iteration 900/1072, Loss: 8.0226\n",
      "Iteration 901/1072, Loss: 8.0077\n",
      "Iteration 902/1072, Loss: 8.0554\n",
      "Iteration 903/1072, Loss: 8.0331\n",
      "Iteration 904/1072, Loss: 8.0730\n",
      "Iteration 905/1072, Loss: 8.0402\n",
      "Iteration 906/1072, Loss: 8.0463\n",
      "Iteration 907/1072, Loss: 8.0522\n",
      "Iteration 908/1072, Loss: 8.0437\n",
      "Iteration 909/1072, Loss: 8.0684\n",
      "Iteration 910/1072, Loss: 7.9971\n",
      "Iteration 911/1072, Loss: 8.0636\n",
      "Iteration 912/1072, Loss: 8.0232\n",
      "Iteration 913/1072, Loss: 8.0073\n",
      "Iteration 914/1072, Loss: 7.9902\n",
      "Iteration 915/1072, Loss: 8.0194\n",
      "Iteration 916/1072, Loss: 8.0714\n",
      "Iteration 917/1072, Loss: 8.0454\n",
      "Iteration 918/1072, Loss: 8.0878\n",
      "Iteration 919/1072, Loss: 8.0446\n",
      "Iteration 920/1072, Loss: 8.0857\n",
      "Iteration 921/1072, Loss: 8.0995\n",
      "Iteration 922/1072, Loss: 8.0556\n",
      "Iteration 923/1072, Loss: 8.0636\n",
      "Iteration 924/1072, Loss: 8.0110\n",
      "Iteration 925/1072, Loss: 8.0411\n",
      "Iteration 926/1072, Loss: 8.0450\n",
      "Iteration 927/1072, Loss: 8.0349\n",
      "Iteration 928/1072, Loss: 8.0498\n",
      "Iteration 929/1072, Loss: 8.0137\n",
      "Iteration 930/1072, Loss: 8.0019\n",
      "Iteration 931/1072, Loss: 8.0432\n",
      "Iteration 932/1072, Loss: 8.0197\n",
      "Iteration 933/1072, Loss: 8.0388\n",
      "Iteration 934/1072, Loss: 8.0060\n",
      "Iteration 935/1072, Loss: 7.9944\n",
      "Iteration 936/1072, Loss: 8.0454\n",
      "Iteration 937/1072, Loss: 8.0711\n",
      "Iteration 938/1072, Loss: 8.0148\n",
      "Iteration 939/1072, Loss: 7.9874\n",
      "Iteration 940/1072, Loss: 8.0347\n",
      "Iteration 941/1072, Loss: 8.0033\n",
      "Iteration 942/1072, Loss: 8.0189\n",
      "Iteration 943/1072, Loss: 8.0851\n",
      "Iteration 944/1072, Loss: 7.9845\n",
      "Iteration 945/1072, Loss: 8.1155\n",
      "Iteration 946/1072, Loss: 8.0513\n",
      "Iteration 947/1072, Loss: 8.0455\n",
      "Iteration 948/1072, Loss: 8.0194\n",
      "Iteration 949/1072, Loss: 8.0429\n",
      "Iteration 950/1072, Loss: 7.9746\n",
      "Iteration 951/1072, Loss: 8.0651\n",
      "Iteration 952/1072, Loss: 8.0277\n",
      "Iteration 953/1072, Loss: 8.0188\n",
      "Iteration 954/1072, Loss: 8.0213\n",
      "Iteration 955/1072, Loss: 8.0246\n",
      "Iteration 956/1072, Loss: 8.0615\n",
      "Iteration 957/1072, Loss: 7.9957\n",
      "Iteration 958/1072, Loss: 7.9798\n",
      "Iteration 959/1072, Loss: 8.0854\n",
      "Iteration 960/1072, Loss: 8.0522\n",
      "Iteration 961/1072, Loss: 8.0447\n",
      "Iteration 962/1072, Loss: 8.0298\n",
      "Iteration 963/1072, Loss: 8.0275\n",
      "Iteration 964/1072, Loss: 8.0647\n",
      "Iteration 965/1072, Loss: 8.0537\n",
      "Iteration 966/1072, Loss: 8.0358\n",
      "Iteration 967/1072, Loss: 7.9570\n",
      "Iteration 968/1072, Loss: 8.0271\n",
      "Iteration 969/1072, Loss: 8.0700\n",
      "Iteration 970/1072, Loss: 7.9826\n",
      "Iteration 971/1072, Loss: 8.0537\n",
      "Iteration 972/1072, Loss: 8.0277\n",
      "Iteration 973/1072, Loss: 8.0305\n",
      "Iteration 974/1072, Loss: 8.0098\n",
      "Iteration 975/1072, Loss: 8.1302\n",
      "Iteration 976/1072, Loss: 8.0577\n",
      "Iteration 977/1072, Loss: 8.0842\n",
      "Iteration 978/1072, Loss: 8.0658\n",
      "Iteration 979/1072, Loss: 8.0532\n",
      "Iteration 980/1072, Loss: 8.0686\n",
      "Iteration 981/1072, Loss: 8.0710\n",
      "Iteration 982/1072, Loss: 7.9951\n",
      "Iteration 983/1072, Loss: 8.0482\n",
      "Iteration 984/1072, Loss: 8.0443\n",
      "Iteration 985/1072, Loss: 8.0430\n",
      "Iteration 986/1072, Loss: 8.0642\n",
      "Iteration 987/1072, Loss: 8.0472\n",
      "Iteration 988/1072, Loss: 8.0057\n",
      "Iteration 989/1072, Loss: 8.0874\n",
      "Iteration 990/1072, Loss: 8.0942\n",
      "Iteration 991/1072, Loss: 8.0392\n",
      "Iteration 992/1072, Loss: 8.0748\n",
      "Iteration 993/1072, Loss: 7.9960\n",
      "Iteration 994/1072, Loss: 8.0057\n",
      "Iteration 995/1072, Loss: 8.0955\n",
      "Iteration 996/1072, Loss: 8.0425\n",
      "Iteration 997/1072, Loss: 8.0375\n",
      "Iteration 998/1072, Loss: 7.9886\n",
      "Iteration 999/1072, Loss: 8.0625\n",
      "Iteration 1000/1072, Loss: 8.0406\n",
      "Iteration 1001/1072, Loss: 8.0578\n",
      "Iteration 1002/1072, Loss: 8.0440\n",
      "Iteration 1003/1072, Loss: 8.0515\n",
      "Iteration 1004/1072, Loss: 8.0266\n",
      "Iteration 1005/1072, Loss: 8.0305\n",
      "Iteration 1006/1072, Loss: 8.0586\n",
      "Iteration 1007/1072, Loss: 8.0722\n",
      "Iteration 1008/1072, Loss: 8.0818\n",
      "Iteration 1009/1072, Loss: 8.0193\n",
      "Iteration 1010/1072, Loss: 8.0519\n",
      "Iteration 1011/1072, Loss: 8.0703\n",
      "Iteration 1012/1072, Loss: 8.0506\n",
      "Iteration 1013/1072, Loss: 8.0779\n",
      "Iteration 1014/1072, Loss: 8.0317\n",
      "Iteration 1015/1072, Loss: 8.0208\n",
      "Iteration 1016/1072, Loss: 8.0470\n",
      "Iteration 1017/1072, Loss: 8.0457\n",
      "Iteration 1018/1072, Loss: 8.0666\n",
      "Iteration 1019/1072, Loss: 8.1042\n",
      "Iteration 1020/1072, Loss: 8.0144\n",
      "Iteration 1021/1072, Loss: 8.0346\n",
      "Iteration 1022/1072, Loss: 7.9914\n",
      "Iteration 1023/1072, Loss: 8.0247\n",
      "Iteration 1024/1072, Loss: 8.0305\n",
      "Iteration 1025/1072, Loss: 8.0895\n",
      "Iteration 1026/1072, Loss: 8.0441\n",
      "Iteration 1027/1072, Loss: 8.0469\n",
      "Iteration 1028/1072, Loss: 8.0159\n",
      "Iteration 1029/1072, Loss: 8.0906\n",
      "Iteration 1030/1072, Loss: 8.0685\n",
      "Iteration 1031/1072, Loss: 8.0725\n",
      "Iteration 1032/1072, Loss: 8.0872\n",
      "Iteration 1033/1072, Loss: 8.0169\n",
      "Iteration 1034/1072, Loss: 8.0238\n",
      "Iteration 1035/1072, Loss: 8.0296\n",
      "Iteration 1036/1072, Loss: 8.0452\n",
      "Iteration 1037/1072, Loss: 8.0617\n",
      "Iteration 1038/1072, Loss: 8.0103\n",
      "Iteration 1039/1072, Loss: 8.0292\n",
      "Iteration 1040/1072, Loss: 8.0068\n",
      "Iteration 1041/1072, Loss: 8.0204\n",
      "Iteration 1042/1072, Loss: 7.9969\n",
      "Iteration 1043/1072, Loss: 8.0513\n",
      "Iteration 1044/1072, Loss: 8.0211\n",
      "Iteration 1045/1072, Loss: 8.1123\n",
      "Iteration 1046/1072, Loss: 8.0881\n",
      "Iteration 1047/1072, Loss: 8.0436\n",
      "Iteration 1048/1072, Loss: 8.0723\n",
      "Iteration 1049/1072, Loss: 8.0593\n",
      "Iteration 1050/1072, Loss: 8.0608\n",
      "Iteration 1051/1072, Loss: 8.0537\n",
      "Iteration 1052/1072, Loss: 7.9597\n",
      "Iteration 1053/1072, Loss: 8.0262\n",
      "Iteration 1054/1072, Loss: 8.0681\n",
      "Iteration 1055/1072, Loss: 8.0089\n",
      "Iteration 1056/1072, Loss: 8.0266\n",
      "Iteration 1057/1072, Loss: 7.9849\n",
      "Iteration 1058/1072, Loss: 7.9782\n",
      "Iteration 1059/1072, Loss: 8.0736\n",
      "Iteration 1060/1072, Loss: 8.0783\n",
      "Iteration 1061/1072, Loss: 8.0589\n",
      "Iteration 1062/1072, Loss: 8.0300\n",
      "Iteration 1063/1072, Loss: 8.0070\n",
      "Iteration 1064/1072, Loss: 7.9941\n",
      "Iteration 1065/1072, Loss: 8.0701\n",
      "Iteration 1066/1072, Loss: 8.0146\n",
      "Iteration 1067/1072, Loss: 8.0696\n",
      "Iteration 1068/1072, Loss: 8.0250\n",
      "Iteration 1069/1072, Loss: 8.0281\n",
      "Iteration 1070/1072, Loss: 8.0322\n",
      "Iteration 1071/1072, Loss: 8.0391\n",
      "Iteration 1072/1072, Loss: 8.0223\n",
      "Epoch 1/10, Loss: 8.0498\n",
      "Checkpoint saved at checkpoints/efficientnet-b4_epoch_1.pth\n",
      "Validation Accuracy: 0.07%\n",
      "Iteration 1/1072, Loss: 8.0578\n",
      "Iteration 2/1072, Loss: 8.0413\n",
      "Iteration 3/1072, Loss: 7.9653\n",
      "Iteration 4/1072, Loss: 8.0320\n",
      "Iteration 5/1072, Loss: 7.9551\n",
      "Iteration 6/1072, Loss: 7.9969\n",
      "Iteration 7/1072, Loss: 7.9888\n",
      "Iteration 8/1072, Loss: 8.0081\n",
      "Iteration 9/1072, Loss: 8.0323\n",
      "Iteration 10/1072, Loss: 7.9590\n",
      "Iteration 11/1072, Loss: 7.9875\n",
      "Iteration 12/1072, Loss: 8.0337\n",
      "Iteration 13/1072, Loss: 7.9999\n",
      "Iteration 14/1072, Loss: 7.9869\n",
      "Iteration 15/1072, Loss: 7.9973\n",
      "Iteration 16/1072, Loss: 7.9768\n",
      "Iteration 17/1072, Loss: 8.0137\n",
      "Iteration 18/1072, Loss: 7.9859\n",
      "Iteration 19/1072, Loss: 8.0197\n",
      "Iteration 20/1072, Loss: 8.0793\n",
      "Iteration 21/1072, Loss: 7.9774\n",
      "Iteration 22/1072, Loss: 7.9557\n",
      "Iteration 23/1072, Loss: 8.0705\n",
      "Iteration 24/1072, Loss: 8.0237\n",
      "Iteration 25/1072, Loss: 8.0291\n",
      "Iteration 26/1072, Loss: 8.0115\n",
      "Iteration 27/1072, Loss: 7.9959\n",
      "Iteration 28/1072, Loss: 8.0175\n",
      "Iteration 29/1072, Loss: 7.9933\n",
      "Iteration 30/1072, Loss: 8.0316\n",
      "Iteration 31/1072, Loss: 7.9911\n",
      "Iteration 32/1072, Loss: 8.0246\n",
      "Iteration 33/1072, Loss: 8.0111\n",
      "Iteration 34/1072, Loss: 8.0002\n",
      "Iteration 35/1072, Loss: 8.0605\n",
      "Iteration 36/1072, Loss: 7.9995\n",
      "Iteration 37/1072, Loss: 7.9850\n",
      "Iteration 38/1072, Loss: 8.0309\n",
      "Iteration 39/1072, Loss: 7.9738\n",
      "Iteration 40/1072, Loss: 7.9840\n",
      "Iteration 41/1072, Loss: 7.9872\n",
      "Iteration 42/1072, Loss: 7.9736\n",
      "Iteration 43/1072, Loss: 8.0618\n",
      "Iteration 44/1072, Loss: 7.9458\n",
      "Iteration 45/1072, Loss: 7.9587\n",
      "Iteration 46/1072, Loss: 8.0328\n",
      "Iteration 47/1072, Loss: 8.0094\n",
      "Iteration 48/1072, Loss: 8.0198\n",
      "Iteration 49/1072, Loss: 8.0195\n",
      "Iteration 50/1072, Loss: 7.9988\n",
      "Iteration 51/1072, Loss: 7.9866\n",
      "Iteration 52/1072, Loss: 7.9917\n",
      "Iteration 53/1072, Loss: 7.9330\n",
      "Iteration 54/1072, Loss: 8.0107\n",
      "Iteration 55/1072, Loss: 7.9826\n",
      "Iteration 56/1072, Loss: 8.0069\n",
      "Iteration 57/1072, Loss: 8.0425\n",
      "Iteration 58/1072, Loss: 8.0819\n",
      "Iteration 59/1072, Loss: 7.9822\n",
      "Iteration 60/1072, Loss: 7.9681\n",
      "Iteration 61/1072, Loss: 8.0162\n",
      "Iteration 62/1072, Loss: 7.9920\n",
      "Iteration 63/1072, Loss: 8.0164\n",
      "Iteration 64/1072, Loss: 8.0418\n",
      "Iteration 65/1072, Loss: 7.9959\n",
      "Iteration 66/1072, Loss: 8.0215\n",
      "Iteration 67/1072, Loss: 7.9884\n",
      "Iteration 68/1072, Loss: 8.0188\n",
      "Iteration 69/1072, Loss: 8.0089\n",
      "Iteration 70/1072, Loss: 8.0652\n",
      "Iteration 71/1072, Loss: 8.0712\n",
      "Iteration 72/1072, Loss: 7.9971\n",
      "Iteration 73/1072, Loss: 8.0610\n",
      "Iteration 74/1072, Loss: 8.0420\n",
      "Iteration 75/1072, Loss: 8.0187\n",
      "Iteration 76/1072, Loss: 7.9787\n",
      "Iteration 77/1072, Loss: 8.0525\n",
      "Iteration 78/1072, Loss: 8.0345\n",
      "Iteration 79/1072, Loss: 7.9958\n",
      "Iteration 80/1072, Loss: 7.9909\n",
      "Iteration 81/1072, Loss: 8.0990\n",
      "Iteration 82/1072, Loss: 8.0279\n",
      "Iteration 83/1072, Loss: 8.0531\n",
      "Iteration 84/1072, Loss: 7.9919\n",
      "Iteration 85/1072, Loss: 8.0631\n",
      "Iteration 86/1072, Loss: 7.9826\n",
      "Iteration 87/1072, Loss: 7.9979\n",
      "Iteration 88/1072, Loss: 8.0543\n",
      "Iteration 89/1072, Loss: 8.0300\n",
      "Iteration 90/1072, Loss: 7.9714\n",
      "Iteration 91/1072, Loss: 8.0363\n",
      "Iteration 92/1072, Loss: 8.0277\n",
      "Iteration 93/1072, Loss: 7.9850\n",
      "Iteration 94/1072, Loss: 7.9593\n",
      "Iteration 95/1072, Loss: 7.9647\n",
      "Iteration 96/1072, Loss: 8.0087\n",
      "Iteration 97/1072, Loss: 7.9811\n",
      "Iteration 98/1072, Loss: 7.9828\n",
      "Iteration 99/1072, Loss: 8.0486\n",
      "Iteration 100/1072, Loss: 8.0889\n",
      "Iteration 101/1072, Loss: 8.0360\n",
      "Iteration 102/1072, Loss: 7.9690\n",
      "Iteration 103/1072, Loss: 7.9783\n",
      "Iteration 104/1072, Loss: 8.0453\n",
      "Iteration 105/1072, Loss: 8.0832\n",
      "Iteration 106/1072, Loss: 7.9596\n",
      "Iteration 107/1072, Loss: 7.9839\n",
      "Iteration 108/1072, Loss: 8.0235\n",
      "Iteration 109/1072, Loss: 7.9808\n",
      "Iteration 110/1072, Loss: 8.0074\n",
      "Iteration 111/1072, Loss: 7.9996\n",
      "Iteration 112/1072, Loss: 8.0185\n",
      "Iteration 113/1072, Loss: 7.9968\n",
      "Iteration 114/1072, Loss: 8.0415\n",
      "Iteration 115/1072, Loss: 7.9631\n",
      "Iteration 116/1072, Loss: 8.0389\n",
      "Iteration 117/1072, Loss: 8.0393\n",
      "Iteration 118/1072, Loss: 8.0306\n",
      "Iteration 119/1072, Loss: 8.0159\n",
      "Iteration 120/1072, Loss: 8.0026\n",
      "Iteration 121/1072, Loss: 7.9972\n",
      "Iteration 122/1072, Loss: 8.0508\n",
      "Iteration 123/1072, Loss: 7.9828\n",
      "Iteration 124/1072, Loss: 7.9868\n",
      "Iteration 125/1072, Loss: 8.0442\n",
      "Iteration 126/1072, Loss: 7.9440\n",
      "Iteration 127/1072, Loss: 8.0264\n",
      "Iteration 128/1072, Loss: 8.0289\n",
      "Iteration 129/1072, Loss: 7.9665\n",
      "Iteration 130/1072, Loss: 8.0002\n",
      "Iteration 131/1072, Loss: 7.9787\n",
      "Iteration 132/1072, Loss: 8.0267\n",
      "Iteration 133/1072, Loss: 8.0157\n",
      "Iteration 134/1072, Loss: 7.9937\n",
      "Iteration 135/1072, Loss: 7.9813\n",
      "Iteration 136/1072, Loss: 8.0420\n",
      "Iteration 137/1072, Loss: 8.0454\n",
      "Iteration 138/1072, Loss: 7.9754\n",
      "Iteration 139/1072, Loss: 7.9434\n",
      "Iteration 140/1072, Loss: 8.0099\n",
      "Iteration 141/1072, Loss: 8.0094\n",
      "Iteration 142/1072, Loss: 8.0506\n",
      "Iteration 143/1072, Loss: 8.0202\n",
      "Iteration 144/1072, Loss: 8.0546\n",
      "Iteration 145/1072, Loss: 8.0286\n",
      "Iteration 146/1072, Loss: 8.0098\n",
      "Iteration 147/1072, Loss: 7.9888\n",
      "Iteration 148/1072, Loss: 8.0368\n",
      "Iteration 149/1072, Loss: 8.0094\n",
      "Iteration 150/1072, Loss: 8.0249\n",
      "Iteration 151/1072, Loss: 8.0635\n",
      "Iteration 152/1072, Loss: 8.0404\n",
      "Iteration 153/1072, Loss: 7.9789\n",
      "Iteration 154/1072, Loss: 8.0006\n",
      "Iteration 155/1072, Loss: 7.9854\n",
      "Iteration 156/1072, Loss: 8.0550\n",
      "Iteration 157/1072, Loss: 8.0027\n",
      "Iteration 158/1072, Loss: 8.0349\n",
      "Iteration 159/1072, Loss: 8.0109\n",
      "Iteration 160/1072, Loss: 8.0316\n",
      "Iteration 161/1072, Loss: 8.0256\n",
      "Iteration 162/1072, Loss: 8.0171\n",
      "Iteration 163/1072, Loss: 8.0190\n",
      "Iteration 164/1072, Loss: 8.0130\n",
      "Iteration 165/1072, Loss: 7.9766\n",
      "Iteration 166/1072, Loss: 8.0393\n",
      "Iteration 167/1072, Loss: 8.0185\n",
      "Iteration 168/1072, Loss: 7.9804\n",
      "Iteration 169/1072, Loss: 8.0408\n",
      "Iteration 170/1072, Loss: 7.9774\n",
      "Iteration 171/1072, Loss: 7.9837\n",
      "Iteration 172/1072, Loss: 7.9989\n",
      "Iteration 173/1072, Loss: 7.9443\n",
      "Iteration 174/1072, Loss: 7.9915\n",
      "Iteration 175/1072, Loss: 7.9135\n",
      "Iteration 176/1072, Loss: 8.0365\n",
      "Iteration 177/1072, Loss: 7.9243\n",
      "Iteration 178/1072, Loss: 7.9987\n",
      "Iteration 179/1072, Loss: 7.9727\n",
      "Iteration 180/1072, Loss: 7.9601\n",
      "Iteration 181/1072, Loss: 7.9940\n",
      "Iteration 182/1072, Loss: 7.9690\n",
      "Iteration 183/1072, Loss: 8.0165\n",
      "Iteration 184/1072, Loss: 8.0641\n",
      "Iteration 185/1072, Loss: 8.0191\n",
      "Iteration 186/1072, Loss: 7.9564\n",
      "Iteration 187/1072, Loss: 8.0447\n",
      "Iteration 188/1072, Loss: 7.9749\n",
      "Iteration 189/1072, Loss: 7.9988\n",
      "Iteration 190/1072, Loss: 7.9734\n",
      "Iteration 191/1072, Loss: 8.0044\n",
      "Iteration 192/1072, Loss: 8.0334\n",
      "Iteration 193/1072, Loss: 8.0833\n",
      "Iteration 194/1072, Loss: 8.0049\n",
      "Iteration 195/1072, Loss: 8.0048\n",
      "Iteration 196/1072, Loss: 8.0210\n",
      "Iteration 197/1072, Loss: 8.0294\n",
      "Iteration 242/1072, Loss: 8.0014\n",
      "Iteration 243/1072, Loss: 8.0118\n",
      "Iteration 244/1072, Loss: 8.1080\n",
      "Iteration 245/1072, Loss: 8.0166\n",
      "Iteration 246/1072, Loss: 7.9789\n",
      "Iteration 247/1072, Loss: 7.9591\n",
      "Iteration 248/1072, Loss: 7.9971\n",
      "Iteration 249/1072, Loss: 7.9720\n",
      "Iteration 250/1072, Loss: 8.0010\n",
      "Iteration 251/1072, Loss: 8.0589\n",
      "Iteration 252/1072, Loss: 8.0241\n",
      "Iteration 253/1072, Loss: 8.0023\n",
      "Iteration 254/1072, Loss: 8.0439\n",
      "Iteration 255/1072, Loss: 7.9964\n",
      "Iteration 256/1072, Loss: 7.9917\n",
      "Iteration 257/1072, Loss: 8.0305\n",
      "Iteration 258/1072, Loss: 8.0039\n",
      "Iteration 259/1072, Loss: 7.9480\n",
      "Iteration 260/1072, Loss: 8.0713\n",
      "Iteration 261/1072, Loss: 8.0781\n",
      "Iteration 262/1072, Loss: 8.0248\n",
      "Iteration 263/1072, Loss: 8.0665\n",
      "Iteration 264/1072, Loss: 7.9508\n",
      "Iteration 265/1072, Loss: 8.0721\n",
      "Iteration 266/1072, Loss: 8.0166\n",
      "Iteration 267/1072, Loss: 8.0195\n",
      "Iteration 268/1072, Loss: 8.0233\n",
      "Iteration 313/1072, Loss: 8.0407\n",
      "Iteration 314/1072, Loss: 8.0285\n",
      "Iteration 315/1072, Loss: 7.9503\n",
      "Iteration 316/1072, Loss: 7.9848\n",
      "Iteration 317/1072, Loss: 7.9924\n",
      "Iteration 318/1072, Loss: 8.0049\n",
      "Iteration 319/1072, Loss: 7.9802\n",
      "Iteration 320/1072, Loss: 8.0670\n",
      "Iteration 321/1072, Loss: 7.9901\n",
      "Iteration 322/1072, Loss: 8.0246\n",
      "Iteration 323/1072, Loss: 7.9806\n",
      "Iteration 324/1072, Loss: 7.9768\n",
      "Iteration 325/1072, Loss: 8.0203\n",
      "Iteration 326/1072, Loss: 8.0370\n",
      "Iteration 327/1072, Loss: 7.9628\n",
      "Iteration 328/1072, Loss: 7.9760\n",
      "Iteration 329/1072, Loss: 8.0006\n",
      "Iteration 330/1072, Loss: 8.0261\n",
      "Iteration 331/1072, Loss: 8.0131\n",
      "Iteration 332/1072, Loss: 7.9872\n",
      "Iteration 333/1072, Loss: 7.9874\n",
      "Iteration 334/1072, Loss: 8.0336\n",
      "Iteration 335/1072, Loss: 7.9491\n",
      "Iteration 336/1072, Loss: 7.9854\n",
      "Iteration 337/1072, Loss: 8.0291\n",
      "Iteration 338/1072, Loss: 8.0318\n",
      "Iteration 339/1072, Loss: 7.9925\n",
      "Iteration 340/1072, Loss: 7.9621\n",
      "Iteration 341/1072, Loss: 8.0101\n",
      "Iteration 342/1072, Loss: 7.9812\n",
      "Iteration 343/1072, Loss: 8.0424\n",
      "Iteration 344/1072, Loss: 7.9440\n",
      "Iteration 345/1072, Loss: 8.0092\n",
      "Iteration 346/1072, Loss: 7.9584\n",
      "Iteration 347/1072, Loss: 7.9893\n",
      "Iteration 348/1072, Loss: 8.0846\n",
      "Iteration 349/1072, Loss: 7.9940\n",
      "Iteration 350/1072, Loss: 7.9991\n",
      "Iteration 351/1072, Loss: 8.0465\n",
      "Iteration 352/1072, Loss: 7.9858\n",
      "Iteration 353/1072, Loss: 8.0015\n",
      "Iteration 354/1072, Loss: 8.0424\n",
      "Iteration 355/1072, Loss: 7.9710\n",
      "Iteration 356/1072, Loss: 8.0064\n",
      "Iteration 357/1072, Loss: 7.9450\n",
      "Iteration 358/1072, Loss: 7.9526\n",
      "Iteration 359/1072, Loss: 8.0640\n",
      "Iteration 360/1072, Loss: 8.0259\n",
      "Iteration 361/1072, Loss: 8.0224\n",
      "Iteration 362/1072, Loss: 8.0152\n",
      "Iteration 363/1072, Loss: 7.9756\n",
      "Iteration 364/1072, Loss: 7.9889\n",
      "Iteration 365/1072, Loss: 7.9955\n",
      "Iteration 366/1072, Loss: 7.9997\n",
      "Iteration 367/1072, Loss: 7.9387\n",
      "Iteration 368/1072, Loss: 7.9924\n",
      "Iteration 369/1072, Loss: 7.9713\n",
      "Iteration 370/1072, Loss: 7.9565\n",
      "Iteration 371/1072, Loss: 8.0286\n",
      "Iteration 372/1072, Loss: 8.0415\n",
      "Iteration 373/1072, Loss: 8.0015\n",
      "Iteration 374/1072, Loss: 7.9820\n",
      "Iteration 375/1072, Loss: 7.9851\n",
      "Iteration 376/1072, Loss: 8.0455\n",
      "Iteration 377/1072, Loss: 8.0090\n",
      "Iteration 378/1072, Loss: 8.0188\n",
      "Iteration 379/1072, Loss: 7.9496\n",
      "Iteration 380/1072, Loss: 7.9866\n",
      "Iteration 381/1072, Loss: 7.9979\n",
      "Iteration 382/1072, Loss: 7.9728\n",
      "Iteration 383/1072, Loss: 7.9807\n",
      "Iteration 384/1072, Loss: 8.0063\n",
      "Iteration 385/1072, Loss: 7.9574\n",
      "Iteration 386/1072, Loss: 7.9739\n",
      "Iteration 387/1072, Loss: 7.9974\n",
      "Iteration 388/1072, Loss: 7.9937\n",
      "Iteration 389/1072, Loss: 7.9670\n",
      "Iteration 390/1072, Loss: 8.0165\n",
      "Iteration 391/1072, Loss: 8.0108\n",
      "Iteration 392/1072, Loss: 8.0002\n",
      "Iteration 393/1072, Loss: 7.9852\n",
      "Iteration 394/1072, Loss: 7.9550\n",
      "Iteration 395/1072, Loss: 7.9957\n",
      "Iteration 396/1072, Loss: 8.0024\n",
      "Iteration 397/1072, Loss: 8.0126\n",
      "Iteration 398/1072, Loss: 8.0426\n",
      "Iteration 399/1072, Loss: 8.0446\n",
      "Iteration 400/1072, Loss: 7.9887\n",
      "Iteration 401/1072, Loss: 7.9910\n",
      "Iteration 402/1072, Loss: 8.0099\n",
      "Iteration 403/1072, Loss: 8.0354\n",
      "Iteration 404/1072, Loss: 7.9860\n",
      "Iteration 405/1072, Loss: 8.0330\n",
      "Iteration 406/1072, Loss: 8.0156\n",
      "Iteration 407/1072, Loss: 8.0095\n",
      "Iteration 408/1072, Loss: 8.0268\n",
      "Iteration 409/1072, Loss: 7.9994\n",
      "Iteration 410/1072, Loss: 8.0634\n",
      "Iteration 411/1072, Loss: 7.9979\n",
      "Iteration 412/1072, Loss: 8.0058\n",
      "Iteration 413/1072, Loss: 7.9928\n",
      "Iteration 414/1072, Loss: 7.9930\n",
      "Iteration 415/1072, Loss: 8.0095\n",
      "Iteration 416/1072, Loss: 8.0290\n",
      "Iteration 417/1072, Loss: 8.0537\n",
      "Iteration 418/1072, Loss: 8.0071\n",
      "Iteration 419/1072, Loss: 8.0239\n",
      "Iteration 420/1072, Loss: 7.9994\n",
      "Iteration 421/1072, Loss: 8.0167\n",
      "Iteration 422/1072, Loss: 7.9786\n",
      "Iteration 423/1072, Loss: 8.0243\n",
      "Iteration 424/1072, Loss: 7.9997\n",
      "Iteration 425/1072, Loss: 7.9410\n",
      "Iteration 426/1072, Loss: 8.0140\n",
      "Iteration 427/1072, Loss: 8.0213\n",
      "Iteration 428/1072, Loss: 7.9897\n",
      "Iteration 429/1072, Loss: 7.9862\n",
      "Iteration 430/1072, Loss: 8.0598\n",
      "Iteration 431/1072, Loss: 8.0015\n",
      "Iteration 432/1072, Loss: 7.9835\n",
      "Iteration 433/1072, Loss: 7.9870\n",
      "Iteration 434/1072, Loss: 8.0020\n",
      "Iteration 435/1072, Loss: 7.9761\n",
      "Iteration 436/1072, Loss: 7.9389\n",
      "Iteration 437/1072, Loss: 7.9080\n",
      "Iteration 438/1072, Loss: 7.9815\n",
      "Iteration 439/1072, Loss: 8.0160\n",
      "Iteration 440/1072, Loss: 8.0181\n",
      "Iteration 441/1072, Loss: 7.9956\n",
      "Iteration 442/1072, Loss: 8.0016\n",
      "Iteration 443/1072, Loss: 8.0240\n",
      "Iteration 444/1072, Loss: 8.0119\n",
      "Iteration 445/1072, Loss: 7.9913\n",
      "Iteration 446/1072, Loss: 7.9688\n",
      "Iteration 447/1072, Loss: 7.9473\n",
      "Iteration 448/1072, Loss: 8.0005\n",
      "Iteration 449/1072, Loss: 8.0196\n",
      "Iteration 450/1072, Loss: 8.0100\n",
      "Iteration 451/1072, Loss: 8.0447\n",
      "Iteration 452/1072, Loss: 7.9962\n",
      "Iteration 453/1072, Loss: 7.9919\n",
      "Iteration 454/1072, Loss: 8.0014\n",
      "Iteration 455/1072, Loss: 7.9777\n",
      "Iteration 456/1072, Loss: 8.0073\n",
      "Iteration 457/1072, Loss: 8.0026\n",
      "Iteration 458/1072, Loss: 8.0216\n",
      "Iteration 459/1072, Loss: 8.0912\n",
      "Iteration 460/1072, Loss: 8.0248\n",
      "Iteration 461/1072, Loss: 7.9611\n",
      "Iteration 462/1072, Loss: 7.9672\n",
      "Iteration 463/1072, Loss: 8.0268\n",
      "Iteration 464/1072, Loss: 8.0260\n",
      "Iteration 465/1072, Loss: 7.9641\n",
      "Iteration 466/1072, Loss: 8.0265\n",
      "Iteration 467/1072, Loss: 7.9843\n",
      "Iteration 468/1072, Loss: 8.0451\n",
      "Iteration 469/1072, Loss: 7.9872\n",
      "Iteration 470/1072, Loss: 7.9621\n",
      "Iteration 471/1072, Loss: 8.0270\n",
      "Iteration 472/1072, Loss: 8.0028\n",
      "Iteration 473/1072, Loss: 7.9809\n",
      "Iteration 474/1072, Loss: 8.0071\n",
      "Iteration 475/1072, Loss: 8.0034\n",
      "Iteration 476/1072, Loss: 8.0031\n",
      "Iteration 477/1072, Loss: 7.9728\n",
      "Iteration 478/1072, Loss: 7.9862\n",
      "Iteration 479/1072, Loss: 7.9546\n",
      "Iteration 480/1072, Loss: 7.9956\n",
      "Iteration 481/1072, Loss: 7.9858\n",
      "Iteration 482/1072, Loss: 7.9864\n",
      "Iteration 483/1072, Loss: 8.0308\n",
      "Iteration 484/1072, Loss: 8.0112\n",
      "Iteration 485/1072, Loss: 7.9969\n",
      "Iteration 486/1072, Loss: 7.9915\n",
      "Iteration 487/1072, Loss: 7.9913\n",
      "Iteration 488/1072, Loss: 8.0036\n",
      "Iteration 489/1072, Loss: 7.9983\n",
      "Iteration 490/1072, Loss: 7.9425\n",
      "Iteration 491/1072, Loss: 7.9880\n",
      "Iteration 492/1072, Loss: 7.9600\n",
      "Iteration 493/1072, Loss: 7.9632\n",
      "Iteration 494/1072, Loss: 7.9898\n",
      "Iteration 495/1072, Loss: 8.0039\n",
      "Iteration 496/1072, Loss: 7.9974\n",
      "Iteration 497/1072, Loss: 7.9774\n",
      "Iteration 498/1072, Loss: 7.9734\n",
      "Iteration 499/1072, Loss: 8.0187\n",
      "Iteration 500/1072, Loss: 8.0257\n",
      "Iteration 501/1072, Loss: 8.0148\n",
      "Iteration 502/1072, Loss: 7.9913\n",
      "Iteration 503/1072, Loss: 7.9865\n",
      "Iteration 504/1072, Loss: 8.0069\n",
      "Iteration 505/1072, Loss: 7.9964\n",
      "Iteration 506/1072, Loss: 7.9830\n",
      "Iteration 507/1072, Loss: 8.0200\n",
      "Iteration 508/1072, Loss: 7.9369\n",
      "Iteration 509/1072, Loss: 7.9697\n",
      "Iteration 510/1072, Loss: 8.0307\n",
      "Iteration 511/1072, Loss: 7.9709\n",
      "Iteration 512/1072, Loss: 8.0005\n",
      "Iteration 513/1072, Loss: 8.0370\n",
      "Iteration 514/1072, Loss: 8.0123\n",
      "Iteration 515/1072, Loss: 7.9640\n",
      "Iteration 516/1072, Loss: 7.9919\n",
      "Iteration 517/1072, Loss: 7.9895\n",
      "Iteration 518/1072, Loss: 8.0345\n",
      "Iteration 519/1072, Loss: 7.9795\n",
      "Iteration 520/1072, Loss: 7.9838\n",
      "Iteration 521/1072, Loss: 7.9341\n",
      "Iteration 522/1072, Loss: 8.0394\n",
      "Iteration 523/1072, Loss: 7.9824\n",
      "Iteration 524/1072, Loss: 7.9642\n",
      "Iteration 525/1072, Loss: 7.9439\n",
      "Iteration 526/1072, Loss: 8.0041\n",
      "Iteration 527/1072, Loss: 7.9984\n",
      "Iteration 528/1072, Loss: 7.9537\n",
      "Iteration 529/1072, Loss: 7.9885\n",
      "Iteration 530/1072, Loss: 8.0166\n",
      "Iteration 531/1072, Loss: 7.9901\n",
      "Iteration 532/1072, Loss: 8.0147\n",
      "Iteration 533/1072, Loss: 7.9868\n",
      "Iteration 534/1072, Loss: 8.0050\n",
      "Iteration 535/1072, Loss: 7.9641\n",
      "Iteration 536/1072, Loss: 7.9920\n",
      "Iteration 537/1072, Loss: 8.0326\n",
      "Iteration 538/1072, Loss: 8.0064\n",
      "Iteration 539/1072, Loss: 8.0143\n",
      "Iteration 540/1072, Loss: 7.9580\n",
      "Iteration 541/1072, Loss: 8.0343\n",
      "Iteration 542/1072, Loss: 7.9990\n",
      "Iteration 543/1072, Loss: 8.0220\n",
      "Iteration 544/1072, Loss: 7.9555\n",
      "Iteration 545/1072, Loss: 8.0282\n",
      "Iteration 546/1072, Loss: 8.0048\n",
      "Iteration 547/1072, Loss: 7.9856\n",
      "Iteration 548/1072, Loss: 8.0612\n",
      "Iteration 549/1072, Loss: 7.9743\n",
      "Iteration 550/1072, Loss: 8.0000\n",
      "Iteration 551/1072, Loss: 7.9997\n",
      "Iteration 552/1072, Loss: 8.0062\n",
      "Iteration 553/1072, Loss: 8.0096\n",
      "Iteration 554/1072, Loss: 8.0306\n",
      "Iteration 555/1072, Loss: 7.9887\n",
      "Iteration 556/1072, Loss: 7.9821\n",
      "Iteration 557/1072, Loss: 7.9737\n",
      "Iteration 558/1072, Loss: 8.0223\n",
      "Iteration 559/1072, Loss: 7.9732\n",
      "Iteration 560/1072, Loss: 7.9887\n",
      "Iteration 561/1072, Loss: 7.9739\n",
      "Iteration 562/1072, Loss: 7.9873\n",
      "Iteration 563/1072, Loss: 7.9284\n",
      "Iteration 564/1072, Loss: 7.9967\n",
      "Iteration 565/1072, Loss: 7.9577\n",
      "Iteration 566/1072, Loss: 8.0638\n",
      "Iteration 567/1072, Loss: 7.9962\n",
      "Iteration 568/1072, Loss: 7.9854\n",
      "Iteration 569/1072, Loss: 8.0386\n",
      "Iteration 570/1072, Loss: 8.0101\n",
      "Iteration 571/1072, Loss: 7.9327\n",
      "Iteration 572/1072, Loss: 8.0305\n",
      "Iteration 573/1072, Loss: 7.9849\n",
      "Iteration 574/1072, Loss: 8.0024\n",
      "Iteration 575/1072, Loss: 7.9694\n",
      "Iteration 576/1072, Loss: 7.9748\n",
      "Iteration 577/1072, Loss: 7.9993\n",
      "Iteration 578/1072, Loss: 8.0369\n",
      "Iteration 579/1072, Loss: 7.9948\n",
      "Iteration 580/1072, Loss: 7.9852\n",
      "Iteration 581/1072, Loss: 8.0172\n",
      "Iteration 582/1072, Loss: 7.9941\n",
      "Iteration 583/1072, Loss: 7.9780\n",
      "Iteration 584/1072, Loss: 8.0173\n",
      "Iteration 585/1072, Loss: 8.0296\n",
      "Iteration 586/1072, Loss: 8.0195\n",
      "Iteration 587/1072, Loss: 7.9728\n",
      "Iteration 588/1072, Loss: 7.9717\n",
      "Iteration 589/1072, Loss: 7.9952\n",
      "Iteration 590/1072, Loss: 8.0077\n",
      "Iteration 591/1072, Loss: 7.9616\n",
      "Iteration 592/1072, Loss: 8.0041\n",
      "Iteration 593/1072, Loss: 8.0341\n",
      "Iteration 594/1072, Loss: 7.9500\n",
      "Iteration 595/1072, Loss: 8.0477\n",
      "Iteration 596/1072, Loss: 8.0465\n",
      "Iteration 597/1072, Loss: 7.9845\n",
      "Iteration 598/1072, Loss: 8.0216\n",
      "Iteration 599/1072, Loss: 7.9554\n",
      "Iteration 600/1072, Loss: 8.0291\n",
      "Iteration 601/1072, Loss: 8.0375\n",
      "Iteration 602/1072, Loss: 8.0263\n",
      "Iteration 603/1072, Loss: 8.0231\n",
      "Iteration 604/1072, Loss: 8.0110\n",
      "Iteration 605/1072, Loss: 8.0154\n",
      "Iteration 606/1072, Loss: 7.9893\n",
      "Iteration 607/1072, Loss: 7.9943\n",
      "Iteration 608/1072, Loss: 7.9550\n",
      "Iteration 609/1072, Loss: 8.0021\n",
      "Iteration 610/1072, Loss: 7.9923\n",
      "Iteration 611/1072, Loss: 7.9947\n",
      "Iteration 612/1072, Loss: 8.0179\n",
      "Iteration 613/1072, Loss: 7.9663\n",
      "Iteration 614/1072, Loss: 8.0017\n",
      "Iteration 615/1072, Loss: 7.9801\n",
      "Iteration 616/1072, Loss: 8.0059\n",
      "Iteration 617/1072, Loss: 7.9747\n",
      "Iteration 618/1072, Loss: 8.0152\n",
      "Iteration 619/1072, Loss: 8.0123\n",
      "Iteration 620/1072, Loss: 8.0179\n",
      "Iteration 621/1072, Loss: 8.0278\n",
      "Iteration 622/1072, Loss: 7.9958\n",
      "Iteration 623/1072, Loss: 7.9780\n",
      "Iteration 624/1072, Loss: 7.9583\n",
      "Iteration 625/1072, Loss: 8.0408\n",
      "Iteration 626/1072, Loss: 7.9727\n",
      "Iteration 627/1072, Loss: 8.0041\n",
      "Iteration 628/1072, Loss: 7.9771\n",
      "Iteration 629/1072, Loss: 8.0167\n",
      "Iteration 630/1072, Loss: 7.9748\n",
      "Iteration 631/1072, Loss: 7.9844\n",
      "Iteration 632/1072, Loss: 7.9811\n",
      "Iteration 633/1072, Loss: 7.9860\n",
      "Iteration 634/1072, Loss: 7.9998\n",
      "Iteration 635/1072, Loss: 8.0138\n",
      "Iteration 636/1072, Loss: 7.9606\n",
      "Iteration 637/1072, Loss: 7.9615\n",
      "Iteration 638/1072, Loss: 8.0447\n",
      "Iteration 639/1072, Loss: 7.9915\n",
      "Iteration 640/1072, Loss: 8.0473\n",
      "Iteration 641/1072, Loss: 8.0046\n",
      "Iteration 642/1072, Loss: 8.0153\n",
      "Iteration 643/1072, Loss: 8.0017\n",
      "Iteration 644/1072, Loss: 7.9993\n",
      "Iteration 645/1072, Loss: 7.9892\n",
      "Iteration 646/1072, Loss: 7.9724\n",
      "Iteration 647/1072, Loss: 7.9790\n",
      "Iteration 648/1072, Loss: 7.9594\n",
      "Iteration 649/1072, Loss: 8.0316\n",
      "Iteration 650/1072, Loss: 8.0194\n",
      "Iteration 651/1072, Loss: 8.0409\n",
      "Iteration 652/1072, Loss: 7.9257\n",
      "Iteration 653/1072, Loss: 7.9959\n",
      "Iteration 654/1072, Loss: 7.9566\n",
      "Iteration 655/1072, Loss: 8.0215\n",
      "Iteration 656/1072, Loss: 8.0109\n",
      "Iteration 657/1072, Loss: 7.9390\n",
      "Iteration 658/1072, Loss: 8.0139\n",
      "Iteration 659/1072, Loss: 8.0429\n",
      "Iteration 660/1072, Loss: 7.9829\n",
      "Iteration 661/1072, Loss: 7.9885\n",
      "Iteration 662/1072, Loss: 7.9891\n",
      "Iteration 663/1072, Loss: 7.9793\n",
      "Iteration 664/1072, Loss: 8.0066\n",
      "Iteration 665/1072, Loss: 7.9656\n",
      "Iteration 666/1072, Loss: 7.9843\n",
      "Iteration 667/1072, Loss: 8.0198\n",
      "Iteration 668/1072, Loss: 7.9219\n",
      "Iteration 669/1072, Loss: 7.9679\n",
      "Iteration 670/1072, Loss: 7.9914\n",
      "Iteration 671/1072, Loss: 7.9939\n",
      "Iteration 672/1072, Loss: 7.9577\n",
      "Iteration 673/1072, Loss: 8.0147\n",
      "Iteration 674/1072, Loss: 8.0199\n",
      "Iteration 675/1072, Loss: 7.9954\n",
      "Iteration 676/1072, Loss: 7.9879\n",
      "Iteration 677/1072, Loss: 8.0307\n",
      "Iteration 678/1072, Loss: 8.0057\n",
      "Iteration 679/1072, Loss: 7.9769\n",
      "Iteration 680/1072, Loss: 7.9995\n",
      "Iteration 681/1072, Loss: 7.9866\n",
      "Iteration 682/1072, Loss: 7.9902\n",
      "Iteration 683/1072, Loss: 8.0103\n",
      "Iteration 684/1072, Loss: 7.9815\n",
      "Iteration 685/1072, Loss: 7.9694\n",
      "Iteration 686/1072, Loss: 7.9622\n",
      "Iteration 687/1072, Loss: 8.0074\n",
      "Iteration 688/1072, Loss: 7.9497\n",
      "Iteration 689/1072, Loss: 7.9636\n",
      "Iteration 690/1072, Loss: 7.9690\n",
      "Iteration 691/1072, Loss: 8.0592\n",
      "Iteration 692/1072, Loss: 7.9838\n",
      "Iteration 693/1072, Loss: 7.9485\n",
      "Iteration 694/1072, Loss: 8.0178\n",
      "Iteration 695/1072, Loss: 7.9774\n",
      "Iteration 696/1072, Loss: 8.0124\n",
      "Iteration 697/1072, Loss: 7.9893\n",
      "Iteration 698/1072, Loss: 8.0347\n",
      "Iteration 699/1072, Loss: 8.0015\n",
      "Iteration 700/1072, Loss: 8.0032\n",
      "Iteration 701/1072, Loss: 7.9896\n",
      "Iteration 702/1072, Loss: 7.9959\n",
      "Iteration 703/1072, Loss: 7.9405\n",
      "Iteration 704/1072, Loss: 7.9476\n",
      "Iteration 705/1072, Loss: 8.0000\n",
      "Iteration 706/1072, Loss: 8.0366\n",
      "Iteration 707/1072, Loss: 7.9507\n",
      "Iteration 708/1072, Loss: 8.0032\n",
      "Iteration 709/1072, Loss: 8.0489\n",
      "Iteration 710/1072, Loss: 8.0189\n",
      "Iteration 711/1072, Loss: 8.0321\n",
      "Iteration 712/1072, Loss: 7.9669\n",
      "Iteration 713/1072, Loss: 7.9505\n",
      "Iteration 714/1072, Loss: 8.0173\n",
      "Iteration 715/1072, Loss: 7.9435\n",
      "Iteration 716/1072, Loss: 7.9538\n",
      "Iteration 717/1072, Loss: 8.0489\n",
      "Iteration 718/1072, Loss: 8.0098\n",
      "Iteration 719/1072, Loss: 7.9814\n",
      "Iteration 720/1072, Loss: 8.0152\n",
      "Iteration 721/1072, Loss: 7.9407\n",
      "Iteration 722/1072, Loss: 8.0103\n",
      "Iteration 723/1072, Loss: 7.9264\n",
      "Iteration 724/1072, Loss: 7.9967\n",
      "Iteration 725/1072, Loss: 7.9570\n",
      "Iteration 726/1072, Loss: 7.9692\n",
      "Iteration 727/1072, Loss: 7.9801\n",
      "Iteration 728/1072, Loss: 8.0013\n",
      "Iteration 729/1072, Loss: 7.9830\n",
      "Iteration 730/1072, Loss: 8.0153\n",
      "Iteration 731/1072, Loss: 8.0466\n",
      "Iteration 732/1072, Loss: 7.9995\n",
      "Iteration 733/1072, Loss: 7.9943\n",
      "Iteration 734/1072, Loss: 7.9622\n",
      "Iteration 735/1072, Loss: 7.9678\n",
      "Iteration 736/1072, Loss: 8.0389\n",
      "Iteration 737/1072, Loss: 8.0118\n",
      "Iteration 738/1072, Loss: 8.0100\n",
      "Iteration 739/1072, Loss: 7.9738\n",
      "Iteration 740/1072, Loss: 8.0179\n",
      "Iteration 741/1072, Loss: 7.9468\n",
      "Iteration 742/1072, Loss: 8.0224\n",
      "Iteration 743/1072, Loss: 7.9480\n",
      "Iteration 744/1072, Loss: 8.0007\n",
      "Iteration 745/1072, Loss: 7.9906\n",
      "Iteration 746/1072, Loss: 8.0373\n",
      "Iteration 747/1072, Loss: 8.0392\n",
      "Iteration 748/1072, Loss: 7.9547\n",
      "Iteration 749/1072, Loss: 7.9746\n",
      "Iteration 750/1072, Loss: 8.0196\n",
      "Iteration 751/1072, Loss: 7.9284\n",
      "Iteration 752/1072, Loss: 7.9343\n",
      "Iteration 753/1072, Loss: 7.9720\n",
      "Iteration 754/1072, Loss: 8.0356\n",
      "Iteration 755/1072, Loss: 7.9919\n",
      "Iteration 756/1072, Loss: 8.0144\n",
      "Iteration 757/1072, Loss: 8.0754\n",
      "Iteration 758/1072, Loss: 8.0241\n",
      "Iteration 759/1072, Loss: 8.0173\n",
      "Iteration 760/1072, Loss: 7.9849\n",
      "Iteration 761/1072, Loss: 8.0149\n",
      "Iteration 762/1072, Loss: 7.9933\n",
      "Iteration 763/1072, Loss: 7.9824\n",
      "Iteration 764/1072, Loss: 8.0452\n",
      "Iteration 765/1072, Loss: 7.9984\n",
      "Iteration 766/1072, Loss: 7.9271\n",
      "Iteration 767/1072, Loss: 8.0154\n",
      "Iteration 768/1072, Loss: 7.9987\n",
      "Iteration 769/1072, Loss: 7.9813\n",
      "Iteration 770/1072, Loss: 8.0065\n",
      "Iteration 771/1072, Loss: 7.9890\n",
      "Iteration 772/1072, Loss: 8.0077\n",
      "Iteration 773/1072, Loss: 8.0205\n",
      "Iteration 774/1072, Loss: 8.0126\n",
      "Iteration 775/1072, Loss: 7.9953\n",
      "Iteration 776/1072, Loss: 8.0145\n",
      "Iteration 777/1072, Loss: 8.0004\n",
      "Iteration 778/1072, Loss: 8.0093\n",
      "Iteration 779/1072, Loss: 8.0119\n",
      "Iteration 780/1072, Loss: 8.0485\n",
      "Iteration 781/1072, Loss: 8.0044\n",
      "Iteration 782/1072, Loss: 8.0124\n",
      "Iteration 783/1072, Loss: 7.9886\n",
      "Iteration 784/1072, Loss: 7.9382\n",
      "Iteration 785/1072, Loss: 7.9781\n",
      "Iteration 786/1072, Loss: 8.0002\n",
      "Iteration 787/1072, Loss: 7.9651\n",
      "Iteration 788/1072, Loss: 7.9521\n",
      "Iteration 789/1072, Loss: 7.9749\n",
      "Iteration 790/1072, Loss: 7.9714\n",
      "Iteration 791/1072, Loss: 7.9783\n",
      "Iteration 792/1072, Loss: 8.0404\n",
      "Iteration 793/1072, Loss: 7.9899\n",
      "Iteration 794/1072, Loss: 7.9822\n",
      "Iteration 795/1072, Loss: 8.0858\n",
      "Iteration 796/1072, Loss: 8.0097\n",
      "Iteration 797/1072, Loss: 7.9703\n",
      "Iteration 798/1072, Loss: 8.0076\n",
      "Iteration 799/1072, Loss: 8.0264\n",
      "Iteration 800/1072, Loss: 7.9954\n",
      "Iteration 801/1072, Loss: 8.0067\n",
      "Iteration 802/1072, Loss: 8.0482\n",
      "Iteration 803/1072, Loss: 8.0509\n",
      "Iteration 804/1072, Loss: 7.9263\n",
      "Iteration 805/1072, Loss: 7.9942\n",
      "Iteration 806/1072, Loss: 8.0144\n",
      "Iteration 807/1072, Loss: 8.0131\n",
      "Iteration 808/1072, Loss: 7.9786\n",
      "Iteration 809/1072, Loss: 8.0304\n",
      "Iteration 810/1072, Loss: 7.9830\n",
      "Iteration 811/1072, Loss: 8.0180\n",
      "Iteration 812/1072, Loss: 7.9376\n",
      "Iteration 813/1072, Loss: 8.0046\n",
      "Iteration 814/1072, Loss: 7.9769\n",
      "Iteration 815/1072, Loss: 7.9748\n",
      "Iteration 816/1072, Loss: 8.0115\n",
      "Iteration 817/1072, Loss: 7.9948\n",
      "Iteration 818/1072, Loss: 7.9621\n",
      "Iteration 819/1072, Loss: 8.0236\n",
      "Iteration 820/1072, Loss: 8.0183\n",
      "Iteration 821/1072, Loss: 7.9742\n",
      "Iteration 822/1072, Loss: 8.0822\n",
      "Iteration 823/1072, Loss: 8.0068\n",
      "Iteration 824/1072, Loss: 7.9737\n",
      "Iteration 825/1072, Loss: 7.9838\n",
      "Iteration 826/1072, Loss: 8.0000\n",
      "Iteration 827/1072, Loss: 7.9889\n",
      "Iteration 828/1072, Loss: 7.9961\n",
      "Iteration 829/1072, Loss: 7.9687\n",
      "Iteration 830/1072, Loss: 8.0190\n",
      "Iteration 831/1072, Loss: 8.0089\n",
      "Iteration 832/1072, Loss: 7.9825\n",
      "Iteration 833/1072, Loss: 7.9405\n",
      "Iteration 834/1072, Loss: 8.0132\n",
      "Iteration 835/1072, Loss: 8.0027\n",
      "Iteration 836/1072, Loss: 8.0366\n",
      "Iteration 837/1072, Loss: 8.0145\n",
      "Iteration 838/1072, Loss: 7.9002\n",
      "Iteration 839/1072, Loss: 7.9635\n",
      "Iteration 840/1072, Loss: 8.0105\n",
      "Iteration 841/1072, Loss: 8.0620\n",
      "Iteration 842/1072, Loss: 7.9852\n",
      "Iteration 843/1072, Loss: 8.0311\n",
      "Iteration 844/1072, Loss: 8.0130\n",
      "Iteration 845/1072, Loss: 8.0485\n",
      "Iteration 846/1072, Loss: 7.9993\n",
      "Iteration 847/1072, Loss: 8.0528\n",
      "Iteration 848/1072, Loss: 7.9900\n",
      "Iteration 849/1072, Loss: 7.9378\n",
      "Iteration 850/1072, Loss: 7.9330\n",
      "Iteration 851/1072, Loss: 7.9945\n",
      "Iteration 852/1072, Loss: 7.9601\n",
      "Iteration 853/1072, Loss: 7.9775\n",
      "Iteration 854/1072, Loss: 7.9574\n",
      "Iteration 855/1072, Loss: 7.9622\n",
      "Iteration 856/1072, Loss: 7.9661\n",
      "Iteration 857/1072, Loss: 8.0039\n",
      "Iteration 858/1072, Loss: 7.9939\n",
      "Iteration 859/1072, Loss: 8.0637\n",
      "Iteration 860/1072, Loss: 7.9827\n",
      "Iteration 861/1072, Loss: 7.9727\n",
      "Iteration 862/1072, Loss: 7.9736\n",
      "Iteration 863/1072, Loss: 7.9872\n",
      "Iteration 864/1072, Loss: 7.9472\n",
      "Iteration 865/1072, Loss: 7.9779\n",
      "Iteration 866/1072, Loss: 7.9713\n",
      "Iteration 867/1072, Loss: 7.9298\n",
      "Iteration 868/1072, Loss: 7.9586\n",
      "Iteration 869/1072, Loss: 7.9485\n",
      "Iteration 870/1072, Loss: 8.0095\n",
      "Iteration 871/1072, Loss: 7.9679\n",
      "Iteration 872/1072, Loss: 7.9609\n",
      "Iteration 873/1072, Loss: 7.9638\n",
      "Iteration 874/1072, Loss: 8.0244\n",
      "Iteration 875/1072, Loss: 7.9991\n",
      "Iteration 876/1072, Loss: 7.9359\n",
      "Iteration 877/1072, Loss: 7.9431\n",
      "Iteration 878/1072, Loss: 8.0488\n",
      "Iteration 879/1072, Loss: 7.9570\n",
      "Iteration 880/1072, Loss: 7.9549\n",
      "Iteration 881/1072, Loss: 7.9793\n",
      "Iteration 882/1072, Loss: 7.9623\n",
      "Iteration 883/1072, Loss: 8.0274\n",
      "Iteration 884/1072, Loss: 8.0090\n",
      "Iteration 885/1072, Loss: 7.9881\n",
      "Iteration 886/1072, Loss: 7.9774\n",
      "Iteration 887/1072, Loss: 8.0391\n",
      "Iteration 888/1072, Loss: 7.9911\n",
      "Iteration 889/1072, Loss: 7.9532\n",
      "Iteration 890/1072, Loss: 7.9930\n",
      "Iteration 891/1072, Loss: 8.0419\n",
      "Iteration 892/1072, Loss: 8.0054\n",
      "Iteration 893/1072, Loss: 8.0025\n",
      "Iteration 894/1072, Loss: 8.0096\n",
      "Iteration 895/1072, Loss: 7.9427\n",
      "Iteration 896/1072, Loss: 7.9830\n",
      "Iteration 897/1072, Loss: 7.9868\n",
      "Iteration 898/1072, Loss: 8.0044\n",
      "Iteration 899/1072, Loss: 7.9925\n",
      "Iteration 900/1072, Loss: 7.9480\n",
      "Iteration 901/1072, Loss: 7.9438\n",
      "Iteration 902/1072, Loss: 7.9629\n",
      "Iteration 903/1072, Loss: 7.9945\n",
      "Iteration 904/1072, Loss: 8.0038\n",
      "Iteration 905/1072, Loss: 7.9828\n",
      "Iteration 906/1072, Loss: 8.0619\n",
      "Iteration 907/1072, Loss: 7.9827\n",
      "Iteration 908/1072, Loss: 7.9797\n",
      "Iteration 909/1072, Loss: 7.9792\n",
      "Iteration 910/1072, Loss: 8.0973\n",
      "Iteration 911/1072, Loss: 7.9728\n",
      "Iteration 912/1072, Loss: 8.0296\n",
      "Iteration 913/1072, Loss: 7.9453\n",
      "Iteration 914/1072, Loss: 7.9859\n",
      "Iteration 915/1072, Loss: 7.9413\n",
      "Iteration 916/1072, Loss: 8.0073\n",
      "Iteration 917/1072, Loss: 7.9937\n",
      "Iteration 918/1072, Loss: 7.9860\n",
      "Iteration 919/1072, Loss: 8.0014\n",
      "Iteration 920/1072, Loss: 8.0053\n",
      "Iteration 921/1072, Loss: 8.0014\n",
      "Iteration 922/1072, Loss: 7.9841\n",
      "Iteration 923/1072, Loss: 7.9817\n",
      "Iteration 924/1072, Loss: 7.9792\n",
      "Iteration 925/1072, Loss: 7.9303\n",
      "Iteration 926/1072, Loss: 8.0063\n",
      "Iteration 927/1072, Loss: 8.0345\n",
      "Iteration 928/1072, Loss: 8.0079\n",
      "Iteration 929/1072, Loss: 7.9734\n",
      "Iteration 930/1072, Loss: 8.0034\n",
      "Iteration 931/1072, Loss: 7.9761\n",
      "Iteration 932/1072, Loss: 7.9428\n",
      "Iteration 933/1072, Loss: 7.9923\n",
      "Iteration 934/1072, Loss: 7.9242\n",
      "Iteration 935/1072, Loss: 7.9699\n",
      "Iteration 936/1072, Loss: 8.0227\n",
      "Iteration 937/1072, Loss: 8.0158\n",
      "Iteration 938/1072, Loss: 7.9840\n",
      "Iteration 939/1072, Loss: 8.0216\n",
      "Iteration 940/1072, Loss: 8.0151\n",
      "Iteration 941/1072, Loss: 7.9925\n",
      "Iteration 942/1072, Loss: 7.9836\n",
      "Iteration 943/1072, Loss: 7.9462\n",
      "Iteration 944/1072, Loss: 8.0257\n",
      "Iteration 945/1072, Loss: 7.9811\n",
      "Iteration 946/1072, Loss: 8.0217\n",
      "Iteration 947/1072, Loss: 8.0011\n",
      "Iteration 948/1072, Loss: 7.9737\n",
      "Iteration 949/1072, Loss: 7.9520\n",
      "Iteration 950/1072, Loss: 7.9784\n",
      "Iteration 951/1072, Loss: 7.9949\n",
      "Iteration 952/1072, Loss: 7.9928\n",
      "Iteration 953/1072, Loss: 7.9401\n",
      "Iteration 954/1072, Loss: 7.9568\n",
      "Iteration 955/1072, Loss: 8.0350\n",
      "Iteration 956/1072, Loss: 8.0319\n",
      "Iteration 957/1072, Loss: 8.0236\n",
      "Iteration 958/1072, Loss: 7.9831\n",
      "Iteration 959/1072, Loss: 7.9776\n",
      "Iteration 960/1072, Loss: 7.9656\n",
      "Iteration 961/1072, Loss: 8.0065\n",
      "Iteration 962/1072, Loss: 7.9298\n",
      "Iteration 963/1072, Loss: 7.9846\n",
      "Iteration 964/1072, Loss: 8.0129\n",
      "Iteration 965/1072, Loss: 7.9925\n",
      "Iteration 966/1072, Loss: 8.0289\n",
      "Iteration 967/1072, Loss: 8.0216\n",
      "Iteration 968/1072, Loss: 8.0185\n",
      "Iteration 969/1072, Loss: 8.0137\n",
      "Iteration 970/1072, Loss: 7.9728\n",
      "Iteration 971/1072, Loss: 8.0184\n",
      "Iteration 972/1072, Loss: 7.9501\n",
      "Iteration 973/1072, Loss: 7.9679\n",
      "Iteration 974/1072, Loss: 8.0229\n",
      "Iteration 975/1072, Loss: 7.9828\n",
      "Iteration 976/1072, Loss: 8.0374\n",
      "Iteration 977/1072, Loss: 8.0676\n",
      "Iteration 978/1072, Loss: 7.9786\n",
      "Iteration 979/1072, Loss: 7.9616\n",
      "Iteration 980/1072, Loss: 8.0185\n",
      "Iteration 981/1072, Loss: 7.9787\n",
      "Iteration 982/1072, Loss: 7.9654\n",
      "Iteration 983/1072, Loss: 7.9571\n",
      "Iteration 984/1072, Loss: 7.9441\n",
      "Iteration 985/1072, Loss: 7.9167\n",
      "Iteration 986/1072, Loss: 8.0101\n",
      "Iteration 987/1072, Loss: 7.9994\n",
      "Iteration 988/1072, Loss: 8.0058\n",
      "Iteration 989/1072, Loss: 8.0079\n",
      "Iteration 990/1072, Loss: 7.9834\n",
      "Iteration 991/1072, Loss: 7.9280\n",
      "Iteration 992/1072, Loss: 8.0348\n",
      "Iteration 993/1072, Loss: 7.9719\n",
      "Iteration 994/1072, Loss: 7.9645\n",
      "Iteration 995/1072, Loss: 8.0065\n",
      "Iteration 996/1072, Loss: 7.9719\n",
      "Iteration 997/1072, Loss: 8.0044\n",
      "Iteration 998/1072, Loss: 8.0220\n",
      "Iteration 999/1072, Loss: 8.0061\n",
      "Iteration 1000/1072, Loss: 8.0445\n",
      "Iteration 1001/1072, Loss: 8.0136\n",
      "Iteration 1002/1072, Loss: 7.9803\n",
      "Iteration 1003/1072, Loss: 7.9802\n",
      "Iteration 1004/1072, Loss: 8.0104\n",
      "Iteration 1005/1072, Loss: 7.9755\n",
      "Iteration 1006/1072, Loss: 7.9332\n",
      "Iteration 1007/1072, Loss: 7.9725\n",
      "Iteration 1008/1072, Loss: 7.9988\n",
      "Iteration 1009/1072, Loss: 7.9863\n",
      "Iteration 1010/1072, Loss: 8.0097\n",
      "Iteration 1011/1072, Loss: 8.0159\n",
      "Iteration 1012/1072, Loss: 8.0189\n",
      "Iteration 1013/1072, Loss: 7.9405\n",
      "Iteration 1014/1072, Loss: 8.0211\n",
      "Iteration 1015/1072, Loss: 7.9724\n",
      "Iteration 1016/1072, Loss: 7.9553\n",
      "Iteration 1017/1072, Loss: 8.0004\n",
      "Iteration 1018/1072, Loss: 7.9608\n",
      "Iteration 1019/1072, Loss: 8.0646\n",
      "Iteration 1020/1072, Loss: 8.0007\n",
      "Iteration 1021/1072, Loss: 7.9748\n",
      "Iteration 1022/1072, Loss: 7.9552\n",
      "Iteration 1023/1072, Loss: 7.9655\n",
      "Iteration 1024/1072, Loss: 8.0034\n",
      "Iteration 1025/1072, Loss: 8.0193\n",
      "Iteration 1026/1072, Loss: 7.9377\n",
      "Iteration 1027/1072, Loss: 8.0400\n",
      "Iteration 1028/1072, Loss: 7.9868\n",
      "Iteration 1029/1072, Loss: 8.0139\n",
      "Iteration 1030/1072, Loss: 7.9765\n",
      "Iteration 1031/1072, Loss: 8.0622\n",
      "Iteration 1032/1072, Loss: 7.9765\n",
      "Iteration 1033/1072, Loss: 7.9395\n",
      "Iteration 1034/1072, Loss: 7.9460\n",
      "Iteration 1035/1072, Loss: 8.0182\n",
      "Iteration 1036/1072, Loss: 8.0103\n",
      "Iteration 1037/1072, Loss: 8.0090\n",
      "Iteration 1038/1072, Loss: 7.9971\n",
      "Iteration 1039/1072, Loss: 8.0221\n",
      "Iteration 1040/1072, Loss: 7.9503\n",
      "Iteration 1041/1072, Loss: 7.9670\n",
      "Iteration 1042/1072, Loss: 8.0480\n",
      "Iteration 1043/1072, Loss: 7.9881\n",
      "Iteration 1044/1072, Loss: 7.9538\n",
      "Iteration 1045/1072, Loss: 7.9606\n",
      "Iteration 1046/1072, Loss: 7.9425\n",
      "Iteration 1047/1072, Loss: 7.9899\n",
      "Iteration 1048/1072, Loss: 7.9931\n",
      "Iteration 1049/1072, Loss: 8.0134\n",
      "Iteration 1050/1072, Loss: 8.0018\n",
      "Iteration 1051/1072, Loss: 7.9873\n",
      "Iteration 1052/1072, Loss: 7.9479\n",
      "Iteration 1053/1072, Loss: 8.0122\n",
      "Iteration 1054/1072, Loss: 7.9937\n",
      "Iteration 1055/1072, Loss: 7.9901\n",
      "Iteration 1056/1072, Loss: 7.9756\n",
      "Iteration 1057/1072, Loss: 7.9593\n",
      "Iteration 1058/1072, Loss: 7.9317\n",
      "Iteration 1059/1072, Loss: 8.0137\n",
      "Iteration 1060/1072, Loss: 7.9886\n",
      "Iteration 1061/1072, Loss: 7.9646\n",
      "Iteration 1062/1072, Loss: 7.9432\n",
      "Iteration 1063/1072, Loss: 8.0276\n",
      "Iteration 1064/1072, Loss: 7.9605\n",
      "Iteration 1065/1072, Loss: 8.0447\n",
      "Iteration 1066/1072, Loss: 7.9677\n",
      "Iteration 1067/1072, Loss: 7.9788\n",
      "Iteration 1068/1072, Loss: 8.0499\n",
      "Iteration 1069/1072, Loss: 7.9693\n",
      "Iteration 1070/1072, Loss: 7.9273\n",
      "Iteration 1071/1072, Loss: 7.9799\n",
      "Iteration 1072/1072, Loss: 7.9785\n",
      "Epoch 2/10, Loss: 7.9987\n",
      "Checkpoint saved at checkpoints/efficientnet-b4_epoch_2.pth\n",
      "Validation Accuracy: 0.12%\n",
      "Iteration 1/1072, Loss: 8.0048\n",
      "Iteration 2/1072, Loss: 7.9475\n",
      "Iteration 3/1072, Loss: 7.9998\n",
      "Iteration 4/1072, Loss: 8.0159\n",
      "Iteration 5/1072, Loss: 7.9650\n",
      "Iteration 6/1072, Loss: 7.9236\n",
      "Iteration 7/1072, Loss: 7.9799\n",
      "Iteration 8/1072, Loss: 7.9311\n",
      "Iteration 9/1072, Loss: 7.9067\n",
      "Iteration 10/1072, Loss: 8.0142\n",
      "Iteration 11/1072, Loss: 7.8814\n",
      "Iteration 12/1072, Loss: 7.9739\n",
      "Iteration 13/1072, Loss: 7.9614\n",
      "Iteration 14/1072, Loss: 7.9881\n",
      "Iteration 15/1072, Loss: 7.9506\n",
      "Iteration 16/1072, Loss: 7.9328\n",
      "Iteration 17/1072, Loss: 7.9381\n",
      "Iteration 18/1072, Loss: 7.9775\n",
      "Iteration 19/1072, Loss: 7.9528\n",
      "Iteration 20/1072, Loss: 7.9139\n",
      "Iteration 21/1072, Loss: 7.9648\n",
      "Iteration 22/1072, Loss: 7.9057\n",
      "Iteration 23/1072, Loss: 7.9945\n",
      "Iteration 24/1072, Loss: 7.9918\n",
      "Iteration 25/1072, Loss: 7.9625\n",
      "Iteration 26/1072, Loss: 7.9994\n",
      "Iteration 27/1072, Loss: 7.9762\n",
      "Iteration 28/1072, Loss: 8.0239\n",
      "Iteration 29/1072, Loss: 7.9439\n",
      "Iteration 30/1072, Loss: 8.0216\n",
      "Iteration 31/1072, Loss: 7.9556\n",
      "Iteration 32/1072, Loss: 7.9373\n",
      "Iteration 33/1072, Loss: 7.9211\n",
      "Iteration 34/1072, Loss: 7.9812\n",
      "Iteration 35/1072, Loss: 7.9281\n",
      "Iteration 36/1072, Loss: 7.9653\n",
      "Iteration 37/1072, Loss: 7.8915\n",
      "Iteration 38/1072, Loss: 7.9577\n",
      "Iteration 39/1072, Loss: 7.9617\n",
      "Iteration 40/1072, Loss: 7.9735\n",
      "Iteration 41/1072, Loss: 7.9982\n",
      "Iteration 42/1072, Loss: 7.8815\n",
      "Iteration 43/1072, Loss: 7.9548\n",
      "Iteration 44/1072, Loss: 8.0400\n",
      "Iteration 45/1072, Loss: 7.9919\n",
      "Iteration 46/1072, Loss: 7.9774\n",
      "Iteration 47/1072, Loss: 7.9572\n",
      "Iteration 48/1072, Loss: 7.8851\n",
      "Iteration 49/1072, Loss: 7.9806\n",
      "Iteration 50/1072, Loss: 8.0043\n",
      "Iteration 51/1072, Loss: 7.9520\n",
      "Iteration 52/1072, Loss: 7.9047\n",
      "Iteration 53/1072, Loss: 8.0164\n",
      "Iteration 54/1072, Loss: 8.0144\n",
      "Iteration 55/1072, Loss: 7.9649\n",
      "Iteration 56/1072, Loss: 8.0128\n",
      "Iteration 57/1072, Loss: 7.9643\n",
      "Iteration 58/1072, Loss: 7.9804\n",
      "Iteration 59/1072, Loss: 7.9729\n",
      "Iteration 60/1072, Loss: 7.9356\n",
      "Iteration 61/1072, Loss: 7.9623\n",
      "Iteration 62/1072, Loss: 7.9297\n",
      "Iteration 63/1072, Loss: 7.9308\n",
      "Iteration 64/1072, Loss: 7.9854\n",
      "Iteration 65/1072, Loss: 7.9392\n",
      "Iteration 66/1072, Loss: 7.9857\n",
      "Iteration 67/1072, Loss: 7.9826\n",
      "Iteration 68/1072, Loss: 7.9572\n",
      "Iteration 69/1072, Loss: 8.0018\n",
      "Iteration 70/1072, Loss: 7.9599\n",
      "Iteration 71/1072, Loss: 7.9734\n",
      "Iteration 72/1072, Loss: 7.9591\n",
      "Iteration 73/1072, Loss: 7.9185\n",
      "Iteration 74/1072, Loss: 7.9797\n",
      "Iteration 75/1072, Loss: 7.9740\n",
      "Iteration 76/1072, Loss: 7.9403\n",
      "Iteration 77/1072, Loss: 7.9362\n",
      "Iteration 78/1072, Loss: 7.9646\n",
      "Iteration 79/1072, Loss: 7.9657\n",
      "Iteration 80/1072, Loss: 7.9617\n",
      "Iteration 81/1072, Loss: 7.9115\n",
      "Iteration 82/1072, Loss: 7.8770\n",
      "Iteration 83/1072, Loss: 8.0062\n",
      "Iteration 84/1072, Loss: 7.9451\n",
      "Iteration 85/1072, Loss: 7.9829\n",
      "Iteration 86/1072, Loss: 7.9667\n",
      "Iteration 87/1072, Loss: 7.9542\n",
      "Iteration 88/1072, Loss: 7.9275\n",
      "Iteration 89/1072, Loss: 7.9654\n",
      "Iteration 90/1072, Loss: 7.8689\n",
      "Iteration 91/1072, Loss: 7.9876\n",
      "Iteration 92/1072, Loss: 7.9641\n",
      "Iteration 93/1072, Loss: 7.9476\n",
      "Iteration 94/1072, Loss: 7.9335\n",
      "Iteration 95/1072, Loss: 7.9649\n",
      "Iteration 96/1072, Loss: 7.9058\n",
      "Iteration 97/1072, Loss: 7.9082\n",
      "Iteration 98/1072, Loss: 8.0012\n",
      "Iteration 99/1072, Loss: 7.9778\n",
      "Iteration 100/1072, Loss: 8.0193\n",
      "Iteration 101/1072, Loss: 7.9893\n",
      "Iteration 102/1072, Loss: 7.9448\n",
      "Iteration 103/1072, Loss: 7.9419\n",
      "Iteration 104/1072, Loss: 7.9651\n",
      "Iteration 105/1072, Loss: 7.9380\n",
      "Iteration 106/1072, Loss: 7.9057\n",
      "Iteration 107/1072, Loss: 7.8575\n",
      "Iteration 108/1072, Loss: 8.0203\n",
      "Iteration 109/1072, Loss: 7.9664\n",
      "Iteration 110/1072, Loss: 7.9718\n",
      "Iteration 111/1072, Loss: 7.9230\n",
      "Iteration 112/1072, Loss: 8.0256\n",
      "Iteration 113/1072, Loss: 7.9626\n",
      "Iteration 114/1072, Loss: 7.9364\n",
      "Iteration 115/1072, Loss: 7.9444\n",
      "Iteration 116/1072, Loss: 7.8689\n",
      "Iteration 117/1072, Loss: 7.9124\n",
      "Iteration 118/1072, Loss: 7.9342\n",
      "Iteration 119/1072, Loss: 7.9451\n",
      "Iteration 120/1072, Loss: 8.0139\n",
      "Iteration 121/1072, Loss: 7.9288\n",
      "Iteration 122/1072, Loss: 7.9733\n",
      "Iteration 123/1072, Loss: 7.9685\n",
      "Iteration 124/1072, Loss: 7.9453\n",
      "Iteration 125/1072, Loss: 7.9423\n",
      "Iteration 126/1072, Loss: 8.0250\n",
      "Iteration 127/1072, Loss: 7.9534\n",
      "Iteration 128/1072, Loss: 7.9455\n",
      "Iteration 129/1072, Loss: 7.9651\n",
      "Iteration 130/1072, Loss: 7.9661\n",
      "Iteration 131/1072, Loss: 7.9859\n",
      "Iteration 132/1072, Loss: 7.9750\n",
      "Iteration 133/1072, Loss: 7.9366\n",
      "Iteration 134/1072, Loss: 7.8975\n",
      "Iteration 135/1072, Loss: 7.9007\n",
      "Iteration 136/1072, Loss: 7.9639\n",
      "Iteration 137/1072, Loss: 7.9397\n",
      "Iteration 138/1072, Loss: 7.9396\n",
      "Iteration 139/1072, Loss: 7.9777\n",
      "Iteration 155/1072, Loss: 7.9710\n",
      "Iteration 156/1072, Loss: 7.9163\n",
      "Iteration 207/1072, Loss: 7.9754\n",
      "Iteration 208/1072, Loss: 7.9357\n",
      "Iteration 209/1072, Loss: 7.9777\n",
      "Iteration 210/1072, Loss: 8.0005\n",
      "Iteration 211/1072, Loss: 7.8973\n",
      "Iteration 212/1072, Loss: 7.9387\n",
      "Iteration 213/1072, Loss: 7.9843\n",
      "Iteration 214/1072, Loss: 7.9964\n",
      "Iteration 215/1072, Loss: 7.9118\n",
      "Iteration 216/1072, Loss: 7.8770\n",
      "Iteration 217/1072, Loss: 7.9520\n",
      "Iteration 218/1072, Loss: 7.9756\n",
      "Iteration 219/1072, Loss: 7.9911\n",
      "Iteration 220/1072, Loss: 7.9761\n",
      "Iteration 221/1072, Loss: 7.9801\n",
      "Iteration 222/1072, Loss: 7.9501\n",
      "Iteration 223/1072, Loss: 7.9350\n",
      "Iteration 224/1072, Loss: 7.9323\n",
      "Iteration 225/1072, Loss: 7.9954\n",
      "Iteration 226/1072, Loss: 7.9616\n",
      "Iteration 227/1072, Loss: 7.9546\n",
      "Iteration 228/1072, Loss: 7.9262\n",
      "Iteration 229/1072, Loss: 7.9159\n",
      "Iteration 230/1072, Loss: 7.9272\n",
      "Iteration 231/1072, Loss: 7.9663\n",
      "Iteration 232/1072, Loss: 7.9393\n",
      "Iteration 233/1072, Loss: 7.9456\n",
      "Iteration 234/1072, Loss: 7.9668\n",
      "Iteration 235/1072, Loss: 8.0112\n",
      "Iteration 236/1072, Loss: 7.9642\n",
      "Iteration 237/1072, Loss: 7.9364\n",
      "Iteration 238/1072, Loss: 8.0027\n",
      "Iteration 239/1072, Loss: 7.9895\n",
      "Iteration 240/1072, Loss: 7.9465\n",
      "Iteration 241/1072, Loss: 7.9224\n",
      "Iteration 242/1072, Loss: 7.9100\n",
      "Iteration 243/1072, Loss: 7.9470\n",
      "Iteration 244/1072, Loss: 7.9586\n",
      "Iteration 245/1072, Loss: 7.9154\n",
      "Iteration 246/1072, Loss: 8.0015\n",
      "Iteration 247/1072, Loss: 7.9340\n",
      "Iteration 248/1072, Loss: 7.9824\n",
      "Iteration 249/1072, Loss: 7.9988\n",
      "Iteration 250/1072, Loss: 7.9481\n",
      "Iteration 251/1072, Loss: 7.8908\n",
      "Iteration 252/1072, Loss: 7.9769\n",
      "Iteration 253/1072, Loss: 7.9522\n",
      "Iteration 254/1072, Loss: 7.9711\n",
      "Iteration 255/1072, Loss: 7.9427\n",
      "Iteration 256/1072, Loss: 7.9291\n",
      "Iteration 257/1072, Loss: 7.8764\n",
      "Iteration 258/1072, Loss: 7.9573\n",
      "Iteration 259/1072, Loss: 7.9674\n",
      "Iteration 260/1072, Loss: 7.9522\n",
      "Iteration 261/1072, Loss: 7.8723\n",
      "Iteration 262/1072, Loss: 7.9711\n",
      "Iteration 263/1072, Loss: 8.0057\n",
      "Iteration 264/1072, Loss: 7.9119\n",
      "Iteration 265/1072, Loss: 7.9209\n",
      "Iteration 266/1072, Loss: 7.9455\n",
      "Iteration 267/1072, Loss: 7.9054\n",
      "Iteration 268/1072, Loss: 7.9370\n",
      "Iteration 269/1072, Loss: 7.9108\n",
      "Iteration 270/1072, Loss: 7.9723\n",
      "Iteration 271/1072, Loss: 8.0017\n",
      "Iteration 272/1072, Loss: 7.9585\n",
      "Iteration 273/1072, Loss: 7.9354\n",
      "Iteration 274/1072, Loss: 7.9662\n",
      "Iteration 275/1072, Loss: 7.9857\n",
      "Iteration 276/1072, Loss: 7.9497\n",
      "Iteration 277/1072, Loss: 7.9781\n",
      "Iteration 278/1072, Loss: 7.9796\n",
      "Iteration 279/1072, Loss: 7.9399\n",
      "Iteration 280/1072, Loss: 7.9712\n",
      "Iteration 281/1072, Loss: 7.8938\n",
      "Iteration 282/1072, Loss: 7.9819\n",
      "Iteration 283/1072, Loss: 7.9481\n",
      "Iteration 284/1072, Loss: 7.9736\n",
      "Iteration 285/1072, Loss: 7.9685\n",
      "Iteration 286/1072, Loss: 7.9210\n",
      "Iteration 287/1072, Loss: 7.9421\n",
      "Iteration 288/1072, Loss: 7.9213\n",
      "Iteration 289/1072, Loss: 7.9734\n",
      "Iteration 290/1072, Loss: 7.9841\n",
      "Iteration 291/1072, Loss: 7.9516\n",
      "Iteration 292/1072, Loss: 7.9219\n",
      "Iteration 293/1072, Loss: 7.9735\n",
      "Iteration 294/1072, Loss: 7.9654\n",
      "Iteration 295/1072, Loss: 7.9226\n",
      "Iteration 296/1072, Loss: 7.9275\n",
      "Iteration 297/1072, Loss: 7.9136\n",
      "Iteration 298/1072, Loss: 7.9396\n",
      "Iteration 299/1072, Loss: 7.9548\n",
      "Iteration 300/1072, Loss: 7.8920\n",
      "Iteration 301/1072, Loss: 7.9907\n",
      "Iteration 302/1072, Loss: 7.9425\n",
      "Iteration 303/1072, Loss: 7.9327\n",
      "Iteration 304/1072, Loss: 7.8840\n",
      "Iteration 305/1072, Loss: 7.9267\n",
      "Iteration 306/1072, Loss: 7.9739\n",
      "Iteration 307/1072, Loss: 7.9417\n",
      "Iteration 308/1072, Loss: 7.9777\n",
      "Iteration 309/1072, Loss: 7.9598\n",
      "Iteration 310/1072, Loss: 7.9999\n",
      "Iteration 311/1072, Loss: 7.9578\n",
      "Iteration 312/1072, Loss: 7.9179\n",
      "Iteration 313/1072, Loss: 7.9230\n",
      "Iteration 314/1072, Loss: 7.9340\n",
      "Iteration 315/1072, Loss: 7.9554\n",
      "Iteration 316/1072, Loss: 7.9878\n",
      "Iteration 317/1072, Loss: 7.9132\n",
      "Iteration 318/1072, Loss: 7.9387\n",
      "Iteration 319/1072, Loss: 7.9720\n",
      "Iteration 320/1072, Loss: 7.9744\n",
      "Iteration 321/1072, Loss: 7.9275\n",
      "Iteration 322/1072, Loss: 7.9202\n",
      "Iteration 323/1072, Loss: 7.9530\n",
      "Iteration 324/1072, Loss: 7.9721\n",
      "Iteration 325/1072, Loss: 8.0001\n",
      "Iteration 326/1072, Loss: 7.9372\n",
      "Iteration 327/1072, Loss: 7.9404\n",
      "Iteration 328/1072, Loss: 7.9226\n",
      "Iteration 329/1072, Loss: 7.9621\n",
      "Iteration 330/1072, Loss: 7.9362\n",
      "Iteration 331/1072, Loss: 7.9004\n",
      "Iteration 332/1072, Loss: 7.9618\n",
      "Iteration 333/1072, Loss: 7.8963\n",
      "Iteration 334/1072, Loss: 7.9608\n",
      "Iteration 335/1072, Loss: 7.9538\n",
      "Iteration 336/1072, Loss: 7.9517\n",
      "Iteration 337/1072, Loss: 7.9920\n",
      "Iteration 338/1072, Loss: 7.9990\n",
      "Iteration 339/1072, Loss: 7.9868\n",
      "Iteration 340/1072, Loss: 7.9649\n",
      "Iteration 341/1072, Loss: 7.9159\n",
      "Iteration 342/1072, Loss: 7.8939\n",
      "Iteration 343/1072, Loss: 7.9740\n",
      "Iteration 344/1072, Loss: 7.9184\n",
      "Iteration 345/1072, Loss: 7.8759\n",
      "Iteration 346/1072, Loss: 7.9521\n",
      "Iteration 347/1072, Loss: 7.9484\n",
      "Iteration 348/1072, Loss: 8.0027\n",
      "Iteration 349/1072, Loss: 8.0113\n",
      "Iteration 350/1072, Loss: 7.9756\n",
      "Iteration 351/1072, Loss: 8.0026\n",
      "Iteration 352/1072, Loss: 7.9573\n",
      "Iteration 353/1072, Loss: 7.9158\n",
      "Iteration 354/1072, Loss: 7.8853\n",
      "Iteration 355/1072, Loss: 7.9529\n",
      "Iteration 356/1072, Loss: 7.9546\n",
      "Iteration 357/1072, Loss: 7.9316\n",
      "Iteration 358/1072, Loss: 7.9693\n",
      "Iteration 359/1072, Loss: 8.0099\n",
      "Iteration 360/1072, Loss: 7.9500\n",
      "Iteration 361/1072, Loss: 7.9419\n",
      "Iteration 362/1072, Loss: 7.9969\n",
      "Iteration 363/1072, Loss: 7.8837\n",
      "Iteration 364/1072, Loss: 7.9863\n",
      "Iteration 365/1072, Loss: 7.9521\n",
      "Iteration 366/1072, Loss: 7.9252\n",
      "Iteration 367/1072, Loss: 7.9136\n",
      "Iteration 368/1072, Loss: 7.9868\n",
      "Iteration 369/1072, Loss: 7.9744\n",
      "Iteration 370/1072, Loss: 7.9360\n",
      "Iteration 371/1072, Loss: 8.0127\n",
      "Iteration 372/1072, Loss: 7.9389\n",
      "Iteration 373/1072, Loss: 7.9220\n",
      "Iteration 374/1072, Loss: 7.9301\n",
      "Iteration 375/1072, Loss: 7.9568\n",
      "Iteration 376/1072, Loss: 7.9385\n",
      "Iteration 377/1072, Loss: 7.9668\n",
      "Iteration 378/1072, Loss: 7.9587\n",
      "Iteration 379/1072, Loss: 7.9514\n",
      "Iteration 380/1072, Loss: 7.9547\n",
      "Iteration 381/1072, Loss: 7.9941\n",
      "Iteration 382/1072, Loss: 7.9329\n",
      "Iteration 383/1072, Loss: 7.9397\n",
      "Iteration 384/1072, Loss: 7.9782\n",
      "Iteration 385/1072, Loss: 7.9410\n",
      "Iteration 386/1072, Loss: 7.9665\n",
      "Iteration 387/1072, Loss: 7.9648\n",
      "Iteration 388/1072, Loss: 7.9380\n",
      "Iteration 389/1072, Loss: 7.9665\n",
      "Iteration 390/1072, Loss: 7.8792\n",
      "Iteration 391/1072, Loss: 7.9491\n",
      "Iteration 392/1072, Loss: 7.9260\n",
      "Iteration 393/1072, Loss: 7.9003\n",
      "Iteration 394/1072, Loss: 7.9260\n",
      "Iteration 395/1072, Loss: 8.0207\n",
      "Iteration 396/1072, Loss: 7.9555\n",
      "Iteration 397/1072, Loss: 7.9029\n",
      "Iteration 398/1072, Loss: 7.9689\n",
      "Iteration 399/1072, Loss: 7.9632\n",
      "Iteration 400/1072, Loss: 7.9907\n",
      "Iteration 401/1072, Loss: 7.9257\n",
      "Iteration 402/1072, Loss: 7.9532\n",
      "Iteration 403/1072, Loss: 7.9790\n",
      "Iteration 404/1072, Loss: 7.9333\n",
      "Iteration 405/1072, Loss: 7.9814\n",
      "Iteration 406/1072, Loss: 7.9234\n",
      "Iteration 407/1072, Loss: 7.9228\n",
      "Iteration 408/1072, Loss: 7.9437\n",
      "Iteration 409/1072, Loss: 7.9800\n",
      "Iteration 410/1072, Loss: 7.9043\n",
      "Iteration 411/1072, Loss: 7.9361\n",
      "Iteration 412/1072, Loss: 7.9936\n",
      "Iteration 413/1072, Loss: 7.9447\n",
      "Iteration 414/1072, Loss: 7.9241\n",
      "Iteration 415/1072, Loss: 7.9353\n",
      "Iteration 416/1072, Loss: 8.0138\n",
      "Iteration 417/1072, Loss: 7.9818\n",
      "Iteration 418/1072, Loss: 7.9785\n",
      "Iteration 419/1072, Loss: 7.9035\n",
      "Iteration 420/1072, Loss: 7.9377\n",
      "Iteration 421/1072, Loss: 7.9653\n",
      "Iteration 422/1072, Loss: 7.9948\n",
      "Iteration 423/1072, Loss: 7.9306\n",
      "Iteration 424/1072, Loss: 7.8713\n",
      "Iteration 425/1072, Loss: 7.9573\n",
      "Iteration 426/1072, Loss: 7.9501\n",
      "Iteration 427/1072, Loss: 7.9890\n",
      "Iteration 428/1072, Loss: 7.9409\n",
      "Iteration 429/1072, Loss: 7.9010\n",
      "Iteration 430/1072, Loss: 7.9418\n",
      "Iteration 431/1072, Loss: 7.9624\n",
      "Iteration 432/1072, Loss: 7.9577\n",
      "Iteration 433/1072, Loss: 7.9438\n",
      "Iteration 434/1072, Loss: 7.9350\n",
      "Iteration 435/1072, Loss: 7.9704\n",
      "Iteration 436/1072, Loss: 7.9237\n",
      "Iteration 437/1072, Loss: 7.9115\n",
      "Iteration 438/1072, Loss: 7.9220\n",
      "Iteration 439/1072, Loss: 7.9144\n",
      "Iteration 440/1072, Loss: 7.9527\n",
      "Iteration 441/1072, Loss: 7.9046\n",
      "Iteration 442/1072, Loss: 7.9247\n",
      "Iteration 443/1072, Loss: 7.9826\n",
      "Iteration 444/1072, Loss: 7.9191\n",
      "Iteration 445/1072, Loss: 7.9527\n",
      "Iteration 446/1072, Loss: 7.8726\n",
      "Iteration 447/1072, Loss: 7.9799\n",
      "Iteration 448/1072, Loss: 7.8870\n",
      "Iteration 449/1072, Loss: 7.9717\n",
      "Iteration 450/1072, Loss: 7.9596\n",
      "Iteration 451/1072, Loss: 7.9426\n",
      "Iteration 452/1072, Loss: 7.9360\n",
      "Iteration 453/1072, Loss: 7.9699\n",
      "Iteration 454/1072, Loss: 8.0052\n",
      "Iteration 455/1072, Loss: 7.9768\n",
      "Iteration 456/1072, Loss: 7.9979\n",
      "Iteration 457/1072, Loss: 7.8914\n",
      "Iteration 458/1072, Loss: 7.9619\n",
      "Iteration 459/1072, Loss: 7.8711\n",
      "Iteration 460/1072, Loss: 7.9185\n",
      "Iteration 461/1072, Loss: 7.9784\n",
      "Iteration 462/1072, Loss: 7.9640\n",
      "Iteration 463/1072, Loss: 7.9542\n",
      "Iteration 464/1072, Loss: 7.9901\n",
      "Iteration 465/1072, Loss: 7.9706\n",
      "Iteration 466/1072, Loss: 7.9648\n",
      "Iteration 467/1072, Loss: 7.9889\n",
      "Iteration 468/1072, Loss: 7.9811\n",
      "Iteration 469/1072, Loss: 7.9257\n",
      "Iteration 470/1072, Loss: 7.9347\n",
      "Iteration 471/1072, Loss: 7.9747\n",
      "Iteration 472/1072, Loss: 7.9359\n",
      "Iteration 473/1072, Loss: 7.9354\n",
      "Iteration 474/1072, Loss: 7.9527\n",
      "Iteration 475/1072, Loss: 7.9602\n",
      "Iteration 476/1072, Loss: 7.9396\n",
      "Iteration 477/1072, Loss: 7.9728\n",
      "Iteration 478/1072, Loss: 7.9434\n",
      "Iteration 479/1072, Loss: 7.9560\n",
      "Iteration 480/1072, Loss: 8.0282\n",
      "Iteration 481/1072, Loss: 7.9584\n",
      "Iteration 482/1072, Loss: 7.9974\n",
      "Iteration 483/1072, Loss: 7.9563\n",
      "Iteration 484/1072, Loss: 7.9378\n",
      "Iteration 485/1072, Loss: 7.9370\n",
      "Iteration 486/1072, Loss: 8.0077\n",
      "Iteration 487/1072, Loss: 7.9241\n",
      "Iteration 488/1072, Loss: 7.9433\n",
      "Iteration 489/1072, Loss: 7.9649\n",
      "Iteration 490/1072, Loss: 7.9519\n",
      "Iteration 491/1072, Loss: 7.9385\n",
      "Iteration 492/1072, Loss: 7.8954\n",
      "Iteration 493/1072, Loss: 7.8823\n",
      "Iteration 494/1072, Loss: 7.9982\n",
      "Iteration 495/1072, Loss: 7.9682\n",
      "Iteration 496/1072, Loss: 7.8983\n",
      "Iteration 497/1072, Loss: 7.9735\n",
      "Iteration 498/1072, Loss: 7.9918\n",
      "Iteration 499/1072, Loss: 7.9232\n",
      "Iteration 500/1072, Loss: 7.9538\n",
      "Iteration 501/1072, Loss: 7.9104\n",
      "Iteration 502/1072, Loss: 7.9437\n",
      "Iteration 503/1072, Loss: 7.9784\n",
      "Iteration 504/1072, Loss: 7.9537\n",
      "Iteration 505/1072, Loss: 7.9665\n",
      "Iteration 506/1072, Loss: 7.9446\n",
      "Iteration 507/1072, Loss: 7.9336\n",
      "Iteration 508/1072, Loss: 7.9968\n",
      "Iteration 509/1072, Loss: 7.9605\n",
      "Iteration 510/1072, Loss: 7.9096\n",
      "Iteration 511/1072, Loss: 7.9935\n",
      "Iteration 512/1072, Loss: 7.9568\n",
      "Iteration 513/1072, Loss: 7.9912\n",
      "Iteration 514/1072, Loss: 7.9215\n",
      "Iteration 515/1072, Loss: 7.9442\n",
      "Iteration 516/1072, Loss: 7.8659\n",
      "Iteration 517/1072, Loss: 7.9607\n",
      "Iteration 518/1072, Loss: 7.8611\n",
      "Iteration 519/1072, Loss: 7.9626\n",
      "Iteration 520/1072, Loss: 7.9368\n",
      "Iteration 521/1072, Loss: 7.9898\n",
      "Iteration 522/1072, Loss: 7.9054\n",
      "Iteration 523/1072, Loss: 7.9531\n",
      "Iteration 524/1072, Loss: 7.9236\n",
      "Iteration 525/1072, Loss: 7.9248\n",
      "Iteration 526/1072, Loss: 7.8982\n",
      "Iteration 527/1072, Loss: 8.0129\n",
      "Iteration 528/1072, Loss: 7.9091\n",
      "Iteration 529/1072, Loss: 7.9244\n",
      "Iteration 530/1072, Loss: 7.9723\n",
      "Iteration 531/1072, Loss: 7.9047\n",
      "Iteration 532/1072, Loss: 7.9815\n",
      "Iteration 533/1072, Loss: 7.9923\n",
      "Iteration 534/1072, Loss: 7.9689\n",
      "Iteration 535/1072, Loss: 7.9188\n",
      "Iteration 536/1072, Loss: 7.9237\n",
      "Iteration 537/1072, Loss: 7.9639\n",
      "Iteration 538/1072, Loss: 7.9126\n",
      "Iteration 539/1072, Loss: 7.9518\n",
      "Iteration 540/1072, Loss: 7.9404\n",
      "Iteration 541/1072, Loss: 7.9558\n",
      "Iteration 542/1072, Loss: 7.9244\n",
      "Iteration 543/1072, Loss: 7.9625\n",
      "Iteration 544/1072, Loss: 7.9490\n",
      "Iteration 545/1072, Loss: 7.9795\n",
      "Iteration 546/1072, Loss: 7.9491\n",
      "Iteration 547/1072, Loss: 7.9914\n",
      "Iteration 548/1072, Loss: 7.9613\n",
      "Iteration 549/1072, Loss: 7.9600\n",
      "Iteration 550/1072, Loss: 7.9411\n",
      "Iteration 551/1072, Loss: 7.9259\n",
      "Iteration 552/1072, Loss: 7.9230\n",
      "Iteration 553/1072, Loss: 7.9447\n",
      "Iteration 554/1072, Loss: 7.9139\n",
      "Iteration 555/1072, Loss: 7.9729\n",
      "Iteration 556/1072, Loss: 7.9467\n",
      "Iteration 557/1072, Loss: 7.9454\n",
      "Iteration 558/1072, Loss: 7.9551\n",
      "Iteration 559/1072, Loss: 7.9159\n",
      "Iteration 560/1072, Loss: 7.9520\n",
      "Iteration 561/1072, Loss: 7.9713\n",
      "Iteration 562/1072, Loss: 7.9858\n",
      "Iteration 563/1072, Loss: 7.8970\n",
      "Iteration 564/1072, Loss: 7.9437\n",
      "Iteration 565/1072, Loss: 7.9919\n",
      "Iteration 566/1072, Loss: 7.9816\n",
      "Iteration 567/1072, Loss: 7.9424\n",
      "Iteration 568/1072, Loss: 7.9063\n",
      "Iteration 569/1072, Loss: 7.9376\n",
      "Iteration 570/1072, Loss: 7.9535\n",
      "Iteration 571/1072, Loss: 7.9157\n",
      "Iteration 572/1072, Loss: 7.9734\n",
      "Iteration 573/1072, Loss: 7.9657\n",
      "Iteration 574/1072, Loss: 7.8908\n",
      "Iteration 575/1072, Loss: 7.9735\n",
      "Iteration 576/1072, Loss: 7.9297\n",
      "Iteration 577/1072, Loss: 7.9404\n",
      "Iteration 578/1072, Loss: 7.9146\n",
      "Iteration 579/1072, Loss: 7.9263\n",
      "Iteration 580/1072, Loss: 7.9341\n",
      "Iteration 581/1072, Loss: 7.9067\n",
      "Iteration 582/1072, Loss: 7.9182\n",
      "Iteration 583/1072, Loss: 7.9307\n",
      "Iteration 584/1072, Loss: 7.9551\n",
      "Iteration 585/1072, Loss: 7.9114\n",
      "Iteration 586/1072, Loss: 7.9626\n",
      "Iteration 587/1072, Loss: 7.9153\n",
      "Iteration 588/1072, Loss: 7.9061\n",
      "Iteration 589/1072, Loss: 7.9460\n",
      "Iteration 590/1072, Loss: 7.9311\n",
      "Iteration 591/1072, Loss: 7.9557\n",
      "Iteration 592/1072, Loss: 7.9816\n",
      "Iteration 593/1072, Loss: 7.9195\n",
      "Iteration 594/1072, Loss: 8.0260\n",
      "Iteration 595/1072, Loss: 7.9104\n",
      "Iteration 596/1072, Loss: 7.9675\n",
      "Iteration 597/1072, Loss: 8.0162\n",
      "Iteration 598/1072, Loss: 7.9853\n",
      "Iteration 599/1072, Loss: 8.0021\n",
      "Iteration 600/1072, Loss: 7.9193\n",
      "Iteration 601/1072, Loss: 7.9651\n",
      "Iteration 602/1072, Loss: 7.9605\n",
      "Iteration 603/1072, Loss: 7.9474\n",
      "Iteration 604/1072, Loss: 7.9153\n",
      "Iteration 605/1072, Loss: 7.9681\n",
      "Iteration 606/1072, Loss: 7.9410\n",
      "Iteration 607/1072, Loss: 7.9266\n",
      "Iteration 608/1072, Loss: 7.9384\n",
      "Iteration 609/1072, Loss: 7.9498\n",
      "Iteration 610/1072, Loss: 7.9367\n",
      "Iteration 611/1072, Loss: 7.9836\n",
      "Iteration 612/1072, Loss: 8.0007\n",
      "Iteration 613/1072, Loss: 7.9788\n",
      "Iteration 614/1072, Loss: 7.8996\n",
      "Iteration 615/1072, Loss: 7.9403\n",
      "Iteration 616/1072, Loss: 7.9341\n",
      "Iteration 617/1072, Loss: 7.9815\n",
      "Iteration 618/1072, Loss: 7.9537\n",
      "Iteration 619/1072, Loss: 7.8671\n",
      "Iteration 620/1072, Loss: 7.9711\n",
      "Iteration 621/1072, Loss: 7.9781\n",
      "Iteration 622/1072, Loss: 7.9863\n",
      "Iteration 623/1072, Loss: 8.0145\n",
      "Iteration 624/1072, Loss: 7.8836\n",
      "Iteration 625/1072, Loss: 7.9647\n",
      "Iteration 626/1072, Loss: 7.9156\n",
      "Iteration 627/1072, Loss: 7.9542\n",
      "Iteration 628/1072, Loss: 7.9705\n",
      "Iteration 629/1072, Loss: 7.9182\n",
      "Iteration 630/1072, Loss: 7.9319\n",
      "Iteration 631/1072, Loss: 7.9322\n",
      "Iteration 632/1072, Loss: 7.9020\n",
      "Iteration 633/1072, Loss: 7.9081\n",
      "Iteration 634/1072, Loss: 7.9604\n",
      "Iteration 635/1072, Loss: 7.9446\n",
      "Iteration 636/1072, Loss: 7.9520\n",
      "Iteration 637/1072, Loss: 7.9391\n",
      "Iteration 638/1072, Loss: 7.9807\n",
      "Iteration 639/1072, Loss: 7.9151\n",
      "Iteration 640/1072, Loss: 7.8736\n",
      "Iteration 641/1072, Loss: 7.9409\n",
      "Iteration 642/1072, Loss: 7.9442\n",
      "Iteration 643/1072, Loss: 7.9543\n",
      "Iteration 644/1072, Loss: 7.9795\n",
      "Iteration 645/1072, Loss: 7.9360\n",
      "Iteration 646/1072, Loss: 7.9986\n",
      "Iteration 647/1072, Loss: 7.9318\n",
      "Iteration 648/1072, Loss: 7.9593\n",
      "Iteration 649/1072, Loss: 7.9087\n",
      "Iteration 650/1072, Loss: 7.9452\n",
      "Iteration 651/1072, Loss: 7.9561\n",
      "Iteration 652/1072, Loss: 7.9259\n",
      "Iteration 653/1072, Loss: 8.0018\n",
      "Iteration 654/1072, Loss: 7.9174\n",
      "Iteration 655/1072, Loss: 7.8923\n",
      "Iteration 656/1072, Loss: 7.9186\n",
      "Iteration 657/1072, Loss: 7.9030\n",
      "Iteration 658/1072, Loss: 7.9515\n",
      "Iteration 659/1072, Loss: 7.9693\n",
      "Iteration 660/1072, Loss: 7.9706\n",
      "Iteration 661/1072, Loss: 7.9164\n",
      "Iteration 662/1072, Loss: 7.9918\n",
      "Iteration 663/1072, Loss: 7.9865\n",
      "Iteration 664/1072, Loss: 7.9030\n",
      "Iteration 665/1072, Loss: 7.8763\n",
      "Iteration 666/1072, Loss: 7.9632\n",
      "Iteration 667/1072, Loss: 7.9071\n",
      "Iteration 668/1072, Loss: 7.9523\n",
      "Iteration 669/1072, Loss: 7.9591\n",
      "Iteration 670/1072, Loss: 7.9166\n",
      "Iteration 671/1072, Loss: 7.9778\n",
      "Iteration 672/1072, Loss: 7.9770\n",
      "Iteration 673/1072, Loss: 7.9801\n",
      "Iteration 674/1072, Loss: 7.9098\n",
      "Iteration 675/1072, Loss: 7.9108\n",
      "Iteration 676/1072, Loss: 7.9965\n",
      "Iteration 677/1072, Loss: 7.9861\n",
      "Iteration 678/1072, Loss: 7.9457\n",
      "Iteration 679/1072, Loss: 7.9384\n",
      "Iteration 680/1072, Loss: 7.9772\n",
      "Iteration 681/1072, Loss: 7.9308\n",
      "Iteration 682/1072, Loss: 7.9487\n",
      "Iteration 683/1072, Loss: 7.9189\n",
      "Iteration 684/1072, Loss: 7.9668\n",
      "Iteration 685/1072, Loss: 7.9649\n",
      "Iteration 686/1072, Loss: 7.9129\n",
      "Iteration 687/1072, Loss: 8.0069\n",
      "Iteration 688/1072, Loss: 7.9811\n",
      "Iteration 689/1072, Loss: 7.9630\n",
      "Iteration 690/1072, Loss: 7.8725\n",
      "Iteration 691/1072, Loss: 7.9615\n",
      "Iteration 692/1072, Loss: 8.0062\n",
      "Iteration 693/1072, Loss: 7.9397\n",
      "Iteration 694/1072, Loss: 7.8939\n",
      "Iteration 695/1072, Loss: 7.9342\n",
      "Iteration 696/1072, Loss: 7.9367\n",
      "Iteration 697/1072, Loss: 7.9643\n",
      "Iteration 698/1072, Loss: 7.8977\n",
      "Iteration 699/1072, Loss: 7.9172\n",
      "Iteration 700/1072, Loss: 7.9507\n",
      "Iteration 701/1072, Loss: 7.9288\n",
      "Iteration 702/1072, Loss: 7.9410\n",
      "Iteration 703/1072, Loss: 7.9462\n",
      "Iteration 704/1072, Loss: 7.9080\n",
      "Iteration 705/1072, Loss: 7.9312\n",
      "Iteration 706/1072, Loss: 7.9123\n",
      "Iteration 707/1072, Loss: 7.9955\n",
      "Iteration 708/1072, Loss: 7.9443\n",
      "Iteration 709/1072, Loss: 7.9541\n",
      "Iteration 710/1072, Loss: 7.9821\n",
      "Iteration 711/1072, Loss: 7.9181\n",
      "Iteration 712/1072, Loss: 7.9793\n",
      "Iteration 713/1072, Loss: 7.9247\n",
      "Iteration 714/1072, Loss: 7.9563\n",
      "Iteration 715/1072, Loss: 7.9574\n",
      "Iteration 716/1072, Loss: 7.9334\n",
      "Iteration 717/1072, Loss: 7.9012\n",
      "Iteration 718/1072, Loss: 7.9696\n",
      "Iteration 719/1072, Loss: 7.9676\n",
      "Iteration 720/1072, Loss: 7.8723\n",
      "Iteration 721/1072, Loss: 7.9951\n",
      "Iteration 722/1072, Loss: 7.9056\n",
      "Iteration 723/1072, Loss: 7.8650\n",
      "Iteration 724/1072, Loss: 7.9369\n",
      "Iteration 725/1072, Loss: 7.9643\n",
      "Iteration 726/1072, Loss: 7.9587\n",
      "Iteration 727/1072, Loss: 7.9385\n",
      "Iteration 728/1072, Loss: 7.9592\n",
      "Iteration 729/1072, Loss: 7.9378\n",
      "Iteration 730/1072, Loss: 7.8948\n",
      "Iteration 731/1072, Loss: 7.9499\n",
      "Iteration 732/1072, Loss: 7.9510\n",
      "Iteration 733/1072, Loss: 7.9380\n",
      "Iteration 734/1072, Loss: 7.9107\n",
      "Iteration 735/1072, Loss: 7.9513\n",
      "Iteration 736/1072, Loss: 7.9345\n",
      "Iteration 737/1072, Loss: 7.9634\n",
      "Iteration 738/1072, Loss: 7.9359\n",
      "Iteration 739/1072, Loss: 7.9924\n",
      "Iteration 740/1072, Loss: 7.9236\n",
      "Iteration 741/1072, Loss: 7.9824\n",
      "Iteration 742/1072, Loss: 8.0210\n",
      "Iteration 743/1072, Loss: 7.9640\n",
      "Iteration 744/1072, Loss: 8.0017\n",
      "Iteration 745/1072, Loss: 7.9539\n",
      "Iteration 746/1072, Loss: 8.0064\n",
      "Iteration 747/1072, Loss: 7.8753\n",
      "Iteration 748/1072, Loss: 7.9528\n",
      "Iteration 749/1072, Loss: 7.9623\n",
      "Iteration 750/1072, Loss: 7.9179\n",
      "Iteration 751/1072, Loss: 7.9528\n",
      "Iteration 752/1072, Loss: 7.9344\n",
      "Iteration 753/1072, Loss: 7.9471\n",
      "Iteration 754/1072, Loss: 7.9746\n",
      "Iteration 755/1072, Loss: 7.9915\n",
      "Iteration 756/1072, Loss: 7.8863\n",
      "Iteration 757/1072, Loss: 7.9305\n",
      "Iteration 758/1072, Loss: 7.9136\n",
      "Iteration 759/1072, Loss: 7.9413\n",
      "Iteration 760/1072, Loss: 7.8459\n",
      "Iteration 761/1072, Loss: 7.9216\n",
      "Iteration 762/1072, Loss: 7.9700\n",
      "Iteration 763/1072, Loss: 7.9289\n",
      "Iteration 764/1072, Loss: 7.8870\n",
      "Iteration 765/1072, Loss: 7.9093\n",
      "Iteration 766/1072, Loss: 7.8921\n",
      "Iteration 767/1072, Loss: 7.9187\n",
      "Iteration 768/1072, Loss: 7.9092\n",
      "Iteration 769/1072, Loss: 7.9816\n",
      "Iteration 770/1072, Loss: 7.9625\n",
      "Iteration 771/1072, Loss: 7.9323\n",
      "Iteration 772/1072, Loss: 7.9714\n",
      "Iteration 773/1072, Loss: 7.9283\n",
      "Iteration 774/1072, Loss: 7.9932\n",
      "Iteration 775/1072, Loss: 7.9646\n",
      "Iteration 776/1072, Loss: 7.8939\n",
      "Iteration 777/1072, Loss: 7.9919\n",
      "Iteration 778/1072, Loss: 7.9355\n",
      "Iteration 779/1072, Loss: 7.9584\n",
      "Iteration 780/1072, Loss: 7.9298\n",
      "Iteration 781/1072, Loss: 7.9901\n",
      "Iteration 782/1072, Loss: 7.8860\n",
      "Iteration 783/1072, Loss: 7.9304\n",
      "Iteration 784/1072, Loss: 7.9317\n",
      "Iteration 785/1072, Loss: 7.9419\n",
      "Iteration 786/1072, Loss: 7.8722\n",
      "Iteration 787/1072, Loss: 7.9625\n",
      "Iteration 788/1072, Loss: 7.9370\n",
      "Iteration 789/1072, Loss: 7.9348\n",
      "Iteration 790/1072, Loss: 7.9610\n",
      "Iteration 791/1072, Loss: 7.8644\n",
      "Iteration 792/1072, Loss: 7.9775\n",
      "Iteration 793/1072, Loss: 7.9340\n",
      "Iteration 794/1072, Loss: 7.9456\n",
      "Iteration 795/1072, Loss: 7.9246\n",
      "Iteration 796/1072, Loss: 7.9074\n",
      "Iteration 797/1072, Loss: 7.9751\n",
      "Iteration 798/1072, Loss: 7.9487\n",
      "Iteration 799/1072, Loss: 7.9861\n",
      "Iteration 800/1072, Loss: 8.0012\n",
      "Iteration 801/1072, Loss: 7.9406\n",
      "Iteration 802/1072, Loss: 7.9345\n",
      "Iteration 803/1072, Loss: 7.9114\n",
      "Iteration 804/1072, Loss: 7.9147\n",
      "Iteration 805/1072, Loss: 7.9146\n",
      "Iteration 806/1072, Loss: 7.9922\n",
      "Iteration 807/1072, Loss: 7.9595\n",
      "Iteration 808/1072, Loss: 7.9417\n",
      "Iteration 809/1072, Loss: 7.9308\n",
      "Iteration 810/1072, Loss: 7.9821\n",
      "Iteration 811/1072, Loss: 7.9665\n",
      "Iteration 812/1072, Loss: 7.9841\n",
      "Iteration 813/1072, Loss: 7.9345\n",
      "Iteration 814/1072, Loss: 7.8794\n",
      "Iteration 815/1072, Loss: 7.9162\n",
      "Iteration 816/1072, Loss: 7.9096\n",
      "Iteration 817/1072, Loss: 7.9341\n",
      "Iteration 818/1072, Loss: 7.9171\n",
      "Iteration 819/1072, Loss: 7.8831\n",
      "Iteration 820/1072, Loss: 7.9531\n",
      "Iteration 821/1072, Loss: 7.8778\n",
      "Iteration 822/1072, Loss: 7.9416\n",
      "Iteration 823/1072, Loss: 7.9404\n",
      "Iteration 824/1072, Loss: 7.9589\n",
      "Iteration 825/1072, Loss: 7.9757\n",
      "Iteration 826/1072, Loss: 7.9430\n",
      "Iteration 827/1072, Loss: 7.9112\n",
      "Iteration 828/1072, Loss: 7.8362\n",
      "Iteration 829/1072, Loss: 7.9919\n",
      "Iteration 830/1072, Loss: 7.9849\n",
      "Iteration 831/1072, Loss: 7.9555\n",
      "Iteration 832/1072, Loss: 7.9507\n",
      "Iteration 833/1072, Loss: 7.9161\n",
      "Iteration 834/1072, Loss: 7.9190\n",
      "Iteration 835/1072, Loss: 7.9356\n",
      "Iteration 836/1072, Loss: 7.9651\n",
      "Iteration 837/1072, Loss: 7.9158\n",
      "Iteration 838/1072, Loss: 7.9912\n",
      "Iteration 839/1072, Loss: 7.9166\n",
      "Iteration 840/1072, Loss: 7.9629\n",
      "Iteration 841/1072, Loss: 7.8986\n",
      "Iteration 842/1072, Loss: 7.9332\n",
      "Iteration 843/1072, Loss: 7.9226\n",
      "Iteration 844/1072, Loss: 8.0047\n",
      "Iteration 845/1072, Loss: 7.9380\n",
      "Iteration 846/1072, Loss: 7.9685\n",
      "Iteration 847/1072, Loss: 7.9412\n",
      "Iteration 848/1072, Loss: 7.8643\n",
      "Iteration 849/1072, Loss: 7.9355\n",
      "Iteration 850/1072, Loss: 7.8857\n",
      "Iteration 851/1072, Loss: 7.9183\n",
      "Iteration 852/1072, Loss: 7.9624\n",
      "Iteration 853/1072, Loss: 7.9219\n",
      "Iteration 854/1072, Loss: 7.9074\n",
      "Iteration 855/1072, Loss: 7.8762\n",
      "Iteration 856/1072, Loss: 7.9115\n",
      "Iteration 857/1072, Loss: 7.9443\n",
      "Iteration 858/1072, Loss: 7.9487\n",
      "Iteration 859/1072, Loss: 7.9506\n",
      "Iteration 860/1072, Loss: 7.9246\n",
      "Iteration 861/1072, Loss: 7.9835\n",
      "Iteration 862/1072, Loss: 7.9004\n",
      "Iteration 863/1072, Loss: 7.9452\n",
      "Iteration 864/1072, Loss: 7.9943\n",
      "Iteration 865/1072, Loss: 7.9406\n",
      "Iteration 866/1072, Loss: 7.9521\n",
      "Iteration 867/1072, Loss: 7.9773\n",
      "Iteration 868/1072, Loss: 7.9408\n",
      "Iteration 869/1072, Loss: 7.8888\n",
      "Iteration 870/1072, Loss: 7.9769\n",
      "Iteration 871/1072, Loss: 7.9046\n",
      "Iteration 872/1072, Loss: 7.8940\n",
      "Iteration 873/1072, Loss: 7.9137\n",
      "Iteration 874/1072, Loss: 7.8648\n",
      "Iteration 875/1072, Loss: 7.9899\n",
      "Iteration 876/1072, Loss: 7.9304\n",
      "Iteration 877/1072, Loss: 7.9349\n",
      "Iteration 878/1072, Loss: 7.9333\n",
      "Iteration 879/1072, Loss: 7.8917\n",
      "Iteration 880/1072, Loss: 7.9253\n",
      "Iteration 881/1072, Loss: 7.9772\n",
      "Iteration 882/1072, Loss: 7.8876\n",
      "Iteration 883/1072, Loss: 7.9441\n",
      "Iteration 884/1072, Loss: 7.9508\n",
      "Iteration 885/1072, Loss: 7.9346\n",
      "Iteration 886/1072, Loss: 7.8928\n",
      "Iteration 887/1072, Loss: 7.9229\n",
      "Iteration 888/1072, Loss: 7.9299\n",
      "Iteration 889/1072, Loss: 7.9417\n",
      "Iteration 890/1072, Loss: 7.9131\n",
      "Iteration 891/1072, Loss: 7.9107\n",
      "Iteration 892/1072, Loss: 7.8853\n",
      "Iteration 893/1072, Loss: 7.9695\n",
      "Iteration 894/1072, Loss: 7.9070\n",
      "Iteration 895/1072, Loss: 7.9468\n",
      "Iteration 896/1072, Loss: 7.9531\n",
      "Iteration 897/1072, Loss: 7.9833\n",
      "Iteration 898/1072, Loss: 7.9064\n",
      "Iteration 899/1072, Loss: 7.9698\n",
      "Iteration 900/1072, Loss: 7.8756\n",
      "Iteration 901/1072, Loss: 7.9447\n",
      "Iteration 902/1072, Loss: 7.8526\n",
      "Iteration 903/1072, Loss: 7.9752\n",
      "Iteration 904/1072, Loss: 7.9120\n",
      "Iteration 905/1072, Loss: 7.9212\n",
      "Iteration 906/1072, Loss: 7.9579\n",
      "Iteration 907/1072, Loss: 7.9331\n",
      "Iteration 908/1072, Loss: 7.9591\n",
      "Iteration 909/1072, Loss: 8.0012\n",
      "Iteration 910/1072, Loss: 7.9406\n",
      "Iteration 911/1072, Loss: 8.0017\n",
      "Iteration 912/1072, Loss: 7.8936\n",
      "Iteration 913/1072, Loss: 7.9909\n",
      "Iteration 914/1072, Loss: 7.9401\n",
      "Iteration 915/1072, Loss: 7.9149\n",
      "Iteration 916/1072, Loss: 7.9248\n",
      "Iteration 917/1072, Loss: 7.9360\n",
      "Iteration 918/1072, Loss: 7.9551\n",
      "Iteration 919/1072, Loss: 7.9737\n",
      "Iteration 920/1072, Loss: 7.9763\n",
      "Iteration 921/1072, Loss: 7.9423\n",
      "Iteration 922/1072, Loss: 7.9441\n",
      "Iteration 923/1072, Loss: 7.8943\n",
      "Iteration 924/1072, Loss: 7.9958\n",
      "Iteration 925/1072, Loss: 7.9038\n",
      "Iteration 926/1072, Loss: 7.9914\n",
      "Iteration 927/1072, Loss: 7.8725\n",
      "Iteration 928/1072, Loss: 7.8946\n",
      "Iteration 929/1072, Loss: 7.9421\n",
      "Iteration 930/1072, Loss: 7.9384\n",
      "Iteration 931/1072, Loss: 7.9053\n",
      "Iteration 932/1072, Loss: 7.9539\n",
      "Iteration 933/1072, Loss: 7.9756\n",
      "Iteration 934/1072, Loss: 7.9514\n",
      "Iteration 935/1072, Loss: 7.8944\n",
      "Iteration 936/1072, Loss: 7.9739\n",
      "Iteration 937/1072, Loss: 7.9391\n",
      "Iteration 938/1072, Loss: 7.9877\n",
      "Iteration 939/1072, Loss: 8.0178\n",
      "Iteration 940/1072, Loss: 7.8734\n",
      "Iteration 941/1072, Loss: 7.9388\n",
      "Iteration 942/1072, Loss: 7.9939\n",
      "Iteration 943/1072, Loss: 7.9435\n",
      "Iteration 944/1072, Loss: 7.9562\n",
      "Iteration 945/1072, Loss: 7.9318\n",
      "Iteration 946/1072, Loss: 7.9134\n",
      "Iteration 947/1072, Loss: 7.9682\n",
      "Iteration 948/1072, Loss: 7.9348\n",
      "Iteration 949/1072, Loss: 7.9218\n",
      "Iteration 950/1072, Loss: 7.9165\n",
      "Iteration 951/1072, Loss: 7.9506\n",
      "Iteration 952/1072, Loss: 7.9113\n",
      "Iteration 953/1072, Loss: 7.9173\n",
      "Iteration 954/1072, Loss: 7.8988\n",
      "Iteration 955/1072, Loss: 7.8987\n",
      "Iteration 956/1072, Loss: 7.9961\n",
      "Iteration 957/1072, Loss: 7.9011\n",
      "Iteration 958/1072, Loss: 7.9372\n",
      "Iteration 959/1072, Loss: 7.8524\n",
      "Iteration 960/1072, Loss: 7.8982\n",
      "Iteration 961/1072, Loss: 7.9463\n",
      "Iteration 962/1072, Loss: 7.9394\n",
      "Iteration 963/1072, Loss: 7.8941\n",
      "Iteration 964/1072, Loss: 7.8461\n",
      "Iteration 965/1072, Loss: 7.9617\n",
      "Iteration 966/1072, Loss: 7.9289\n",
      "Iteration 967/1072, Loss: 7.9736\n",
      "Iteration 968/1072, Loss: 7.9397\n",
      "Iteration 969/1072, Loss: 7.8719\n",
      "Iteration 970/1072, Loss: 7.9090\n",
      "Iteration 971/1072, Loss: 7.9619\n",
      "Iteration 972/1072, Loss: 7.9478\n",
      "Iteration 973/1072, Loss: 8.0076\n",
      "Iteration 974/1072, Loss: 7.9157\n",
      "Iteration 975/1072, Loss: 7.9549\n",
      "Iteration 976/1072, Loss: 7.9220\n",
      "Iteration 977/1072, Loss: 7.9801\n",
      "Iteration 978/1072, Loss: 7.9068\n",
      "Iteration 979/1072, Loss: 7.9071\n",
      "Iteration 980/1072, Loss: 7.9576\n",
      "Iteration 981/1072, Loss: 7.9341\n",
      "Iteration 982/1072, Loss: 7.9095\n",
      "Iteration 983/1072, Loss: 7.9808\n",
      "Iteration 984/1072, Loss: 7.9148\n",
      "Iteration 985/1072, Loss: 7.9721\n",
      "Iteration 986/1072, Loss: 7.9760\n",
      "Iteration 987/1072, Loss: 7.9875\n",
      "Iteration 988/1072, Loss: 7.9060\n",
      "Iteration 989/1072, Loss: 7.9263\n",
      "Iteration 990/1072, Loss: 7.9801\n",
      "Iteration 991/1072, Loss: 7.9459\n",
      "Iteration 992/1072, Loss: 7.8663\n",
      "Iteration 993/1072, Loss: 7.9719\n",
      "Iteration 994/1072, Loss: 7.8559\n",
      "Iteration 995/1072, Loss: 7.9453\n",
      "Iteration 996/1072, Loss: 7.9230\n",
      "Iteration 997/1072, Loss: 7.9216\n",
      "Iteration 998/1072, Loss: 7.9276\n",
      "Iteration 999/1072, Loss: 7.8506\n",
      "Iteration 1000/1072, Loss: 7.9349\n",
      "Iteration 1001/1072, Loss: 7.9567\n",
      "Iteration 1002/1072, Loss: 7.9058\n",
      "Iteration 1003/1072, Loss: 7.9108\n",
      "Iteration 1004/1072, Loss: 7.9212\n",
      "Iteration 1005/1072, Loss: 7.9604\n",
      "Iteration 1006/1072, Loss: 7.9642\n",
      "Iteration 1007/1072, Loss: 7.9401\n",
      "Iteration 1008/1072, Loss: 7.9238\n",
      "Iteration 1009/1072, Loss: 7.9383\n",
      "Iteration 1010/1072, Loss: 7.8949\n",
      "Iteration 1011/1072, Loss: 8.0366\n",
      "Iteration 1012/1072, Loss: 7.9935\n",
      "Iteration 1013/1072, Loss: 7.9775\n",
      "Iteration 1014/1072, Loss: 7.9701\n",
      "Iteration 1015/1072, Loss: 7.9104\n",
      "Iteration 1016/1072, Loss: 7.9752\n",
      "Iteration 1017/1072, Loss: 7.9492\n",
      "Iteration 1018/1072, Loss: 7.9918\n",
      "Iteration 1019/1072, Loss: 7.9234\n",
      "Iteration 1020/1072, Loss: 7.9818\n",
      "Iteration 1021/1072, Loss: 7.8756\n",
      "Iteration 1022/1072, Loss: 7.9805\n",
      "Iteration 1023/1072, Loss: 7.9049\n",
      "Iteration 1024/1072, Loss: 7.9201\n",
      "Iteration 1025/1072, Loss: 7.9734\n",
      "Iteration 1026/1072, Loss: 7.9042\n",
      "Iteration 1027/1072, Loss: 7.9472\n",
      "Iteration 1028/1072, Loss: 7.9846\n",
      "Iteration 1029/1072, Loss: 7.9526\n",
      "Iteration 1030/1072, Loss: 7.9032\n",
      "Iteration 1031/1072, Loss: 7.9087\n",
      "Iteration 1032/1072, Loss: 7.9243\n",
      "Iteration 1033/1072, Loss: 7.9059\n",
      "Iteration 1034/1072, Loss: 7.9065\n",
      "Iteration 1035/1072, Loss: 7.9264\n",
      "Iteration 1036/1072, Loss: 7.9197\n",
      "Iteration 1037/1072, Loss: 7.9892\n",
      "Iteration 1038/1072, Loss: 7.9697\n",
      "Iteration 1039/1072, Loss: 7.9554\n",
      "Iteration 1040/1072, Loss: 7.8993\n",
      "Iteration 1041/1072, Loss: 7.9072\n",
      "Iteration 1042/1072, Loss: 7.8965\n",
      "Iteration 1043/1072, Loss: 7.9315\n",
      "Iteration 1044/1072, Loss: 7.9198\n",
      "Iteration 1045/1072, Loss: 7.9467\n",
      "Iteration 1046/1072, Loss: 8.0038\n",
      "Iteration 1047/1072, Loss: 7.9278\n",
      "Iteration 1048/1072, Loss: 7.9247\n",
      "Iteration 1049/1072, Loss: 7.9258\n",
      "Iteration 1050/1072, Loss: 7.9383\n",
      "Iteration 1051/1072, Loss: 7.9338\n",
      "Iteration 1052/1072, Loss: 7.9336\n",
      "Iteration 1053/1072, Loss: 7.9563\n",
      "Iteration 1054/1072, Loss: 7.8969\n",
      "Iteration 1055/1072, Loss: 7.8825\n",
      "Iteration 1056/1072, Loss: 7.9715\n",
      "Iteration 1057/1072, Loss: 7.9470\n",
      "Iteration 1058/1072, Loss: 7.9203\n",
      "Iteration 1059/1072, Loss: 7.9739\n",
      "Iteration 1060/1072, Loss: 7.9912\n",
      "Iteration 1061/1072, Loss: 7.9522\n",
      "Iteration 1062/1072, Loss: 7.8955\n",
      "Iteration 1063/1072, Loss: 7.9299\n",
      "Iteration 1064/1072, Loss: 7.9718\n",
      "Iteration 1065/1072, Loss: 7.9292\n",
      "Iteration 1066/1072, Loss: 7.9732\n",
      "Iteration 1067/1072, Loss: 7.9090\n",
      "Iteration 1068/1072, Loss: 7.9808\n",
      "Iteration 1069/1072, Loss: 7.9441\n",
      "Iteration 1070/1072, Loss: 7.8579\n",
      "Iteration 1071/1072, Loss: 7.8975\n",
      "Iteration 1072/1072, Loss: 8.0979\n",
      "Epoch 3/10, Loss: 7.9463\n",
      "Checkpoint saved at checkpoints/efficientnet-b4_epoch_3.pth\n",
      "Validation Accuracy: 0.45%\n",
      "Iteration 1/1072, Loss: 7.8923\n",
      "Iteration 2/1072, Loss: 7.8880\n",
      "Iteration 3/1072, Loss: 7.9582\n",
      "Iteration 4/1072, Loss: 7.9314\n",
      "Iteration 5/1072, Loss: 7.9667\n",
      "Iteration 6/1072, Loss: 7.9502\n",
      "Iteration 7/1072, Loss: 7.8556\n",
      "Iteration 8/1072, Loss: 7.9286\n",
      "Iteration 9/1072, Loss: 7.8549\n",
      "Iteration 10/1072, Loss: 7.9213\n",
      "Iteration 11/1072, Loss: 7.8726\n",
      "Iteration 12/1072, Loss: 7.9005\n",
      "Iteration 13/1072, Loss: 7.8739\n",
      "Iteration 14/1072, Loss: 7.8994\n",
      "Iteration 15/1072, Loss: 7.8690\n",
      "Iteration 16/1072, Loss: 7.8815\n",
      "Iteration 17/1072, Loss: 7.8975\n",
      "Iteration 18/1072, Loss: 7.9229\n",
      "Iteration 19/1072, Loss: 7.8951\n",
      "Iteration 20/1072, Loss: 7.9163\n",
      "Iteration 21/1072, Loss: 7.9047\n",
      "Iteration 22/1072, Loss: 7.8628\n",
      "Iteration 23/1072, Loss: 7.9787\n",
      "Iteration 24/1072, Loss: 7.8144\n",
      "Iteration 25/1072, Loss: 7.8961\n",
      "Iteration 26/1072, Loss: 7.8188\n",
      "Iteration 27/1072, Loss: 7.8978\n",
      "Iteration 28/1072, Loss: 7.8733\n",
      "Iteration 29/1072, Loss: 7.9208\n",
      "Iteration 30/1072, Loss: 7.9154\n",
      "Iteration 31/1072, Loss: 7.8945\n",
      "Iteration 32/1072, Loss: 7.9187\n",
      "Iteration 33/1072, Loss: 7.8980\n",
      "Iteration 34/1072, Loss: 7.8863\n",
      "Iteration 35/1072, Loss: 7.8996\n",
      "Iteration 36/1072, Loss: 7.9271\n",
      "Iteration 37/1072, Loss: 7.8839\n",
      "Iteration 38/1072, Loss: 7.8889\n",
      "Iteration 39/1072, Loss: 7.9209\n",
      "Iteration 40/1072, Loss: 7.9014\n",
      "Iteration 41/1072, Loss: 7.8654\n",
      "Iteration 42/1072, Loss: 7.8734\n",
      "Iteration 43/1072, Loss: 7.9041\n",
      "Iteration 44/1072, Loss: 7.9343\n",
      "Iteration 45/1072, Loss: 7.8407\n",
      "Iteration 46/1072, Loss: 7.8974\n",
      "Iteration 47/1072, Loss: 7.8857\n",
      "Iteration 48/1072, Loss: 7.9299\n",
      "Iteration 49/1072, Loss: 7.8815\n",
      "Iteration 50/1072, Loss: 7.8722\n",
      "Iteration 51/1072, Loss: 7.8624\n",
      "Iteration 52/1072, Loss: 7.9086\n",
      "Iteration 53/1072, Loss: 7.9358\n",
      "Iteration 54/1072, Loss: 7.9355\n",
      "Iteration 55/1072, Loss: 7.8875\n",
      "Iteration 56/1072, Loss: 7.8661\n",
      "Iteration 57/1072, Loss: 7.9047\n",
      "Iteration 58/1072, Loss: 7.9183\n",
      "Iteration 59/1072, Loss: 7.9132\n",
      "Iteration 60/1072, Loss: 7.8940\n",
      "Iteration 61/1072, Loss: 7.8851\n",
      "Iteration 62/1072, Loss: 7.8902\n",
      "Iteration 63/1072, Loss: 7.8977\n",
      "Iteration 64/1072, Loss: 7.8363\n",
      "Iteration 65/1072, Loss: 7.8904\n",
      "Iteration 66/1072, Loss: 7.9522\n",
      "Iteration 67/1072, Loss: 7.9215\n",
      "Iteration 68/1072, Loss: 7.9157\n",
      "Iteration 69/1072, Loss: 7.9247\n",
      "Iteration 70/1072, Loss: 7.9160\n",
      "Iteration 71/1072, Loss: 7.8907\n",
      "Iteration 72/1072, Loss: 7.7831\n",
      "Iteration 73/1072, Loss: 7.8807\n",
      "Iteration 74/1072, Loss: 7.9275\n",
      "Iteration 75/1072, Loss: 7.8277\n",
      "Iteration 76/1072, Loss: 7.9295\n",
      "Iteration 77/1072, Loss: 7.8694\n",
      "Iteration 78/1072, Loss: 7.9376\n",
      "Iteration 79/1072, Loss: 7.8955\n",
      "Iteration 80/1072, Loss: 7.8774\n",
      "Iteration 81/1072, Loss: 7.8959\n",
      "Iteration 82/1072, Loss: 7.9094\n",
      "Iteration 83/1072, Loss: 7.9378\n",
      "Iteration 84/1072, Loss: 7.9241\n",
      "Iteration 85/1072, Loss: 7.9104\n",
      "Iteration 86/1072, Loss: 7.9483\n",
      "Iteration 87/1072, Loss: 7.9321\n",
      "Iteration 88/1072, Loss: 7.8978\n",
      "Iteration 89/1072, Loss: 7.9067\n",
      "Iteration 90/1072, Loss: 7.9265\n",
      "Iteration 91/1072, Loss: 7.8522\n",
      "Iteration 92/1072, Loss: 7.8698\n",
      "Iteration 93/1072, Loss: 7.9215\n",
      "Iteration 94/1072, Loss: 7.9153\n",
      "Iteration 95/1072, Loss: 7.8695\n",
      "Iteration 96/1072, Loss: 7.9544\n",
      "Iteration 97/1072, Loss: 7.9154\n",
      "Iteration 98/1072, Loss: 7.8860\n",
      "Iteration 99/1072, Loss: 7.9688\n",
      "Iteration 100/1072, Loss: 7.8306\n",
      "Iteration 101/1072, Loss: 7.8874\n",
      "Iteration 102/1072, Loss: 7.9101\n",
      "Iteration 103/1072, Loss: 7.9603\n",
      "Iteration 104/1072, Loss: 7.9347\n",
      "Iteration 105/1072, Loss: 7.8743\n",
      "Iteration 106/1072, Loss: 7.8525\n",
      "Iteration 107/1072, Loss: 7.8945\n",
      "Iteration 108/1072, Loss: 7.9173\n",
      "Iteration 109/1072, Loss: 7.9417\n",
      "Iteration 110/1072, Loss: 7.8630\n",
      "Iteration 111/1072, Loss: 7.9512\n",
      "Iteration 112/1072, Loss: 7.8978\n",
      "Iteration 113/1072, Loss: 7.8700\n",
      "Iteration 114/1072, Loss: 7.9320\n",
      "Iteration 115/1072, Loss: 7.8363\n",
      "Iteration 116/1072, Loss: 7.8498\n",
      "Iteration 117/1072, Loss: 7.9223\n",
      "Iteration 118/1072, Loss: 7.8670\n",
      "Iteration 119/1072, Loss: 7.8783\n",
      "Iteration 120/1072, Loss: 7.8824\n",
      "Iteration 121/1072, Loss: 7.8698\n",
      "Iteration 122/1072, Loss: 7.9304\n",
      "Iteration 123/1072, Loss: 7.9333\n",
      "Iteration 124/1072, Loss: 7.9456\n",
      "Iteration 125/1072, Loss: 7.8586\n",
      "Iteration 126/1072, Loss: 7.9165\n",
      "Iteration 127/1072, Loss: 7.9051\n",
      "Iteration 128/1072, Loss: 7.9010\n",
      "Iteration 129/1072, Loss: 7.8861\n",
      "Iteration 130/1072, Loss: 7.8779\n",
      "Iteration 131/1072, Loss: 7.9101\n",
      "Iteration 132/1072, Loss: 7.8457\n",
      "Iteration 133/1072, Loss: 7.9391\n",
      "Iteration 134/1072, Loss: 7.9170\n",
      "Iteration 135/1072, Loss: 7.9569\n",
      "Iteration 136/1072, Loss: 7.8647\n",
      "Iteration 137/1072, Loss: 7.9077\n",
      "Iteration 138/1072, Loss: 7.9195\n",
      "Iteration 139/1072, Loss: 7.8534\n",
      "Iteration 140/1072, Loss: 7.9015\n",
      "Iteration 141/1072, Loss: 7.8738\n",
      "Iteration 142/1072, Loss: 7.9018\n",
      "Iteration 143/1072, Loss: 7.8609\n",
      "Iteration 144/1072, Loss: 7.9112\n",
      "Iteration 145/1072, Loss: 7.9018\n",
      "Iteration 146/1072, Loss: 7.8968\n",
      "Iteration 147/1072, Loss: 7.9218\n",
      "Iteration 148/1072, Loss: 7.8498\n",
      "Iteration 149/1072, Loss: 7.9364\n",
      "Iteration 150/1072, Loss: 7.9683\n",
      "Iteration 151/1072, Loss: 7.9203\n",
      "Iteration 152/1072, Loss: 7.8357\n",
      "Iteration 153/1072, Loss: 7.8512\n",
      "Iteration 154/1072, Loss: 7.9505\n",
      "Iteration 155/1072, Loss: 7.9112\n",
      "Iteration 156/1072, Loss: 7.9197\n",
      "Iteration 157/1072, Loss: 7.8857\n",
      "Iteration 158/1072, Loss: 7.8782\n",
      "Iteration 159/1072, Loss: 7.9157\n",
      "Iteration 160/1072, Loss: 7.8740\n",
      "Iteration 161/1072, Loss: 7.8438\n",
      "Iteration 162/1072, Loss: 7.9344\n",
      "Iteration 163/1072, Loss: 7.8762\n",
      "Iteration 164/1072, Loss: 7.9091\n",
      "Iteration 165/1072, Loss: 7.9382\n",
      "Iteration 166/1072, Loss: 7.9299\n",
      "Iteration 167/1072, Loss: 7.8430\n",
      "Iteration 168/1072, Loss: 7.9090\n",
      "Iteration 169/1072, Loss: 7.8867\n",
      "Iteration 170/1072, Loss: 7.9622\n",
      "Iteration 171/1072, Loss: 7.9001\n",
      "Iteration 172/1072, Loss: 7.8636\n",
      "Iteration 173/1072, Loss: 7.8753\n",
      "Iteration 174/1072, Loss: 7.8885\n",
      "Iteration 175/1072, Loss: 7.9076\n",
      "Iteration 176/1072, Loss: 7.9435\n",
      "Iteration 177/1072, Loss: 7.8954\n",
      "Iteration 178/1072, Loss: 7.8977\n",
      "Iteration 179/1072, Loss: 7.8366\n",
      "Iteration 180/1072, Loss: 7.9029\n",
      "Iteration 181/1072, Loss: 7.8789\n",
      "Iteration 182/1072, Loss: 7.8061\n",
      "Iteration 183/1072, Loss: 7.8490\n",
      "Iteration 184/1072, Loss: 7.9554\n",
      "Iteration 185/1072, Loss: 7.9366\n",
      "Iteration 186/1072, Loss: 7.9604\n",
      "Iteration 187/1072, Loss: 7.9011\n",
      "Iteration 188/1072, Loss: 7.8512\n",
      "Iteration 189/1072, Loss: 7.8945\n",
      "Iteration 190/1072, Loss: 7.9392\n",
      "Iteration 191/1072, Loss: 7.8717\n",
      "Iteration 192/1072, Loss: 7.9031\n",
      "Iteration 193/1072, Loss: 7.8598\n",
      "Iteration 194/1072, Loss: 7.8969\n",
      "Iteration 195/1072, Loss: 7.8918\n",
      "Iteration 196/1072, Loss: 7.9388\n",
      "Iteration 197/1072, Loss: 7.8430\n",
      "Iteration 198/1072, Loss: 7.9628\n",
      "Iteration 199/1072, Loss: 7.8654\n",
      "Iteration 200/1072, Loss: 7.9379\n",
      "Iteration 201/1072, Loss: 7.9016\n",
      "Iteration 202/1072, Loss: 7.8978\n",
      "Iteration 203/1072, Loss: 7.9127\n",
      "Iteration 204/1072, Loss: 7.8299\n",
      "Iteration 205/1072, Loss: 7.9269\n",
      "Iteration 206/1072, Loss: 7.8446\n",
      "Iteration 207/1072, Loss: 7.8809\n",
      "Iteration 208/1072, Loss: 7.9321\n",
      "Iteration 209/1072, Loss: 7.9155\n",
      "Iteration 210/1072, Loss: 7.9153\n",
      "Iteration 211/1072, Loss: 7.9136\n",
      "Iteration 212/1072, Loss: 7.9131\n",
      "Iteration 213/1072, Loss: 7.9405\n",
      "Iteration 214/1072, Loss: 7.9262\n",
      "Iteration 215/1072, Loss: 7.8759\n",
      "Iteration 216/1072, Loss: 7.9191\n",
      "Iteration 217/1072, Loss: 7.9254\n",
      "Iteration 218/1072, Loss: 7.8873\n",
      "Iteration 219/1072, Loss: 7.9214\n",
      "Iteration 220/1072, Loss: 7.9255\n",
      "Iteration 221/1072, Loss: 7.8998\n",
      "Iteration 222/1072, Loss: 7.9190\n",
      "Iteration 223/1072, Loss: 7.9427\n",
      "Iteration 224/1072, Loss: 7.8339\n",
      "Iteration 225/1072, Loss: 7.8775\n",
      "Iteration 226/1072, Loss: 7.9262\n",
      "Iteration 227/1072, Loss: 7.8787\n",
      "Iteration 228/1072, Loss: 7.8535\n",
      "Iteration 229/1072, Loss: 7.8769\n",
      "Iteration 230/1072, Loss: 7.9325\n",
      "Iteration 231/1072, Loss: 7.8444\n",
      "Iteration 232/1072, Loss: 7.9627\n",
      "Iteration 233/1072, Loss: 7.9154\n",
      "Iteration 234/1072, Loss: 7.8358\n",
      "Iteration 235/1072, Loss: 7.8988\n",
      "Iteration 236/1072, Loss: 7.8429\n",
      "Iteration 237/1072, Loss: 7.8976\n",
      "Iteration 238/1072, Loss: 7.9039\n",
      "Iteration 239/1072, Loss: 7.8988\n",
      "Iteration 240/1072, Loss: 7.8672\n",
      "Iteration 241/1072, Loss: 7.9040\n",
      "Iteration 242/1072, Loss: 7.8820\n",
      "Iteration 243/1072, Loss: 7.8998\n",
      "Iteration 244/1072, Loss: 7.9357\n",
      "Iteration 245/1072, Loss: 7.8856\n",
      "Iteration 246/1072, Loss: 7.8781\n",
      "Iteration 247/1072, Loss: 7.9300\n",
      "Iteration 248/1072, Loss: 7.9137\n",
      "Iteration 249/1072, Loss: 7.8121\n",
      "Iteration 250/1072, Loss: 7.8784\n",
      "Iteration 251/1072, Loss: 7.9060\n",
      "Iteration 252/1072, Loss: 7.8903\n",
      "Iteration 253/1072, Loss: 7.8883\n",
      "Iteration 254/1072, Loss: 7.9394\n",
      "Iteration 255/1072, Loss: 7.8979\n",
      "Iteration 256/1072, Loss: 7.9714\n",
      "Iteration 257/1072, Loss: 7.8657\n",
      "Iteration 258/1072, Loss: 7.8866\n",
      "Iteration 259/1072, Loss: 7.9367\n",
      "Iteration 260/1072, Loss: 7.9127\n",
      "Iteration 261/1072, Loss: 7.8885\n",
      "Iteration 262/1072, Loss: 7.8423\n",
      "Iteration 263/1072, Loss: 7.8774\n",
      "Iteration 264/1072, Loss: 7.9113\n",
      "Iteration 265/1072, Loss: 7.8915\n",
      "Iteration 266/1072, Loss: 7.9000\n",
      "Iteration 267/1072, Loss: 7.8534\n",
      "Iteration 268/1072, Loss: 7.9014\n",
      "Iteration 269/1072, Loss: 7.9268\n",
      "Iteration 270/1072, Loss: 7.8702\n",
      "Iteration 271/1072, Loss: 7.9177\n",
      "Iteration 272/1072, Loss: 7.8792\n",
      "Iteration 273/1072, Loss: 7.9160\n",
      "Iteration 274/1072, Loss: 7.9548\n",
      "Iteration 275/1072, Loss: 7.9212\n",
      "Iteration 276/1072, Loss: 7.8738\n",
      "Iteration 277/1072, Loss: 7.9144\n",
      "Iteration 278/1072, Loss: 7.8624\n",
      "Iteration 279/1072, Loss: 7.8671\n",
      "Iteration 280/1072, Loss: 7.8190\n",
      "Iteration 281/1072, Loss: 7.8929\n",
      "Iteration 282/1072, Loss: 7.8798\n",
      "Iteration 283/1072, Loss: 7.9276\n",
      "Iteration 284/1072, Loss: 7.8736\n",
      "Iteration 285/1072, Loss: 7.8640\n",
      "Iteration 286/1072, Loss: 7.8949\n",
      "Iteration 287/1072, Loss: 7.8974\n",
      "Iteration 288/1072, Loss: 7.9108\n",
      "Iteration 289/1072, Loss: 7.9352\n",
      "Iteration 291/1072, Loss: 7.9029\n",
      "Iteration 337/1072, Loss: 7.8832\n",
      "Iteration 338/1072, Loss: 7.9246\n",
      "Iteration 339/1072, Loss: 7.9177\n",
      "Iteration 340/1072, Loss: 7.9298\n",
      "Iteration 341/1072, Loss: 7.8902\n",
      "Iteration 342/1072, Loss: 7.8867\n",
      "Iteration 343/1072, Loss: 7.7946\n",
      "Iteration 344/1072, Loss: 7.9228\n",
      "Iteration 345/1072, Loss: 7.8964\n",
      "Iteration 346/1072, Loss: 7.8526\n",
      "Iteration 347/1072, Loss: 7.9164\n",
      "Iteration 348/1072, Loss: 7.9546\n",
      "Iteration 349/1072, Loss: 7.9053\n",
      "Iteration 350/1072, Loss: 7.8913\n",
      "Iteration 351/1072, Loss: 7.8962\n",
      "Iteration 352/1072, Loss: 7.7963\n",
      "Iteration 353/1072, Loss: 7.8706\n",
      "Iteration 354/1072, Loss: 7.9859\n",
      "Iteration 355/1072, Loss: 7.8955\n",
      "Iteration 356/1072, Loss: 7.8743\n",
      "Iteration 357/1072, Loss: 7.8775\n",
      "Iteration 358/1072, Loss: 7.8533\n",
      "Iteration 359/1072, Loss: 7.8690\n",
      "Iteration 360/1072, Loss: 7.8727\n",
      "Iteration 361/1072, Loss: 7.9534\n",
      "Iteration 362/1072, Loss: 7.9241\n",
      "Iteration 363/1072, Loss: 7.8659\n",
      "Iteration 364/1072, Loss: 7.8507\n",
      "Iteration 365/1072, Loss: 7.8201\n",
      "Iteration 366/1072, Loss: 7.9167\n",
      "Iteration 367/1072, Loss: 7.8409\n",
      "Iteration 368/1072, Loss: 7.9785\n",
      "Iteration 369/1072, Loss: 7.8468\n",
      "Iteration 370/1072, Loss: 7.9127\n",
      "Iteration 371/1072, Loss: 7.9433\n",
      "Iteration 372/1072, Loss: 7.8647\n",
      "Iteration 373/1072, Loss: 7.9023\n",
      "Iteration 374/1072, Loss: 7.8846\n",
      "Iteration 375/1072, Loss: 7.9740\n",
      "Iteration 376/1072, Loss: 7.9328\n",
      "Iteration 377/1072, Loss: 7.8719\n",
      "Iteration 378/1072, Loss: 7.8541\n",
      "Iteration 379/1072, Loss: 7.9300\n",
      "Iteration 380/1072, Loss: 7.9078\n",
      "Iteration 381/1072, Loss: 7.9288\n",
      "Iteration 382/1072, Loss: 7.8902\n",
      "Iteration 383/1072, Loss: 7.8930\n",
      "Iteration 384/1072, Loss: 7.8445\n",
      "Iteration 385/1072, Loss: 7.8533\n",
      "Iteration 386/1072, Loss: 7.8901\n",
      "Iteration 387/1072, Loss: 7.9470\n",
      "Iteration 388/1072, Loss: 7.8608\n",
      "Iteration 389/1072, Loss: 7.8554\n",
      "Iteration 390/1072, Loss: 7.8848\n",
      "Iteration 391/1072, Loss: 7.8366\n",
      "Iteration 392/1072, Loss: 7.8924\n",
      "Iteration 393/1072, Loss: 7.8438\n",
      "Iteration 394/1072, Loss: 7.8426\n",
      "Iteration 395/1072, Loss: 7.8836\n",
      "Iteration 396/1072, Loss: 7.8808\n",
      "Iteration 397/1072, Loss: 7.8753\n",
      "Iteration 398/1072, Loss: 7.9122\n",
      "Iteration 399/1072, Loss: 7.8983\n",
      "Iteration 400/1072, Loss: 7.9464\n",
      "Iteration 401/1072, Loss: 7.9024\n",
      "Iteration 402/1072, Loss: 7.9046\n",
      "Iteration 403/1072, Loss: 7.9125\n",
      "Iteration 404/1072, Loss: 7.9142\n",
      "Iteration 405/1072, Loss: 7.9486\n",
      "Iteration 406/1072, Loss: 7.9027\n",
      "Iteration 407/1072, Loss: 7.8598\n",
      "Iteration 408/1072, Loss: 7.9269\n",
      "Iteration 409/1072, Loss: 7.8576\n",
      "Iteration 410/1072, Loss: 7.9221\n",
      "Iteration 411/1072, Loss: 7.9070\n",
      "Iteration 412/1072, Loss: 7.8556\n",
      "Iteration 413/1072, Loss: 7.9620\n",
      "Iteration 414/1072, Loss: 7.8854\n",
      "Iteration 415/1072, Loss: 7.8494\n",
      "Iteration 416/1072, Loss: 7.9119\n",
      "Iteration 417/1072, Loss: 7.8856\n",
      "Iteration 418/1072, Loss: 7.8834\n",
      "Iteration 419/1072, Loss: 7.8356\n",
      "Iteration 420/1072, Loss: 7.9227\n",
      "Iteration 421/1072, Loss: 7.8627\n",
      "Iteration 422/1072, Loss: 7.8006\n",
      "Iteration 423/1072, Loss: 7.9133\n",
      "Iteration 424/1072, Loss: 7.8588\n",
      "Iteration 425/1072, Loss: 7.8667\n",
      "Iteration 426/1072, Loss: 7.8121\n",
      "Iteration 427/1072, Loss: 7.8664\n",
      "Iteration 428/1072, Loss: 7.8489\n",
      "Iteration 429/1072, Loss: 7.8750\n",
      "Iteration 430/1072, Loss: 7.8646\n",
      "Iteration 431/1072, Loss: 7.9328\n",
      "Iteration 432/1072, Loss: 7.8645\n",
      "Iteration 433/1072, Loss: 7.8844\n",
      "Iteration 434/1072, Loss: 7.9063\n",
      "Iteration 435/1072, Loss: 7.9656\n",
      "Iteration 436/1072, Loss: 7.8797\n",
      "Iteration 437/1072, Loss: 7.8864\n",
      "Iteration 438/1072, Loss: 7.8689\n",
      "Iteration 439/1072, Loss: 7.9259\n",
      "Iteration 440/1072, Loss: 7.8772\n",
      "Iteration 441/1072, Loss: 7.9193\n",
      "Iteration 442/1072, Loss: 7.8605\n",
      "Iteration 443/1072, Loss: 7.9618\n",
      "Iteration 444/1072, Loss: 7.8814\n",
      "Iteration 445/1072, Loss: 7.8891\n",
      "Iteration 446/1072, Loss: 7.8068\n",
      "Iteration 447/1072, Loss: 7.8667\n",
      "Iteration 448/1072, Loss: 7.8681\n",
      "Iteration 449/1072, Loss: 7.8844\n",
      "Iteration 450/1072, Loss: 7.8789\n",
      "Iteration 451/1072, Loss: 7.9098\n",
      "Iteration 452/1072, Loss: 7.9236\n",
      "Iteration 453/1072, Loss: 7.9379\n",
      "Iteration 454/1072, Loss: 7.8538\n",
      "Iteration 455/1072, Loss: 7.8772\n",
      "Iteration 456/1072, Loss: 7.9362\n",
      "Iteration 457/1072, Loss: 7.8137\n",
      "Iteration 458/1072, Loss: 7.8821\n",
      "Iteration 459/1072, Loss: 7.9383\n",
      "Iteration 460/1072, Loss: 7.8841\n",
      "Iteration 461/1072, Loss: 7.8884\n",
      "Iteration 462/1072, Loss: 7.9036\n",
      "Iteration 463/1072, Loss: 7.8714\n",
      "Iteration 464/1072, Loss: 7.9320\n",
      "Iteration 465/1072, Loss: 7.9463\n",
      "Iteration 466/1072, Loss: 7.9040\n",
      "Iteration 467/1072, Loss: 7.9452\n",
      "Iteration 468/1072, Loss: 7.8699\n",
      "Iteration 469/1072, Loss: 7.8791\n",
      "Iteration 470/1072, Loss: 7.9206\n",
      "Iteration 471/1072, Loss: 7.8368\n",
      "Iteration 472/1072, Loss: 7.8191\n",
      "Iteration 473/1072, Loss: 7.8995\n",
      "Iteration 474/1072, Loss: 7.9151\n",
      "Iteration 475/1072, Loss: 7.9459\n",
      "Iteration 476/1072, Loss: 7.8799\n",
      "Iteration 477/1072, Loss: 7.8669\n",
      "Iteration 478/1072, Loss: 7.9153\n",
      "Iteration 479/1072, Loss: 7.9165\n",
      "Iteration 480/1072, Loss: 7.8138\n",
      "Iteration 481/1072, Loss: 7.8306\n",
      "Iteration 482/1072, Loss: 7.9290\n",
      "Iteration 483/1072, Loss: 7.8967\n",
      "Iteration 484/1072, Loss: 7.9053\n",
      "Iteration 485/1072, Loss: 7.9065\n",
      "Iteration 486/1072, Loss: 7.9164\n",
      "Iteration 487/1072, Loss: 7.9385\n",
      "Iteration 488/1072, Loss: 7.8404\n",
      "Iteration 489/1072, Loss: 7.9217\n",
      "Iteration 490/1072, Loss: 7.8967\n",
      "Iteration 491/1072, Loss: 7.9289\n",
      "Iteration 492/1072, Loss: 7.9161\n",
      "Iteration 493/1072, Loss: 7.8693\n",
      "Iteration 494/1072, Loss: 7.8891\n",
      "Iteration 495/1072, Loss: 7.9233\n",
      "Iteration 496/1072, Loss: 7.8116\n",
      "Iteration 497/1072, Loss: 7.8698\n",
      "Iteration 498/1072, Loss: 7.8591\n",
      "Iteration 499/1072, Loss: 7.8961\n",
      "Iteration 500/1072, Loss: 7.9145\n",
      "Iteration 501/1072, Loss: 7.9527\n",
      "Iteration 502/1072, Loss: 7.8858\n",
      "Iteration 503/1072, Loss: 7.9016\n",
      "Iteration 504/1072, Loss: 7.8912\n",
      "Iteration 505/1072, Loss: 7.8594\n",
      "Iteration 506/1072, Loss: 7.9326\n",
      "Iteration 507/1072, Loss: 7.8913\n",
      "Iteration 508/1072, Loss: 7.9612\n",
      "Iteration 509/1072, Loss: 7.9592\n",
      "Iteration 510/1072, Loss: 7.9091\n",
      "Iteration 511/1072, Loss: 7.9670\n",
      "Iteration 512/1072, Loss: 7.8615\n",
      "Iteration 513/1072, Loss: 7.8869\n",
      "Iteration 514/1072, Loss: 7.8630\n",
      "Iteration 515/1072, Loss: 7.8786\n",
      "Iteration 516/1072, Loss: 7.8905\n",
      "Iteration 517/1072, Loss: 7.8775\n",
      "Iteration 518/1072, Loss: 7.7973\n",
      "Iteration 519/1072, Loss: 7.8823\n",
      "Iteration 520/1072, Loss: 7.8570\n",
      "Iteration 521/1072, Loss: 7.8738\n",
      "Iteration 522/1072, Loss: 7.9286\n",
      "Iteration 523/1072, Loss: 7.8292\n",
      "Iteration 524/1072, Loss: 7.9310\n",
      "Iteration 525/1072, Loss: 7.9982\n",
      "Iteration 526/1072, Loss: 7.8411\n",
      "Iteration 527/1072, Loss: 7.9732\n",
      "Iteration 528/1072, Loss: 7.9376\n",
      "Iteration 529/1072, Loss: 7.8840\n",
      "Iteration 530/1072, Loss: 7.8850\n",
      "Iteration 531/1072, Loss: 7.8950\n",
      "Iteration 532/1072, Loss: 7.8185\n",
      "Iteration 533/1072, Loss: 7.9599\n",
      "Iteration 534/1072, Loss: 7.8473\n",
      "Iteration 535/1072, Loss: 7.9113\n",
      "Iteration 536/1072, Loss: 7.8844\n",
      "Iteration 537/1072, Loss: 7.8875\n",
      "Iteration 538/1072, Loss: 7.9133\n",
      "Iteration 539/1072, Loss: 7.8935\n",
      "Iteration 540/1072, Loss: 7.8842\n",
      "Iteration 541/1072, Loss: 7.9001\n",
      "Iteration 542/1072, Loss: 7.8924\n",
      "Iteration 543/1072, Loss: 7.8847\n",
      "Iteration 544/1072, Loss: 7.8791\n",
      "Iteration 545/1072, Loss: 7.8955\n",
      "Iteration 546/1072, Loss: 7.9035\n",
      "Iteration 547/1072, Loss: 7.9094\n",
      "Iteration 548/1072, Loss: 7.9196\n",
      "Iteration 549/1072, Loss: 7.8056\n",
      "Iteration 550/1072, Loss: 7.9116\n",
      "Iteration 551/1072, Loss: 7.8969\n",
      "Iteration 552/1072, Loss: 7.8514\n",
      "Iteration 553/1072, Loss: 7.8636\n",
      "Iteration 554/1072, Loss: 7.8554\n",
      "Iteration 555/1072, Loss: 7.9047\n",
      "Iteration 556/1072, Loss: 7.9251\n",
      "Iteration 557/1072, Loss: 7.9264\n",
      "Iteration 558/1072, Loss: 7.9154\n",
      "Iteration 559/1072, Loss: 7.9079\n",
      "Iteration 560/1072, Loss: 7.9769\n",
      "Iteration 561/1072, Loss: 7.8597\n",
      "Iteration 562/1072, Loss: 7.8704\n",
      "Iteration 563/1072, Loss: 7.9149\n",
      "Iteration 564/1072, Loss: 7.8729\n",
      "Iteration 565/1072, Loss: 7.9040\n",
      "Iteration 566/1072, Loss: 7.8797\n",
      "Iteration 567/1072, Loss: 7.8763\n",
      "Iteration 568/1072, Loss: 7.8661\n",
      "Iteration 569/1072, Loss: 7.8733\n",
      "Iteration 570/1072, Loss: 7.8734\n",
      "Iteration 571/1072, Loss: 7.8987\n",
      "Iteration 572/1072, Loss: 7.8303\n",
      "Iteration 573/1072, Loss: 7.8507\n",
      "Iteration 574/1072, Loss: 7.8758\n",
      "Iteration 575/1072, Loss: 7.8794\n",
      "Iteration 576/1072, Loss: 7.9474\n",
      "Iteration 577/1072, Loss: 7.8657\n",
      "Iteration 578/1072, Loss: 7.8795\n",
      "Iteration 579/1072, Loss: 7.8816\n",
      "Iteration 580/1072, Loss: 7.8853\n",
      "Iteration 581/1072, Loss: 7.9014\n",
      "Iteration 582/1072, Loss: 7.8716\n",
      "Iteration 583/1072, Loss: 7.8570\n",
      "Iteration 584/1072, Loss: 7.9057\n",
      "Iteration 585/1072, Loss: 7.8793\n",
      "Iteration 586/1072, Loss: 7.9157\n",
      "Iteration 587/1072, Loss: 7.8609\n",
      "Iteration 588/1072, Loss: 7.9369\n",
      "Iteration 589/1072, Loss: 7.9499\n",
      "Iteration 590/1072, Loss: 7.8558\n",
      "Iteration 591/1072, Loss: 7.9348\n",
      "Iteration 592/1072, Loss: 7.8947\n",
      "Iteration 593/1072, Loss: 7.8898\n",
      "Iteration 594/1072, Loss: 7.8819\n",
      "Iteration 595/1072, Loss: 7.8737\n",
      "Iteration 596/1072, Loss: 7.8639\n",
      "Iteration 597/1072, Loss: 7.9501\n",
      "Iteration 598/1072, Loss: 7.9003\n",
      "Iteration 599/1072, Loss: 7.8356\n",
      "Iteration 600/1072, Loss: 7.9159\n",
      "Iteration 601/1072, Loss: 7.8401\n",
      "Iteration 602/1072, Loss: 7.8730\n",
      "Iteration 603/1072, Loss: 7.8635\n",
      "Iteration 604/1072, Loss: 7.9145\n",
      "Iteration 605/1072, Loss: 7.8957\n",
      "Iteration 606/1072, Loss: 7.9126\n",
      "Iteration 607/1072, Loss: 7.8943\n",
      "Iteration 608/1072, Loss: 7.8958\n",
      "Iteration 609/1072, Loss: 7.8882\n",
      "Iteration 610/1072, Loss: 7.8948\n",
      "Iteration 611/1072, Loss: 7.9289\n",
      "Iteration 612/1072, Loss: 7.8167\n",
      "Iteration 613/1072, Loss: 7.8492\n",
      "Iteration 614/1072, Loss: 7.9035\n",
      "Iteration 615/1072, Loss: 7.9304\n",
      "Iteration 616/1072, Loss: 7.9276\n",
      "Iteration 617/1072, Loss: 7.8949\n",
      "Iteration 618/1072, Loss: 7.8858\n",
      "Iteration 619/1072, Loss: 7.8945\n",
      "Iteration 620/1072, Loss: 7.8961\n",
      "Iteration 621/1072, Loss: 7.9270\n",
      "Iteration 622/1072, Loss: 7.8378\n",
      "Iteration 623/1072, Loss: 7.9721\n",
      "Iteration 624/1072, Loss: 7.8260\n",
      "Iteration 625/1072, Loss: 7.8973\n",
      "Iteration 626/1072, Loss: 7.9596\n",
      "Iteration 627/1072, Loss: 7.9527\n",
      "Iteration 628/1072, Loss: 7.8612\n",
      "Iteration 629/1072, Loss: 7.8971\n",
      "Iteration 630/1072, Loss: 7.8163\n",
      "Iteration 631/1072, Loss: 7.8727\n",
      "Iteration 632/1072, Loss: 7.9437\n",
      "Iteration 633/1072, Loss: 7.8349\n",
      "Iteration 634/1072, Loss: 7.8982\n",
      "Iteration 635/1072, Loss: 7.8845\n",
      "Iteration 636/1072, Loss: 7.8591\n",
      "Iteration 637/1072, Loss: 7.8563\n",
      "Iteration 638/1072, Loss: 7.8699\n",
      "Iteration 639/1072, Loss: 7.8967\n",
      "Iteration 640/1072, Loss: 7.9261\n",
      "Iteration 641/1072, Loss: 7.8908\n",
      "Iteration 642/1072, Loss: 7.9185\n",
      "Iteration 643/1072, Loss: 7.9226\n",
      "Iteration 644/1072, Loss: 7.9263\n",
      "Iteration 645/1072, Loss: 7.8896\n",
      "Iteration 646/1072, Loss: 7.9570\n",
      "Iteration 647/1072, Loss: 7.8797\n",
      "Iteration 648/1072, Loss: 7.9462\n",
      "Iteration 649/1072, Loss: 7.8607\n",
      "Iteration 650/1072, Loss: 7.9377\n",
      "Iteration 651/1072, Loss: 7.9067\n",
      "Iteration 652/1072, Loss: 7.8817\n",
      "Iteration 653/1072, Loss: 7.9296\n",
      "Iteration 654/1072, Loss: 7.7878\n",
      "Iteration 655/1072, Loss: 7.9411\n",
      "Iteration 656/1072, Loss: 7.8812\n",
      "Iteration 657/1072, Loss: 7.8737\n",
      "Iteration 658/1072, Loss: 7.8504\n",
      "Iteration 659/1072, Loss: 7.8080\n",
      "Iteration 660/1072, Loss: 7.8981\n",
      "Iteration 661/1072, Loss: 7.8639\n",
      "Iteration 662/1072, Loss: 7.8748\n",
      "Iteration 663/1072, Loss: 7.8408\n",
      "Iteration 664/1072, Loss: 7.8905\n",
      "Iteration 665/1072, Loss: 7.8963\n",
      "Iteration 666/1072, Loss: 7.8448\n",
      "Iteration 667/1072, Loss: 7.9322\n",
      "Iteration 668/1072, Loss: 7.8785\n",
      "Iteration 669/1072, Loss: 7.8957\n",
      "Iteration 670/1072, Loss: 7.8883\n",
      "Iteration 671/1072, Loss: 7.9092\n",
      "Iteration 672/1072, Loss: 7.8920\n",
      "Iteration 673/1072, Loss: 7.9173\n",
      "Iteration 674/1072, Loss: 7.8481\n",
      "Iteration 675/1072, Loss: 7.9152\n",
      "Iteration 677/1072, Loss: 7.9555\n",
      "Iteration 678/1072, Loss: 7.9146\n",
      "Iteration 679/1072, Loss: 7.8833\n",
      "Iteration 680/1072, Loss: 7.8556\n",
      "Iteration 681/1072, Loss: 7.9001\n",
      "Iteration 682/1072, Loss: 7.9061\n",
      "Iteration 683/1072, Loss: 7.8541\n",
      "Iteration 684/1072, Loss: 7.8966\n",
      "Iteration 685/1072, Loss: 7.8895\n",
      "Iteration 686/1072, Loss: 7.8789\n",
      "Iteration 687/1072, Loss: 7.8487\n",
      "Iteration 688/1072, Loss: 7.8572\n",
      "Iteration 689/1072, Loss: 7.8593\n",
      "Iteration 690/1072, Loss: 7.8677\n",
      "Iteration 691/1072, Loss: 7.8577\n",
      "Iteration 692/1072, Loss: 7.9879\n",
      "Iteration 693/1072, Loss: 7.8324\n",
      "Iteration 694/1072, Loss: 7.8830\n",
      "Iteration 695/1072, Loss: 7.8688\n",
      "Iteration 696/1072, Loss: 7.8196\n",
      "Iteration 697/1072, Loss: 7.8485\n",
      "Iteration 698/1072, Loss: 7.9337\n",
      "Iteration 699/1072, Loss: 7.8507\n",
      "Iteration 700/1072, Loss: 7.8057\n",
      "Iteration 701/1072, Loss: 7.9020\n",
      "Iteration 702/1072, Loss: 7.8757\n",
      "Iteration 703/1072, Loss: 7.9131\n",
      "Iteration 704/1072, Loss: 7.8566\n",
      "Iteration 705/1072, Loss: 7.8816\n",
      "Iteration 706/1072, Loss: 7.9268\n",
      "Iteration 707/1072, Loss: 7.8611\n",
      "Iteration 708/1072, Loss: 7.9225\n",
      "Iteration 709/1072, Loss: 7.8528\n",
      "Iteration 710/1072, Loss: 7.9662\n",
      "Iteration 711/1072, Loss: 7.8914\n",
      "Iteration 712/1072, Loss: 7.9059\n",
      "Iteration 713/1072, Loss: 7.9455\n",
      "Iteration 714/1072, Loss: 7.8398\n",
      "Iteration 715/1072, Loss: 7.8665\n",
      "Iteration 716/1072, Loss: 7.8907\n",
      "Iteration 717/1072, Loss: 7.8376\n",
      "Iteration 718/1072, Loss: 7.9718\n",
      "Iteration 719/1072, Loss: 7.8498\n",
      "Iteration 720/1072, Loss: 7.8821\n",
      "Iteration 721/1072, Loss: 7.8687\n",
      "Iteration 722/1072, Loss: 7.8016\n",
      "Iteration 723/1072, Loss: 7.8745\n",
      "Iteration 724/1072, Loss: 7.8539\n",
      "Iteration 725/1072, Loss: 7.9340\n",
      "Iteration 726/1072, Loss: 7.9123\n",
      "Iteration 727/1072, Loss: 7.9133\n",
      "Iteration 728/1072, Loss: 7.8907\n",
      "Iteration 729/1072, Loss: 7.8330\n",
      "Iteration 730/1072, Loss: 7.8535\n",
      "Iteration 731/1072, Loss: 7.8654\n",
      "Iteration 732/1072, Loss: 7.8572\n",
      "Iteration 733/1072, Loss: 7.9193\n",
      "Iteration 734/1072, Loss: 7.9095\n",
      "Iteration 735/1072, Loss: 7.8074\n",
      "Iteration 736/1072, Loss: 7.8510\n",
      "Iteration 737/1072, Loss: 7.8800\n",
      "Iteration 738/1072, Loss: 7.8834\n",
      "Iteration 739/1072, Loss: 7.8779\n",
      "Iteration 740/1072, Loss: 7.7949\n",
      "Iteration 741/1072, Loss: 7.8034\n",
      "Iteration 742/1072, Loss: 7.9242\n",
      "Iteration 743/1072, Loss: 7.8826\n",
      "Iteration 744/1072, Loss: 7.8886\n",
      "Iteration 745/1072, Loss: 7.8582\n",
      "Iteration 746/1072, Loss: 7.8839\n",
      "Iteration 747/1072, Loss: 7.8750\n",
      "Iteration 748/1072, Loss: 7.8727\n",
      "Iteration 749/1072, Loss: 7.9341\n",
      "Iteration 750/1072, Loss: 7.9042\n",
      "Iteration 751/1072, Loss: 7.8994\n",
      "Iteration 752/1072, Loss: 7.8942\n",
      "Iteration 753/1072, Loss: 7.8550\n",
      "Iteration 754/1072, Loss: 7.8084\n",
      "Iteration 755/1072, Loss: 7.8326\n",
      "Iteration 756/1072, Loss: 7.9568\n",
      "Iteration 757/1072, Loss: 7.9009\n",
      "Iteration 758/1072, Loss: 7.9260\n",
      "Iteration 759/1072, Loss: 7.8519\n",
      "Iteration 760/1072, Loss: 7.9003\n",
      "Iteration 761/1072, Loss: 7.8320\n",
      "Iteration 762/1072, Loss: 7.8793\n",
      "Iteration 763/1072, Loss: 7.8775\n",
      "Iteration 764/1072, Loss: 7.9013\n",
      "Iteration 765/1072, Loss: 7.9091\n",
      "Iteration 766/1072, Loss: 7.9442\n",
      "Iteration 767/1072, Loss: 7.8807\n",
      "Iteration 768/1072, Loss: 7.8408\n",
      "Iteration 769/1072, Loss: 7.9109\n",
      "Iteration 770/1072, Loss: 7.8694\n",
      "Iteration 771/1072, Loss: 7.8808\n",
      "Iteration 772/1072, Loss: 7.8690\n",
      "Iteration 773/1072, Loss: 7.8605\n",
      "Iteration 774/1072, Loss: 7.8675\n",
      "Iteration 775/1072, Loss: 7.8428\n",
      "Iteration 776/1072, Loss: 7.8479\n",
      "Iteration 777/1072, Loss: 7.8993\n",
      "Iteration 778/1072, Loss: 7.8487\n",
      "Iteration 779/1072, Loss: 7.8813\n",
      "Iteration 780/1072, Loss: 7.8250\n",
      "Iteration 781/1072, Loss: 7.9371\n",
      "Iteration 782/1072, Loss: 7.8695\n",
      "Iteration 783/1072, Loss: 7.8652\n",
      "Iteration 784/1072, Loss: 7.9460\n",
      "Iteration 785/1072, Loss: 7.8859\n",
      "Iteration 786/1072, Loss: 7.8855\n",
      "Iteration 787/1072, Loss: 7.8354\n",
      "Iteration 788/1072, Loss: 7.8403\n",
      "Iteration 789/1072, Loss: 7.8530\n",
      "Iteration 790/1072, Loss: 7.9036\n",
      "Iteration 791/1072, Loss: 7.8707\n",
      "Iteration 792/1072, Loss: 7.8533\n",
      "Iteration 793/1072, Loss: 7.8084\n",
      "Iteration 794/1072, Loss: 7.8819\n",
      "Iteration 795/1072, Loss: 7.8358\n",
      "Iteration 796/1072, Loss: 7.9153\n",
      "Iteration 797/1072, Loss: 7.8135\n",
      "Iteration 798/1072, Loss: 7.9394\n",
      "Iteration 799/1072, Loss: 7.8710\n",
      "Iteration 800/1072, Loss: 7.8605\n",
      "Iteration 801/1072, Loss: 7.8646\n",
      "Iteration 802/1072, Loss: 7.8886\n",
      "Iteration 803/1072, Loss: 7.8535\n",
      "Iteration 804/1072, Loss: 7.8032\n",
      "Iteration 805/1072, Loss: 7.8504\n",
      "Iteration 806/1072, Loss: 7.8852\n",
      "Iteration 807/1072, Loss: 7.9325\n",
      "Iteration 808/1072, Loss: 7.8903\n",
      "Iteration 809/1072, Loss: 7.9195\n",
      "Iteration 810/1072, Loss: 7.9378\n",
      "Iteration 811/1072, Loss: 7.8924\n",
      "Iteration 812/1072, Loss: 7.8377\n",
      "Iteration 813/1072, Loss: 7.8948\n",
      "Iteration 814/1072, Loss: 7.9092\n",
      "Iteration 815/1072, Loss: 7.8511\n",
      "Iteration 816/1072, Loss: 7.8519\n",
      "Iteration 817/1072, Loss: 7.8191\n",
      "Iteration 818/1072, Loss: 7.8715\n",
      "Iteration 819/1072, Loss: 7.8018\n",
      "Iteration 820/1072, Loss: 7.8738\n",
      "Iteration 821/1072, Loss: 7.9357\n",
      "Iteration 822/1072, Loss: 7.9161\n",
      "Iteration 823/1072, Loss: 7.8773\n",
      "Iteration 824/1072, Loss: 7.8418\n",
      "Iteration 825/1072, Loss: 7.9310\n",
      "Iteration 826/1072, Loss: 7.8706\n",
      "Iteration 827/1072, Loss: 7.9277\n",
      "Iteration 828/1072, Loss: 7.9255\n",
      "Iteration 829/1072, Loss: 7.8950\n",
      "Iteration 830/1072, Loss: 7.8601\n",
      "Iteration 831/1072, Loss: 7.8791\n",
      "Iteration 832/1072, Loss: 7.9097\n",
      "Iteration 833/1072, Loss: 7.9507\n",
      "Iteration 834/1072, Loss: 7.9216\n",
      "Iteration 835/1072, Loss: 7.8864\n",
      "Iteration 836/1072, Loss: 7.9477\n",
      "Iteration 837/1072, Loss: 7.8933\n",
      "Iteration 838/1072, Loss: 7.8897\n",
      "Iteration 839/1072, Loss: 7.8297\n",
      "Iteration 840/1072, Loss: 7.9074\n",
      "Iteration 841/1072, Loss: 7.8664\n",
      "Iteration 842/1072, Loss: 7.9265\n",
      "Iteration 843/1072, Loss: 7.9312\n",
      "Iteration 844/1072, Loss: 7.8997\n",
      "Iteration 845/1072, Loss: 7.9065\n",
      "Iteration 846/1072, Loss: 7.8825\n",
      "Iteration 847/1072, Loss: 7.8835\n",
      "Iteration 848/1072, Loss: 7.9677\n",
      "Iteration 849/1072, Loss: 7.8602\n",
      "Iteration 850/1072, Loss: 7.8756\n",
      "Iteration 851/1072, Loss: 7.8970\n",
      "Iteration 852/1072, Loss: 7.9123\n",
      "Iteration 853/1072, Loss: 7.8734\n",
      "Iteration 854/1072, Loss: 7.8898\n",
      "Iteration 855/1072, Loss: 7.8623\n",
      "Iteration 856/1072, Loss: 7.8558\n",
      "Iteration 857/1072, Loss: 7.8857\n",
      "Iteration 858/1072, Loss: 7.8197\n",
      "Iteration 859/1072, Loss: 7.9045\n",
      "Iteration 860/1072, Loss: 7.8750\n",
      "Iteration 861/1072, Loss: 7.9008\n",
      "Iteration 862/1072, Loss: 7.8465\n",
      "Iteration 863/1072, Loss: 7.8768\n",
      "Iteration 864/1072, Loss: 7.7740\n",
      "Iteration 865/1072, Loss: 7.8290\n",
      "Iteration 866/1072, Loss: 7.8494\n",
      "Iteration 867/1072, Loss: 7.8970\n",
      "Iteration 868/1072, Loss: 7.9039\n",
      "Iteration 869/1072, Loss: 7.8702\n",
      "Iteration 870/1072, Loss: 7.9303\n",
      "Iteration 871/1072, Loss: 7.8606\n",
      "Iteration 872/1072, Loss: 7.8618\n",
      "Iteration 873/1072, Loss: 7.8972\n",
      "Iteration 874/1072, Loss: 7.8508\n",
      "Iteration 875/1072, Loss: 7.8582\n",
      "Iteration 876/1072, Loss: 7.9381\n",
      "Iteration 877/1072, Loss: 7.8596\n",
      "Iteration 878/1072, Loss: 7.8877\n",
      "Iteration 879/1072, Loss: 7.8912\n",
      "Iteration 880/1072, Loss: 7.8912\n",
      "Iteration 881/1072, Loss: 7.8731\n",
      "Iteration 882/1072, Loss: 7.8422\n",
      "Iteration 883/1072, Loss: 7.8395\n",
      "Iteration 884/1072, Loss: 7.9014\n",
      "Iteration 885/1072, Loss: 7.9515\n",
      "Iteration 886/1072, Loss: 7.8539\n",
      "Iteration 887/1072, Loss: 7.8255\n",
      "Iteration 888/1072, Loss: 7.9499\n",
      "Iteration 889/1072, Loss: 7.9092\n",
      "Iteration 890/1072, Loss: 7.8992\n",
      "Iteration 891/1072, Loss: 7.8826\n",
      "Iteration 892/1072, Loss: 7.8819\n",
      "Iteration 893/1072, Loss: 7.8821\n",
      "Iteration 894/1072, Loss: 7.8652\n",
      "Iteration 895/1072, Loss: 7.8971\n",
      "Iteration 896/1072, Loss: 7.9107\n",
      "Iteration 897/1072, Loss: 7.8536\n",
      "Iteration 898/1072, Loss: 7.8639\n",
      "Iteration 899/1072, Loss: 7.9115\n",
      "Iteration 900/1072, Loss: 7.8916\n",
      "Iteration 901/1072, Loss: 7.8824\n",
      "Iteration 902/1072, Loss: 7.8279\n",
      "Iteration 903/1072, Loss: 7.9338\n",
      "Iteration 904/1072, Loss: 7.8517\n",
      "Iteration 905/1072, Loss: 7.8930\n",
      "Iteration 906/1072, Loss: 7.8208\n",
      "Iteration 907/1072, Loss: 7.9111\n",
      "Iteration 908/1072, Loss: 7.8816\n",
      "Iteration 909/1072, Loss: 7.8371\n",
      "Iteration 910/1072, Loss: 7.8294\n",
      "Iteration 911/1072, Loss: 7.8716\n",
      "Iteration 912/1072, Loss: 7.9046\n",
      "Iteration 913/1072, Loss: 7.8396\n",
      "Iteration 914/1072, Loss: 7.9418\n",
      "Iteration 915/1072, Loss: 7.8878\n",
      "Iteration 916/1072, Loss: 7.8972\n",
      "Iteration 917/1072, Loss: 7.8810\n",
      "Iteration 918/1072, Loss: 7.8955\n",
      "Iteration 919/1072, Loss: 7.8900\n",
      "Iteration 920/1072, Loss: 7.9185\n",
      "Iteration 921/1072, Loss: 7.8878\n",
      "Iteration 922/1072, Loss: 7.9134\n",
      "Iteration 923/1072, Loss: 7.9430\n",
      "Iteration 924/1072, Loss: 7.8806\n",
      "Iteration 925/1072, Loss: 7.9118\n",
      "Iteration 926/1072, Loss: 7.8663\n",
      "Iteration 927/1072, Loss: 7.8657\n",
      "Iteration 928/1072, Loss: 7.8986\n",
      "Iteration 929/1072, Loss: 7.7870\n",
      "Iteration 930/1072, Loss: 7.9312\n",
      "Iteration 931/1072, Loss: 7.8794\n",
      "Iteration 932/1072, Loss: 7.8369\n",
      "Iteration 933/1072, Loss: 7.8640\n",
      "Iteration 934/1072, Loss: 7.8771\n",
      "Iteration 935/1072, Loss: 7.8270\n",
      "Iteration 936/1072, Loss: 7.8678\n",
      "Iteration 937/1072, Loss: 7.9269\n",
      "Iteration 938/1072, Loss: 7.9181\n",
      "Iteration 939/1072, Loss: 7.8580\n",
      "Iteration 940/1072, Loss: 7.8874\n",
      "Iteration 941/1072, Loss: 7.8629\n",
      "Iteration 942/1072, Loss: 7.9493\n",
      "Iteration 943/1072, Loss: 7.9105\n",
      "Iteration 944/1072, Loss: 7.9224\n",
      "Iteration 945/1072, Loss: 7.8945\n",
      "Iteration 946/1072, Loss: 7.8432\n",
      "Iteration 947/1072, Loss: 7.9060\n",
      "Iteration 948/1072, Loss: 7.8984\n",
      "Iteration 949/1072, Loss: 7.8557\n",
      "Iteration 950/1072, Loss: 7.8475\n",
      "Iteration 951/1072, Loss: 7.9174\n",
      "Iteration 952/1072, Loss: 7.8958\n",
      "Iteration 953/1072, Loss: 7.9014\n",
      "Iteration 954/1072, Loss: 7.9051\n",
      "Iteration 955/1072, Loss: 7.8139\n",
      "Iteration 956/1072, Loss: 7.8670\n",
      "Iteration 957/1072, Loss: 7.8658\n",
      "Iteration 958/1072, Loss: 7.8965\n",
      "Iteration 959/1072, Loss: 7.8493\n",
      "Iteration 960/1072, Loss: 7.8980\n",
      "Iteration 961/1072, Loss: 7.9390\n",
      "Iteration 962/1072, Loss: 7.8588\n",
      "Iteration 963/1072, Loss: 7.9283\n",
      "Iteration 964/1072, Loss: 7.8486\n",
      "Iteration 965/1072, Loss: 7.8727\n",
      "Iteration 966/1072, Loss: 7.8536\n",
      "Iteration 967/1072, Loss: 7.8475\n",
      "Iteration 968/1072, Loss: 7.8855\n",
      "Iteration 969/1072, Loss: 7.8539\n",
      "Iteration 970/1072, Loss: 7.9219\n",
      "Iteration 971/1072, Loss: 7.9338\n",
      "Iteration 972/1072, Loss: 7.8648\n",
      "Iteration 973/1072, Loss: 7.9014\n",
      "Iteration 974/1072, Loss: 7.8404\n",
      "Iteration 975/1072, Loss: 7.8631\n",
      "Iteration 976/1072, Loss: 7.8591\n",
      "Iteration 977/1072, Loss: 7.8679\n",
      "Iteration 978/1072, Loss: 7.8481\n",
      "Iteration 979/1072, Loss: 7.8782\n",
      "Iteration 980/1072, Loss: 7.9231\n",
      "Iteration 981/1072, Loss: 7.7996\n",
      "Iteration 982/1072, Loss: 7.8392\n",
      "Iteration 983/1072, Loss: 7.8794\n",
      "Iteration 984/1072, Loss: 7.8593\n",
      "Iteration 985/1072, Loss: 7.9117\n",
      "Iteration 986/1072, Loss: 7.8344\n",
      "Iteration 987/1072, Loss: 7.8543\n",
      "Iteration 988/1072, Loss: 7.8587\n",
      "Iteration 989/1072, Loss: 7.9055\n",
      "Iteration 990/1072, Loss: 7.9029\n",
      "Iteration 991/1072, Loss: 7.8716\n",
      "Iteration 992/1072, Loss: 7.8776\n",
      "Iteration 993/1072, Loss: 7.8489\n",
      "Iteration 994/1072, Loss: 7.8882\n",
      "Iteration 995/1072, Loss: 7.8731\n",
      "Iteration 996/1072, Loss: 7.8167\n",
      "Iteration 997/1072, Loss: 7.9513\n",
      "Iteration 998/1072, Loss: 7.8492\n",
      "Iteration 999/1072, Loss: 7.9026\n",
      "Iteration 1000/1072, Loss: 7.9689\n",
      "Iteration 1001/1072, Loss: 7.8599\n",
      "Iteration 1002/1072, Loss: 7.8887\n",
      "Iteration 1003/1072, Loss: 7.8961\n",
      "Iteration 1004/1072, Loss: 7.8760\n",
      "Iteration 1005/1072, Loss: 7.8642\n",
      "Iteration 1006/1072, Loss: 7.8719\n",
      "Iteration 1007/1072, Loss: 7.8391\n",
      "Iteration 1008/1072, Loss: 7.8963\n",
      "Iteration 1009/1072, Loss: 7.9287\n",
      "Iteration 1010/1072, Loss: 7.8988\n",
      "Iteration 1011/1072, Loss: 7.8658\n",
      "Iteration 1012/1072, Loss: 7.9533\n",
      "Iteration 1013/1072, Loss: 7.8449\n",
      "Iteration 1014/1072, Loss: 7.9182\n",
      "Iteration 1015/1072, Loss: 7.8889\n",
      "Iteration 1016/1072, Loss: 7.8979\n",
      "Iteration 1017/1072, Loss: 7.8725\n",
      "Iteration 1018/1072, Loss: 7.8620\n",
      "Iteration 1019/1072, Loss: 7.9193\n",
      "Iteration 1020/1072, Loss: 7.9053\n",
      "Iteration 1021/1072, Loss: 7.8623\n",
      "Iteration 1064/1072, Loss: 7.8637\n",
      "Iteration 1065/1072, Loss: 7.8764\n",
      "Iteration 1066/1072, Loss: 7.8661\n",
      "Iteration 1067/1072, Loss: 7.9253\n",
      "Iteration 1068/1072, Loss: 7.8844\n",
      "Iteration 1069/1072, Loss: 7.9535\n",
      "Iteration 1070/1072, Loss: 7.9186\n",
      "Iteration 1071/1072, Loss: 7.8584\n",
      "Iteration 1072/1072, Loss: 7.9580\n",
      "Epoch 4/10, Loss: 7.8883\n",
      "Checkpoint saved at checkpoints/efficientnet-b4_epoch_4.pth\n",
      "Validation Accuracy: 0.98%\n",
      "Iteration 1/1072, Loss: 7.9106\n",
      "Iteration 2/1072, Loss: 7.8561\n",
      "Iteration 3/1072, Loss: 7.8136\n",
      "Iteration 4/1072, Loss: 7.8589\n",
      "Iteration 5/1072, Loss: 7.8783\n",
      "Iteration 6/1072, Loss: 7.8463\n",
      "Iteration 7/1072, Loss: 7.7206\n",
      "Iteration 8/1072, Loss: 7.8754\n",
      "Iteration 9/1072, Loss: 7.8416\n",
      "Iteration 10/1072, Loss: 7.8738\n",
      "Iteration 11/1072, Loss: 7.8576\n",
      "Iteration 12/1072, Loss: 7.8712\n",
      "Iteration 13/1072, Loss: 7.6825\n",
      "Iteration 14/1072, Loss: 7.7889\n",
      "Iteration 15/1072, Loss: 7.8630\n",
      "Iteration 16/1072, Loss: 7.8547\n",
      "Iteration 17/1072, Loss: 7.8582\n",
      "Iteration 18/1072, Loss: 7.8325\n",
      "Iteration 19/1072, Loss: 7.8615\n",
      "Iteration 20/1072, Loss: 7.9231\n",
      "Iteration 21/1072, Loss: 7.8119\n",
      "Iteration 22/1072, Loss: 7.8276\n",
      "Iteration 23/1072, Loss: 7.8019\n",
      "Iteration 24/1072, Loss: 7.7934\n",
      "Iteration 25/1072, Loss: 7.8829\n",
      "Iteration 26/1072, Loss: 7.8271\n",
      "Iteration 27/1072, Loss: 7.8456\n",
      "Iteration 28/1072, Loss: 7.8362\n",
      "Iteration 29/1072, Loss: 7.8718\n",
      "Iteration 30/1072, Loss: 7.8762\n",
      "Iteration 31/1072, Loss: 7.8335\n",
      "Iteration 32/1072, Loss: 7.8686\n",
      "Iteration 33/1072, Loss: 7.8137\n",
      "Iteration 34/1072, Loss: 7.9050\n",
      "Iteration 35/1072, Loss: 7.7732\n",
      "Iteration 36/1072, Loss: 7.8463\n",
      "Iteration 37/1072, Loss: 7.8412\n",
      "Iteration 38/1072, Loss: 7.8473\n",
      "Iteration 39/1072, Loss: 7.8452\n",
      "Iteration 40/1072, Loss: 7.8997\n",
      "Iteration 41/1072, Loss: 7.8057\n",
      "Iteration 42/1072, Loss: 7.8707\n",
      "Iteration 86/1072, Loss: 7.8576\n",
      "Iteration 87/1072, Loss: 7.8307\n",
      "Iteration 88/1072, Loss: 7.8611\n",
      "Iteration 89/1072, Loss: 7.8423\n",
      "Iteration 90/1072, Loss: 7.8183\n",
      "Iteration 91/1072, Loss: 7.8094\n",
      "Iteration 92/1072, Loss: 7.7999\n",
      "Iteration 93/1072, Loss: 7.9141\n",
      "Iteration 94/1072, Loss: 7.8650\n",
      "Iteration 95/1072, Loss: 7.8856\n",
      "Iteration 96/1072, Loss: 7.9244\n",
      "Iteration 97/1072, Loss: 7.7969\n",
      "Iteration 98/1072, Loss: 7.8271\n",
      "Iteration 99/1072, Loss: 7.7707\n",
      "Iteration 100/1072, Loss: 7.7479\n",
      "Iteration 101/1072, Loss: 7.8352\n",
      "Iteration 102/1072, Loss: 7.8462\n",
      "Iteration 103/1072, Loss: 7.8707\n",
      "Iteration 104/1072, Loss: 7.7994\n",
      "Iteration 105/1072, Loss: 7.8148\n",
      "Iteration 106/1072, Loss: 7.8436\n",
      "Iteration 107/1072, Loss: 7.9180\n",
      "Iteration 108/1072, Loss: 7.8444\n",
      "Iteration 109/1072, Loss: 7.8132\n",
      "Iteration 110/1072, Loss: 7.8648\n",
      "Iteration 111/1072, Loss: 7.8890\n",
      "Iteration 112/1072, Loss: 7.9095\n",
      "Iteration 113/1072, Loss: 7.8306\n",
      "Iteration 114/1072, Loss: 7.8684\n",
      "Iteration 115/1072, Loss: 7.7428\n",
      "Iteration 116/1072, Loss: 7.8914\n",
      "Iteration 117/1072, Loss: 7.8390\n",
      "Iteration 118/1072, Loss: 7.8718\n",
      "Iteration 119/1072, Loss: 7.8563\n",
      "Iteration 120/1072, Loss: 7.8527\n",
      "Iteration 121/1072, Loss: 7.8061\n",
      "Iteration 122/1072, Loss: 7.7870\n",
      "Iteration 123/1072, Loss: 7.8442\n",
      "Iteration 124/1072, Loss: 7.8667\n",
      "Iteration 125/1072, Loss: 7.8046\n",
      "Iteration 126/1072, Loss: 7.8795\n",
      "Iteration 127/1072, Loss: 7.8359\n",
      "Iteration 128/1072, Loss: 7.7771\n",
      "Iteration 129/1072, Loss: 7.8548\n",
      "Iteration 130/1072, Loss: 7.7788\n",
      "Iteration 131/1072, Loss: 7.8830\n",
      "Iteration 132/1072, Loss: 7.8435\n",
      "Iteration 133/1072, Loss: 7.8862\n",
      "Iteration 134/1072, Loss: 7.8306\n",
      "Iteration 135/1072, Loss: 7.8471\n",
      "Iteration 136/1072, Loss: 7.7846\n",
      "Iteration 137/1072, Loss: 7.8566\n",
      "Iteration 138/1072, Loss: 7.8525\n",
      "Iteration 139/1072, Loss: 7.9184\n",
      "Iteration 140/1072, Loss: 7.7191\n",
      "Iteration 141/1072, Loss: 7.9109\n",
      "Iteration 142/1072, Loss: 7.7903\n",
      "Iteration 143/1072, Loss: 7.8243\n",
      "Iteration 144/1072, Loss: 7.8518\n",
      "Iteration 145/1072, Loss: 7.8351\n",
      "Iteration 146/1072, Loss: 7.8664\n",
      "Iteration 147/1072, Loss: 7.8637\n",
      "Iteration 148/1072, Loss: 7.8195\n",
      "Iteration 149/1072, Loss: 7.8292\n",
      "Iteration 150/1072, Loss: 7.8189\n",
      "Iteration 151/1072, Loss: 7.7854\n",
      "Iteration 152/1072, Loss: 7.8624\n",
      "Iteration 153/1072, Loss: 7.7845\n",
      "Iteration 154/1072, Loss: 7.8301\n",
      "Iteration 155/1072, Loss: 7.8159\n",
      "Iteration 156/1072, Loss: 7.8558\n",
      "Iteration 157/1072, Loss: 7.8253\n",
      "Iteration 158/1072, Loss: 7.8655\n",
      "Iteration 159/1072, Loss: 7.8479\n",
      "Iteration 160/1072, Loss: 7.7813\n",
      "Iteration 161/1072, Loss: 7.8424\n",
      "Iteration 162/1072, Loss: 7.8790\n",
      "Iteration 163/1072, Loss: 7.8842\n",
      "Iteration 164/1072, Loss: 7.8322\n",
      "Iteration 165/1072, Loss: 7.8496\n",
      "Iteration 166/1072, Loss: 7.8372\n",
      "Iteration 167/1072, Loss: 7.8192\n",
      "Iteration 168/1072, Loss: 7.8645\n",
      "Iteration 169/1072, Loss: 7.7926\n",
      "Iteration 170/1072, Loss: 7.9064\n",
      "Iteration 171/1072, Loss: 7.8358\n",
      "Iteration 172/1072, Loss: 7.8162\n",
      "Iteration 173/1072, Loss: 7.8414\n",
      "Iteration 174/1072, Loss: 7.8583\n",
      "Iteration 175/1072, Loss: 7.8439\n",
      "Iteration 176/1072, Loss: 7.8610\n",
      "Iteration 177/1072, Loss: 7.8828\n",
      "Iteration 178/1072, Loss: 7.7789\n",
      "Iteration 179/1072, Loss: 7.8882\n",
      "Iteration 180/1072, Loss: 7.8456\n",
      "Iteration 181/1072, Loss: 7.8033\n",
      "Iteration 182/1072, Loss: 7.8731\n",
      "Iteration 183/1072, Loss: 7.8602\n",
      "Iteration 184/1072, Loss: 7.8291\n",
      "Iteration 185/1072, Loss: 7.8655\n",
      "Iteration 186/1072, Loss: 7.7664\n",
      "Iteration 187/1072, Loss: 7.8115\n",
      "Iteration 188/1072, Loss: 7.8304\n",
      "Iteration 189/1072, Loss: 7.8185\n",
      "Iteration 190/1072, Loss: 7.8160\n",
      "Iteration 191/1072, Loss: 7.8443\n",
      "Iteration 192/1072, Loss: 7.7940\n",
      "Iteration 193/1072, Loss: 7.7837\n",
      "Iteration 194/1072, Loss: 7.7908\n",
      "Iteration 195/1072, Loss: 7.8566\n",
      "Iteration 196/1072, Loss: 7.8305\n",
      "Iteration 197/1072, Loss: 7.8515\n",
      "Iteration 198/1072, Loss: 7.8353\n",
      "Iteration 199/1072, Loss: 7.8658\n",
      "Iteration 200/1072, Loss: 7.8187\n",
      "Iteration 201/1072, Loss: 7.8562\n",
      "Iteration 202/1072, Loss: 7.7889\n",
      "Iteration 203/1072, Loss: 7.8182\n",
      "Iteration 204/1072, Loss: 7.8329\n",
      "Iteration 205/1072, Loss: 7.8332\n",
      "Iteration 206/1072, Loss: 7.8534\n",
      "Iteration 207/1072, Loss: 7.8444\n",
      "Iteration 208/1072, Loss: 7.8434\n",
      "Iteration 209/1072, Loss: 7.8295\n",
      "Iteration 210/1072, Loss: 7.8205\n",
      "Iteration 211/1072, Loss: 7.8388\n",
      "Iteration 212/1072, Loss: 7.8612\n",
      "Iteration 213/1072, Loss: 7.8778\n",
      "Iteration 214/1072, Loss: 7.7852\n",
      "Iteration 215/1072, Loss: 7.7944\n",
      "Iteration 216/1072, Loss: 7.8074\n",
      "Iteration 217/1072, Loss: 7.8585\n",
      "Iteration 218/1072, Loss: 7.8520\n",
      "Iteration 219/1072, Loss: 7.8260\n",
      "Iteration 220/1072, Loss: 7.8512\n",
      "Iteration 221/1072, Loss: 7.9001\n",
      "Iteration 222/1072, Loss: 7.7772\n",
      "Iteration 223/1072, Loss: 7.8139\n",
      "Iteration 224/1072, Loss: 7.8460\n",
      "Iteration 225/1072, Loss: 7.8199\n",
      "Iteration 226/1072, Loss: 7.9003\n",
      "Iteration 227/1072, Loss: 7.8475\n",
      "Iteration 228/1072, Loss: 7.8731\n",
      "Iteration 229/1072, Loss: 7.8056\n",
      "Iteration 230/1072, Loss: 7.8329\n",
      "Iteration 231/1072, Loss: 7.7440\n",
      "Iteration 232/1072, Loss: 7.8537\n",
      "Iteration 233/1072, Loss: 7.8075\n",
      "Iteration 234/1072, Loss: 7.8053\n",
      "Iteration 235/1072, Loss: 7.7591\n",
      "Iteration 236/1072, Loss: 7.8966\n",
      "Iteration 237/1072, Loss: 7.9421\n",
      "Iteration 238/1072, Loss: 7.8530\n",
      "Iteration 239/1072, Loss: 7.7909\n",
      "Iteration 240/1072, Loss: 7.8660\n",
      "Iteration 241/1072, Loss: 7.8654\n",
      "Iteration 242/1072, Loss: 7.7658\n",
      "Iteration 243/1072, Loss: 7.8505\n",
      "Iteration 244/1072, Loss: 7.9401\n",
      "Iteration 245/1072, Loss: 7.8151\n",
      "Iteration 246/1072, Loss: 7.9016\n",
      "Iteration 247/1072, Loss: 7.8129\n",
      "Iteration 248/1072, Loss: 7.7810\n",
      "Iteration 249/1072, Loss: 7.8067\n",
      "Iteration 250/1072, Loss: 7.9035\n",
      "Iteration 251/1072, Loss: 7.8270\n",
      "Iteration 252/1072, Loss: 7.8370\n",
      "Iteration 253/1072, Loss: 7.9007\n",
      "Iteration 254/1072, Loss: 7.7302\n",
      "Iteration 255/1072, Loss: 7.8936\n",
      "Iteration 256/1072, Loss: 7.8400\n",
      "Iteration 257/1072, Loss: 7.8172\n",
      "Iteration 258/1072, Loss: 7.8130\n",
      "Iteration 259/1072, Loss: 7.8164\n",
      "Iteration 260/1072, Loss: 7.8314\n",
      "Iteration 261/1072, Loss: 7.7677\n",
      "Iteration 262/1072, Loss: 7.7927\n",
      "Iteration 263/1072, Loss: 7.9120\n",
      "Iteration 264/1072, Loss: 7.8305\n",
      "Iteration 265/1072, Loss: 7.9191\n",
      "Iteration 266/1072, Loss: 7.8114\n",
      "Iteration 267/1072, Loss: 7.7891\n",
      "Iteration 268/1072, Loss: 7.7579\n",
      "Iteration 269/1072, Loss: 7.8980\n",
      "Iteration 270/1072, Loss: 7.8624\n",
      "Iteration 271/1072, Loss: 7.8005\n",
      "Iteration 272/1072, Loss: 7.8531\n",
      "Iteration 273/1072, Loss: 7.8027\n",
      "Iteration 274/1072, Loss: 7.7467\n",
      "Iteration 275/1072, Loss: 7.8221\n",
      "Iteration 276/1072, Loss: 7.8079\n",
      "Iteration 277/1072, Loss: 7.9011\n",
      "Iteration 278/1072, Loss: 7.8762\n",
      "Iteration 279/1072, Loss: 7.8257\n",
      "Iteration 280/1072, Loss: 7.8554\n",
      "Iteration 281/1072, Loss: 7.7923\n",
      "Iteration 282/1072, Loss: 7.8392\n",
      "Iteration 283/1072, Loss: 7.7710\n",
      "Iteration 284/1072, Loss: 7.8489\n",
      "Iteration 285/1072, Loss: 7.8383\n",
      "Iteration 286/1072, Loss: 7.9251\n",
      "Iteration 287/1072, Loss: 7.8656\n",
      "Iteration 288/1072, Loss: 7.8218\n",
      "Iteration 289/1072, Loss: 7.8625\n",
      "Iteration 290/1072, Loss: 7.8122\n",
      "Iteration 291/1072, Loss: 7.7722\n",
      "Iteration 292/1072, Loss: 7.8041\n",
      "Iteration 293/1072, Loss: 7.8680\n",
      "Iteration 294/1072, Loss: 7.8367\n",
      "Iteration 295/1072, Loss: 7.8381\n",
      "Iteration 296/1072, Loss: 7.7925\n",
      "Iteration 297/1072, Loss: 7.8286\n",
      "Iteration 298/1072, Loss: 7.8256\n",
      "Iteration 299/1072, Loss: 7.8936\n",
      "Iteration 300/1072, Loss: 7.8369\n",
      "Iteration 301/1072, Loss: 7.7594\n",
      "Iteration 302/1072, Loss: 7.7874\n",
      "Iteration 303/1072, Loss: 7.8155\n",
      "Iteration 304/1072, Loss: 7.8212\n",
      "Iteration 305/1072, Loss: 7.8847\n",
      "Iteration 306/1072, Loss: 7.8641\n",
      "Iteration 307/1072, Loss: 7.8072\n",
      "Iteration 308/1072, Loss: 7.8909\n",
      "Iteration 309/1072, Loss: 7.8928\n",
      "Iteration 310/1072, Loss: 7.8457\n",
      "Iteration 311/1072, Loss: 7.8444\n",
      "Iteration 312/1072, Loss: 7.8776\n",
      "Iteration 313/1072, Loss: 7.8727\n",
      "Iteration 314/1072, Loss: 7.8592\n",
      "Iteration 315/1072, Loss: 7.7633\n",
      "Iteration 316/1072, Loss: 7.7860\n",
      "Iteration 317/1072, Loss: 7.7715\n",
      "Iteration 318/1072, Loss: 7.8467\n",
      "Iteration 319/1072, Loss: 7.9079\n",
      "Iteration 320/1072, Loss: 7.8279\n",
      "Iteration 321/1072, Loss: 7.8121\n",
      "Iteration 322/1072, Loss: 7.8150\n",
      "Iteration 323/1072, Loss: 7.7749\n",
      "Iteration 324/1072, Loss: 7.8938\n",
      "Iteration 325/1072, Loss: 7.8555\n",
      "Iteration 326/1072, Loss: 7.8375\n",
      "Iteration 327/1072, Loss: 7.8099\n",
      "Iteration 328/1072, Loss: 7.7662\n",
      "Iteration 329/1072, Loss: 7.8584\n",
      "Iteration 330/1072, Loss: 7.8570\n",
      "Iteration 331/1072, Loss: 7.8137\n",
      "Iteration 332/1072, Loss: 7.9047\n",
      "Iteration 333/1072, Loss: 7.8383\n",
      "Iteration 334/1072, Loss: 7.8425\n",
      "Iteration 335/1072, Loss: 7.8834\n",
      "Iteration 336/1072, Loss: 7.8730\n",
      "Iteration 337/1072, Loss: 7.8221\n",
      "Iteration 338/1072, Loss: 7.8556\n",
      "Iteration 339/1072, Loss: 7.8346\n",
      "Iteration 340/1072, Loss: 7.8066\n",
      "Iteration 341/1072, Loss: 7.8027\n",
      "Iteration 342/1072, Loss: 7.8213\n",
      "Iteration 343/1072, Loss: 7.9070\n",
      "Iteration 344/1072, Loss: 7.8030\n",
      "Iteration 345/1072, Loss: 7.7627\n",
      "Iteration 346/1072, Loss: 7.7690\n",
      "Iteration 347/1072, Loss: 7.8304\n",
      "Iteration 348/1072, Loss: 7.7681\n",
      "Iteration 349/1072, Loss: 7.8750\n",
      "Iteration 350/1072, Loss: 7.8068\n",
      "Iteration 351/1072, Loss: 7.9020\n",
      "Iteration 352/1072, Loss: 7.7502\n",
      "Iteration 353/1072, Loss: 7.8339\n",
      "Iteration 354/1072, Loss: 7.7911\n",
      "Iteration 355/1072, Loss: 7.8054\n",
      "Iteration 356/1072, Loss: 7.8138\n",
      "Iteration 357/1072, Loss: 7.8289\n",
      "Iteration 358/1072, Loss: 7.8435\n",
      "Iteration 359/1072, Loss: 7.8261\n",
      "Iteration 360/1072, Loss: 7.7755\n",
      "Iteration 361/1072, Loss: 7.7980\n",
      "Iteration 362/1072, Loss: 7.8185\n",
      "Iteration 363/1072, Loss: 7.7935\n",
      "Iteration 364/1072, Loss: 7.9080\n",
      "Iteration 365/1072, Loss: 7.8333\n",
      "Iteration 366/1072, Loss: 7.7905\n",
      "Iteration 367/1072, Loss: 7.8560\n",
      "Iteration 368/1072, Loss: 7.7831\n",
      "Iteration 369/1072, Loss: 7.8382\n",
      "Iteration 370/1072, Loss: 7.7973\n",
      "Iteration 371/1072, Loss: 7.8926\n",
      "Iteration 372/1072, Loss: 7.8514\n",
      "Iteration 373/1072, Loss: 7.8689\n",
      "Iteration 374/1072, Loss: 7.7928\n",
      "Iteration 375/1072, Loss: 7.8363\n",
      "Iteration 376/1072, Loss: 7.7075\n",
      "Iteration 377/1072, Loss: 7.8287\n",
      "Iteration 378/1072, Loss: 7.8229\n",
      "Iteration 379/1072, Loss: 7.7931\n",
      "Iteration 380/1072, Loss: 7.8859\n",
      "Iteration 381/1072, Loss: 7.8086\n",
      "Iteration 382/1072, Loss: 7.8688\n",
      "Iteration 383/1072, Loss: 7.7414\n",
      "Iteration 384/1072, Loss: 7.8328\n",
      "Iteration 385/1072, Loss: 7.8279\n",
      "Iteration 386/1072, Loss: 7.7645\n",
      "Iteration 387/1072, Loss: 7.8309\n",
      "Iteration 388/1072, Loss: 7.7993\n",
      "Iteration 389/1072, Loss: 7.8087\n",
      "Iteration 390/1072, Loss: 7.8632\n",
      "Iteration 391/1072, Loss: 7.8149\n",
      "Iteration 392/1072, Loss: 7.7727\n",
      "Iteration 393/1072, Loss: 7.8387\n",
      "Iteration 394/1072, Loss: 7.8383\n",
      "Iteration 395/1072, Loss: 7.7833\n",
      "Iteration 396/1072, Loss: 7.8380\n",
      "Iteration 397/1072, Loss: 7.8502\n",
      "Iteration 398/1072, Loss: 7.8109\n",
      "Iteration 399/1072, Loss: 7.8094\n",
      "Iteration 400/1072, Loss: 7.8378\n",
      "Iteration 401/1072, Loss: 7.8561\n",
      "Iteration 402/1072, Loss: 7.7741\n",
      "Iteration 403/1072, Loss: 7.8606\n",
      "Iteration 404/1072, Loss: 7.8080\n",
      "Iteration 405/1072, Loss: 7.8752\n",
      "Iteration 406/1072, Loss: 7.7999\n",
      "Iteration 407/1072, Loss: 7.8292\n",
      "Iteration 408/1072, Loss: 7.8176\n",
      "Iteration 409/1072, Loss: 7.8378\n",
      "Iteration 410/1072, Loss: 7.7882\n",
      "Iteration 411/1072, Loss: 7.8044\n",
      "Iteration 412/1072, Loss: 7.7891\n",
      "Iteration 413/1072, Loss: 7.8917\n",
      "Iteration 414/1072, Loss: 7.7763\n",
      "Iteration 415/1072, Loss: 7.8267\n",
      "Iteration 416/1072, Loss: 7.8128\n",
      "Iteration 417/1072, Loss: 7.8593\n",
      "Iteration 418/1072, Loss: 7.7919\n",
      "Iteration 419/1072, Loss: 7.8897\n",
      "Iteration 420/1072, Loss: 7.8729\n",
      "Iteration 421/1072, Loss: 7.8926\n",
      "Iteration 422/1072, Loss: 7.8336\n",
      "Iteration 423/1072, Loss: 7.7897\n",
      "Iteration 424/1072, Loss: 7.8124\n",
      "Iteration 425/1072, Loss: 7.8772\n",
      "Iteration 426/1072, Loss: 7.8496\n",
      "Iteration 427/1072, Loss: 7.7639\n",
      "Iteration 428/1072, Loss: 7.7980\n",
      "Iteration 429/1072, Loss: 7.9260\n",
      "Iteration 430/1072, Loss: 7.7965\n",
      "Iteration 431/1072, Loss: 7.8802\n",
      "Iteration 432/1072, Loss: 7.7517\n",
      "Iteration 433/1072, Loss: 7.8186\n",
      "Iteration 434/1072, Loss: 7.8221\n",
      "Iteration 435/1072, Loss: 7.8168\n",
      "Iteration 436/1072, Loss: 7.8204\n",
      "Iteration 437/1072, Loss: 7.7991\n",
      "Iteration 438/1072, Loss: 7.8231\n",
      "Iteration 439/1072, Loss: 7.7171\n",
      "Iteration 440/1072, Loss: 7.8720\n",
      "Iteration 441/1072, Loss: 7.8416\n",
      "Iteration 442/1072, Loss: 7.8686\n",
      "Iteration 443/1072, Loss: 7.8449\n",
      "Iteration 444/1072, Loss: 7.8491\n",
      "Iteration 445/1072, Loss: 7.8458\n",
      "Iteration 446/1072, Loss: 7.8699\n",
      "Iteration 447/1072, Loss: 7.7765\n",
      "Iteration 448/1072, Loss: 7.8437\n",
      "Iteration 449/1072, Loss: 7.7904\n",
      "Iteration 450/1072, Loss: 7.7960\n",
      "Iteration 451/1072, Loss: 7.8445\n",
      "Iteration 452/1072, Loss: 7.8546\n",
      "Iteration 453/1072, Loss: 7.7856\n",
      "Iteration 454/1072, Loss: 7.7848\n",
      "Iteration 455/1072, Loss: 7.8411\n",
      "Iteration 456/1072, Loss: 7.8066\n",
      "Iteration 457/1072, Loss: 7.8985\n",
      "Iteration 458/1072, Loss: 7.8056\n",
      "Iteration 459/1072, Loss: 7.8482\n",
      "Iteration 460/1072, Loss: 7.8782\n",
      "Iteration 461/1072, Loss: 7.7849\n",
      "Iteration 462/1072, Loss: 7.9287\n",
      "Iteration 463/1072, Loss: 7.8762\n",
      "Iteration 464/1072, Loss: 7.7896\n",
      "Iteration 465/1072, Loss: 7.8671\n",
      "Iteration 466/1072, Loss: 7.8190\n",
      "Iteration 467/1072, Loss: 7.8020\n",
      "Iteration 468/1072, Loss: 7.7406\n",
      "Iteration 469/1072, Loss: 7.7311\n",
      "Iteration 470/1072, Loss: 7.8107\n",
      "Iteration 471/1072, Loss: 7.7937\n",
      "Iteration 472/1072, Loss: 7.7765\n",
      "Iteration 473/1072, Loss: 7.8424\n",
      "Iteration 474/1072, Loss: 7.8080\n",
      "Iteration 475/1072, Loss: 7.9004\n",
      "Iteration 476/1072, Loss: 7.7909\n",
      "Iteration 477/1072, Loss: 7.8065\n",
      "Iteration 478/1072, Loss: 7.8912\n",
      "Iteration 479/1072, Loss: 7.7554\n",
      "Iteration 480/1072, Loss: 7.8498\n",
      "Iteration 481/1072, Loss: 7.9043\n",
      "Iteration 482/1072, Loss: 7.8485\n",
      "Iteration 483/1072, Loss: 7.8939\n",
      "Iteration 484/1072, Loss: 7.8781\n",
      "Iteration 485/1072, Loss: 7.8456\n",
      "Iteration 486/1072, Loss: 7.7894\n",
      "Iteration 487/1072, Loss: 7.7386\n",
      "Iteration 488/1072, Loss: 7.8010\n",
      "Iteration 489/1072, Loss: 7.8115\n",
      "Iteration 490/1072, Loss: 7.8733\n",
      "Iteration 491/1072, Loss: 7.8568\n",
      "Iteration 492/1072, Loss: 7.8518\n",
      "Iteration 493/1072, Loss: 7.7472\n",
      "Iteration 494/1072, Loss: 7.8215\n",
      "Iteration 495/1072, Loss: 7.8330\n",
      "Iteration 496/1072, Loss: 7.8262\n",
      "Iteration 497/1072, Loss: 7.8242\n",
      "Iteration 498/1072, Loss: 7.8560\n",
      "Iteration 499/1072, Loss: 7.8833\n",
      "Iteration 500/1072, Loss: 7.7966\n",
      "Iteration 501/1072, Loss: 7.8428\n",
      "Iteration 502/1072, Loss: 7.8770\n",
      "Iteration 503/1072, Loss: 7.8069\n",
      "Iteration 504/1072, Loss: 7.8152\n",
      "Iteration 505/1072, Loss: 7.8576\n",
      "Iteration 506/1072, Loss: 7.8271\n",
      "Iteration 507/1072, Loss: 7.8190\n",
      "Iteration 508/1072, Loss: 7.8407\n",
      "Iteration 509/1072, Loss: 7.8457\n",
      "Iteration 510/1072, Loss: 7.8832\n",
      "Iteration 511/1072, Loss: 7.8262\n",
      "Iteration 512/1072, Loss: 7.8654\n",
      "Iteration 513/1072, Loss: 7.8218\n",
      "Iteration 514/1072, Loss: 7.8312\n",
      "Iteration 515/1072, Loss: 7.8006\n",
      "Iteration 516/1072, Loss: 7.8558\n",
      "Iteration 517/1072, Loss: 7.7745\n",
      "Iteration 518/1072, Loss: 7.8061\n",
      "Iteration 519/1072, Loss: 7.8462\n",
      "Iteration 520/1072, Loss: 7.8576\n",
      "Iteration 521/1072, Loss: 7.8782\n",
      "Iteration 522/1072, Loss: 7.8399\n",
      "Iteration 523/1072, Loss: 7.8517\n",
      "Iteration 524/1072, Loss: 7.7517\n",
      "Iteration 525/1072, Loss: 7.8579\n",
      "Iteration 526/1072, Loss: 7.8666\n",
      "Iteration 527/1072, Loss: 7.8439\n",
      "Iteration 528/1072, Loss: 7.8385\n",
      "Iteration 529/1072, Loss: 7.8117\n",
      "Iteration 530/1072, Loss: 7.8255\n",
      "Iteration 531/1072, Loss: 7.8094\n",
      "Iteration 532/1072, Loss: 7.8197\n",
      "Iteration 533/1072, Loss: 7.8153\n",
      "Iteration 534/1072, Loss: 7.8318\n",
      "Iteration 535/1072, Loss: 7.8999\n",
      "Iteration 536/1072, Loss: 7.7666\n",
      "Iteration 537/1072, Loss: 7.8148\n",
      "Iteration 538/1072, Loss: 7.7896\n",
      "Iteration 539/1072, Loss: 7.8399\n",
      "Iteration 540/1072, Loss: 7.7896\n",
      "Iteration 541/1072, Loss: 7.8352\n",
      "Iteration 542/1072, Loss: 7.8336\n",
      "Iteration 543/1072, Loss: 7.8463\n",
      "Iteration 544/1072, Loss: 7.8100\n",
      "Iteration 545/1072, Loss: 7.8443\n",
      "Iteration 546/1072, Loss: 7.7801\n",
      "Iteration 547/1072, Loss: 7.8526\n",
      "Iteration 548/1072, Loss: 7.8152\n",
      "Iteration 549/1072, Loss: 7.9025\n",
      "Iteration 550/1072, Loss: 7.9040\n",
      "Iteration 551/1072, Loss: 7.9131\n",
      "Iteration 552/1072, Loss: 7.8245\n",
      "Iteration 553/1072, Loss: 7.7920\n",
      "Iteration 554/1072, Loss: 7.7454\n",
      "Iteration 555/1072, Loss: 7.8567\n",
      "Iteration 556/1072, Loss: 7.8102\n",
      "Iteration 557/1072, Loss: 7.7955\n",
      "Iteration 558/1072, Loss: 7.7601\n",
      "Iteration 559/1072, Loss: 7.7939\n",
      "Iteration 560/1072, Loss: 7.7681\n",
      "Iteration 561/1072, Loss: 7.7645\n",
      "Iteration 562/1072, Loss: 7.8441\n",
      "Iteration 563/1072, Loss: 7.9048\n",
      "Iteration 564/1072, Loss: 7.7890\n",
      "Iteration 565/1072, Loss: 7.8482\n",
      "Iteration 566/1072, Loss: 7.8113\n",
      "Iteration 567/1072, Loss: 7.8405\n",
      "Iteration 568/1072, Loss: 7.8411\n",
      "Iteration 569/1072, Loss: 7.8385\n",
      "Iteration 570/1072, Loss: 7.8242\n",
      "Iteration 571/1072, Loss: 7.8536\n",
      "Iteration 572/1072, Loss: 7.9195\n",
      "Iteration 573/1072, Loss: 7.8774\n",
      "Iteration 574/1072, Loss: 7.7987\n",
      "Iteration 575/1072, Loss: 7.8003\n",
      "Iteration 576/1072, Loss: 7.9172\n",
      "Iteration 577/1072, Loss: 7.8157\n",
      "Iteration 578/1072, Loss: 7.8896\n",
      "Iteration 579/1072, Loss: 7.8477\n",
      "Iteration 580/1072, Loss: 7.8435\n",
      "Iteration 581/1072, Loss: 7.9331\n",
      "Iteration 582/1072, Loss: 7.8057\n",
      "Iteration 583/1072, Loss: 7.8809\n",
      "Iteration 584/1072, Loss: 7.8355\n",
      "Iteration 585/1072, Loss: 7.8041\n",
      "Iteration 586/1072, Loss: 7.9018\n",
      "Iteration 587/1072, Loss: 7.7835\n",
      "Iteration 588/1072, Loss: 7.8244\n",
      "Iteration 589/1072, Loss: 7.7793\n",
      "Iteration 590/1072, Loss: 7.7746\n",
      "Iteration 591/1072, Loss: 7.8091\n",
      "Iteration 592/1072, Loss: 7.8390\n",
      "Iteration 593/1072, Loss: 7.8121\n",
      "Iteration 594/1072, Loss: 7.8876\n",
      "Iteration 595/1072, Loss: 7.8029\n",
      "Iteration 596/1072, Loss: 7.8131\n",
      "Iteration 597/1072, Loss: 7.9139\n",
      "Iteration 598/1072, Loss: 7.8333\n",
      "Iteration 599/1072, Loss: 7.7656\n",
      "Iteration 600/1072, Loss: 7.7910\n",
      "Iteration 601/1072, Loss: 7.8057\n",
      "Iteration 602/1072, Loss: 7.8597\n",
      "Iteration 603/1072, Loss: 7.8795\n",
      "Iteration 604/1072, Loss: 7.7649\n",
      "Iteration 605/1072, Loss: 7.8509\n",
      "Iteration 606/1072, Loss: 7.7573\n",
      "Iteration 607/1072, Loss: 7.8175\n",
      "Iteration 608/1072, Loss: 7.8209\n",
      "Iteration 609/1072, Loss: 7.8147\n",
      "Iteration 610/1072, Loss: 7.7589\n",
      "Iteration 611/1072, Loss: 7.8619\n",
      "Iteration 612/1072, Loss: 7.8032\n",
      "Iteration 613/1072, Loss: 7.7673\n",
      "Iteration 614/1072, Loss: 7.7550\n",
      "Iteration 615/1072, Loss: 7.8086\n",
      "Iteration 616/1072, Loss: 7.7590\n",
      "Iteration 617/1072, Loss: 7.8125\n",
      "Iteration 618/1072, Loss: 7.8991\n",
      "Iteration 619/1072, Loss: 7.8782\n",
      "Iteration 620/1072, Loss: 7.7974\n",
      "Iteration 621/1072, Loss: 7.8580\n",
      "Iteration 622/1072, Loss: 7.8440\n",
      "Iteration 623/1072, Loss: 7.8653\n",
      "Iteration 624/1072, Loss: 7.7679\n",
      "Iteration 625/1072, Loss: 7.8187\n",
      "Iteration 626/1072, Loss: 7.8487\n",
      "Iteration 627/1072, Loss: 7.8349\n",
      "Iteration 628/1072, Loss: 7.7909\n",
      "Iteration 629/1072, Loss: 7.8190\n",
      "Iteration 630/1072, Loss: 7.7993\n",
      "Iteration 631/1072, Loss: 7.8320\n",
      "Iteration 632/1072, Loss: 7.8204\n",
      "Iteration 633/1072, Loss: 7.8382\n",
      "Iteration 634/1072, Loss: 7.8121\n",
      "Iteration 635/1072, Loss: 7.8987\n",
      "Iteration 636/1072, Loss: 7.8170\n",
      "Iteration 637/1072, Loss: 7.7373\n",
      "Iteration 638/1072, Loss: 7.8095\n",
      "Iteration 639/1072, Loss: 7.7839\n",
      "Iteration 640/1072, Loss: 7.7964\n",
      "Iteration 641/1072, Loss: 7.8216\n",
      "Iteration 642/1072, Loss: 7.8239\n",
      "Iteration 643/1072, Loss: 7.7982\n",
      "Iteration 644/1072, Loss: 7.7909\n",
      "Iteration 645/1072, Loss: 7.7457\n",
      "Iteration 646/1072, Loss: 7.8970\n",
      "Iteration 647/1072, Loss: 7.7730\n",
      "Iteration 648/1072, Loss: 7.7754\n",
      "Iteration 649/1072, Loss: 7.8237\n",
      "Iteration 650/1072, Loss: 7.7825\n",
      "Iteration 651/1072, Loss: 7.8025\n",
      "Iteration 652/1072, Loss: 7.8582\n",
      "Iteration 653/1072, Loss: 7.7370\n",
      "Iteration 654/1072, Loss: 7.8148\n",
      "Iteration 655/1072, Loss: 7.8319\n",
      "Iteration 656/1072, Loss: 7.8318\n",
      "Iteration 657/1072, Loss: 7.8212\n",
      "Iteration 659/1072, Loss: 7.7493\n",
      "Iteration 660/1072, Loss: 7.8987\n",
      "Iteration 661/1072, Loss: 7.8222\n",
      "Iteration 662/1072, Loss: 7.7865\n",
      "Iteration 663/1072, Loss: 7.7896\n",
      "Iteration 664/1072, Loss: 7.8324\n",
      "Iteration 665/1072, Loss: 7.8205\n",
      "Iteration 666/1072, Loss: 7.8002\n",
      "Iteration 667/1072, Loss: 7.8442\n",
      "Iteration 668/1072, Loss: 7.8416\n",
      "Iteration 669/1072, Loss: 7.7679\n",
      "Iteration 670/1072, Loss: 7.8143\n",
      "Iteration 671/1072, Loss: 7.7459\n",
      "Iteration 672/1072, Loss: 7.8818\n",
      "Iteration 673/1072, Loss: 7.7955\n",
      "Iteration 674/1072, Loss: 7.8207\n",
      "Iteration 675/1072, Loss: 7.8485\n",
      "Iteration 676/1072, Loss: 7.8146\n",
      "Iteration 677/1072, Loss: 7.8452\n",
      "Iteration 678/1072, Loss: 7.8880\n",
      "Iteration 679/1072, Loss: 7.8055\n",
      "Iteration 680/1072, Loss: 7.8261\n",
      "Iteration 681/1072, Loss: 7.8590\n",
      "Iteration 682/1072, Loss: 7.8139\n",
      "Iteration 683/1072, Loss: 7.8411\n",
      "Iteration 684/1072, Loss: 7.8564\n",
      "Iteration 685/1072, Loss: 7.8307\n",
      "Iteration 686/1072, Loss: 7.8419\n",
      "Iteration 687/1072, Loss: 7.8223\n",
      "Iteration 688/1072, Loss: 7.8279\n",
      "Iteration 689/1072, Loss: 7.7418\n",
      "Iteration 690/1072, Loss: 7.8436\n",
      "Iteration 691/1072, Loss: 7.8058\n",
      "Iteration 692/1072, Loss: 7.6946\n",
      "Iteration 693/1072, Loss: 7.8890\n",
      "Iteration 694/1072, Loss: 7.8089\n",
      "Iteration 695/1072, Loss: 7.8208\n",
      "Iteration 696/1072, Loss: 7.8029\n",
      "Iteration 697/1072, Loss: 7.7518\n",
      "Iteration 698/1072, Loss: 7.8529\n",
      "Iteration 699/1072, Loss: 7.8604\n",
      "Iteration 700/1072, Loss: 7.7693\n",
      "Iteration 701/1072, Loss: 7.8417\n",
      "Iteration 702/1072, Loss: 7.8079\n",
      "Iteration 703/1072, Loss: 7.9016\n",
      "Iteration 704/1072, Loss: 7.8491\n",
      "Iteration 705/1072, Loss: 7.9059\n",
      "Iteration 706/1072, Loss: 7.8838\n",
      "Iteration 707/1072, Loss: 7.8411\n",
      "Iteration 708/1072, Loss: 7.8320\n",
      "Iteration 709/1072, Loss: 7.7980\n",
      "Iteration 710/1072, Loss: 7.8402\n",
      "Iteration 711/1072, Loss: 7.7741\n",
      "Iteration 712/1072, Loss: 7.8184\n",
      "Iteration 713/1072, Loss: 7.8648\n",
      "Iteration 714/1072, Loss: 7.8322\n",
      "Iteration 715/1072, Loss: 7.7702\n",
      "Iteration 716/1072, Loss: 7.8131\n",
      "Iteration 717/1072, Loss: 7.7776\n",
      "Iteration 718/1072, Loss: 7.8846\n",
      "Iteration 719/1072, Loss: 7.7348\n",
      "Iteration 720/1072, Loss: 7.8183\n",
      "Iteration 721/1072, Loss: 7.7844\n",
      "Iteration 722/1072, Loss: 7.8916\n",
      "Iteration 723/1072, Loss: 7.7944\n",
      "Iteration 724/1072, Loss: 7.8052\n",
      "Iteration 725/1072, Loss: 7.8372\n",
      "Iteration 726/1072, Loss: 7.8005\n",
      "Iteration 727/1072, Loss: 7.8305\n",
      "Iteration 728/1072, Loss: 7.8163\n",
      "Iteration 729/1072, Loss: 7.8166\n",
      "Iteration 730/1072, Loss: 7.7969\n",
      "Iteration 731/1072, Loss: 7.7837\n",
      "Iteration 732/1072, Loss: 7.8233\n",
      "Iteration 733/1072, Loss: 7.8537\n",
      "Iteration 734/1072, Loss: 7.8124\n",
      "Iteration 735/1072, Loss: 7.7984\n",
      "Iteration 736/1072, Loss: 7.8139\n",
      "Iteration 737/1072, Loss: 7.7419\n",
      "Iteration 738/1072, Loss: 7.7990\n",
      "Iteration 739/1072, Loss: 7.7881\n",
      "Iteration 740/1072, Loss: 7.8213\n",
      "Iteration 741/1072, Loss: 7.8490\n",
      "Iteration 742/1072, Loss: 7.7593\n",
      "Iteration 743/1072, Loss: 7.7337\n",
      "Iteration 744/1072, Loss: 7.8689\n",
      "Iteration 745/1072, Loss: 7.7924\n",
      "Iteration 746/1072, Loss: 7.8797\n",
      "Iteration 747/1072, Loss: 7.8354\n",
      "Iteration 748/1072, Loss: 7.7870\n",
      "Iteration 749/1072, Loss: 7.7404\n",
      "Iteration 750/1072, Loss: 7.7807\n",
      "Iteration 751/1072, Loss: 7.8498\n",
      "Iteration 752/1072, Loss: 7.6915\n",
      "Iteration 753/1072, Loss: 7.8726\n",
      "Iteration 754/1072, Loss: 7.7214\n",
      "Iteration 755/1072, Loss: 7.8522\n",
      "Iteration 756/1072, Loss: 7.8435\n",
      "Iteration 757/1072, Loss: 7.7875\n",
      "Iteration 758/1072, Loss: 7.8191\n",
      "Iteration 759/1072, Loss: 7.9443\n",
      "Iteration 760/1072, Loss: 7.8215\n",
      "Iteration 761/1072, Loss: 7.7879\n",
      "Iteration 762/1072, Loss: 7.7548\n",
      "Iteration 763/1072, Loss: 7.8089\n",
      "Iteration 764/1072, Loss: 7.8300\n",
      "Iteration 765/1072, Loss: 7.7980\n",
      "Iteration 766/1072, Loss: 7.8387\n",
      "Iteration 767/1072, Loss: 7.8154\n",
      "Iteration 768/1072, Loss: 7.8834\n",
      "Iteration 769/1072, Loss: 7.7442\n",
      "Iteration 770/1072, Loss: 7.7939\n",
      "Iteration 771/1072, Loss: 7.7360\n",
      "Iteration 772/1072, Loss: 7.8524\n",
      "Iteration 773/1072, Loss: 7.7894\n",
      "Iteration 774/1072, Loss: 7.7986\n",
      "Iteration 775/1072, Loss: 7.7952\n",
      "Iteration 776/1072, Loss: 7.8283\n",
      "Iteration 777/1072, Loss: 7.8492\n",
      "Iteration 778/1072, Loss: 7.8049\n",
      "Iteration 779/1072, Loss: 7.7776\n",
      "Iteration 780/1072, Loss: 7.8326\n",
      "Iteration 781/1072, Loss: 7.8194\n",
      "Iteration 782/1072, Loss: 7.8716\n",
      "Iteration 783/1072, Loss: 7.7403\n",
      "Iteration 784/1072, Loss: 7.7759\n",
      "Iteration 785/1072, Loss: 7.7852\n",
      "Iteration 786/1072, Loss: 7.7942\n",
      "Iteration 787/1072, Loss: 7.7978\n",
      "Iteration 788/1072, Loss: 7.8142\n",
      "Iteration 789/1072, Loss: 7.8307\n",
      "Iteration 790/1072, Loss: 7.8481\n",
      "Iteration 791/1072, Loss: 7.7895\n",
      "Iteration 792/1072, Loss: 7.8544\n",
      "Iteration 793/1072, Loss: 7.8355\n",
      "Iteration 794/1072, Loss: 7.8208\n",
      "Iteration 795/1072, Loss: 7.7699\n",
      "Iteration 796/1072, Loss: 7.7390\n",
      "Iteration 797/1072, Loss: 7.8006\n",
      "Iteration 798/1072, Loss: 7.7868\n",
      "Iteration 799/1072, Loss: 7.7643\n",
      "Iteration 800/1072, Loss: 7.8572\n",
      "Iteration 801/1072, Loss: 7.7891\n",
      "Iteration 802/1072, Loss: 7.9438\n",
      "Iteration 803/1072, Loss: 7.7772\n",
      "Iteration 804/1072, Loss: 7.8246\n",
      "Iteration 805/1072, Loss: 7.7699\n",
      "Iteration 806/1072, Loss: 7.7698\n",
      "Iteration 807/1072, Loss: 7.7936\n",
      "Iteration 808/1072, Loss: 7.7732\n",
      "Iteration 809/1072, Loss: 7.8132\n",
      "Iteration 810/1072, Loss: 7.7443\n",
      "Iteration 811/1072, Loss: 7.8321\n",
      "Iteration 812/1072, Loss: 7.8061\n",
      "Iteration 813/1072, Loss: 7.7755\n",
      "Iteration 814/1072, Loss: 7.8068\n",
      "Iteration 815/1072, Loss: 7.7702\n",
      "Iteration 816/1072, Loss: 7.8026\n",
      "Iteration 817/1072, Loss: 7.7961\n",
      "Iteration 818/1072, Loss: 7.7694\n",
      "Iteration 819/1072, Loss: 7.7235\n",
      "Iteration 820/1072, Loss: 7.7675\n",
      "Iteration 821/1072, Loss: 7.7872\n",
      "Iteration 822/1072, Loss: 7.8130\n",
      "Iteration 823/1072, Loss: 7.7842\n",
      "Iteration 824/1072, Loss: 7.8222\n",
      "Iteration 825/1072, Loss: 7.8724\n",
      "Iteration 826/1072, Loss: 7.7145\n",
      "Iteration 827/1072, Loss: 7.7788\n",
      "Iteration 828/1072, Loss: 7.7938\n",
      "Iteration 829/1072, Loss: 7.8008\n",
      "Iteration 830/1072, Loss: 7.8256\n",
      "Iteration 831/1072, Loss: 7.8286\n",
      "Iteration 832/1072, Loss: 7.8148\n",
      "Iteration 833/1072, Loss: 7.7994\n",
      "Iteration 834/1072, Loss: 7.7789\n",
      "Iteration 835/1072, Loss: 7.7872\n",
      "Iteration 836/1072, Loss: 7.7473\n",
      "Iteration 837/1072, Loss: 7.8484\n",
      "Iteration 838/1072, Loss: 7.8304\n",
      "Iteration 839/1072, Loss: 7.7126\n",
      "Iteration 840/1072, Loss: 7.7770\n",
      "Iteration 841/1072, Loss: 7.8064\n",
      "Iteration 842/1072, Loss: 7.7005\n",
      "Iteration 843/1072, Loss: 7.8162\n",
      "Iteration 844/1072, Loss: 7.8747\n",
      "Iteration 845/1072, Loss: 7.7934\n",
      "Iteration 846/1072, Loss: 7.7559\n",
      "Iteration 847/1072, Loss: 7.8445\n",
      "Iteration 848/1072, Loss: 7.8531\n",
      "Iteration 849/1072, Loss: 7.8147\n",
      "Iteration 850/1072, Loss: 7.9177\n",
      "Iteration 851/1072, Loss: 7.8666\n",
      "Iteration 852/1072, Loss: 7.7676\n",
      "Iteration 853/1072, Loss: 7.7904\n",
      "Iteration 854/1072, Loss: 7.7876\n",
      "Iteration 855/1072, Loss: 7.8328\n",
      "Iteration 856/1072, Loss: 7.8209\n",
      "Iteration 857/1072, Loss: 7.8633\n",
      "Iteration 858/1072, Loss: 7.8717\n",
      "Iteration 859/1072, Loss: 7.8493\n",
      "Iteration 860/1072, Loss: 7.8117\n",
      "Iteration 861/1072, Loss: 7.7722\n",
      "Iteration 862/1072, Loss: 7.8171\n",
      "Iteration 863/1072, Loss: 7.8268\n",
      "Iteration 864/1072, Loss: 7.8997\n",
      "Iteration 865/1072, Loss: 7.7281\n",
      "Iteration 866/1072, Loss: 7.8259\n",
      "Iteration 867/1072, Loss: 7.7866\n",
      "Iteration 868/1072, Loss: 7.8584\n",
      "Iteration 869/1072, Loss: 7.7024\n",
      "Iteration 870/1072, Loss: 7.7872\n",
      "Iteration 871/1072, Loss: 7.8309\n",
      "Iteration 872/1072, Loss: 7.8834\n",
      "Iteration 873/1072, Loss: 7.8028\n",
      "Iteration 874/1072, Loss: 7.8770\n",
      "Iteration 875/1072, Loss: 7.7984\n",
      "Iteration 876/1072, Loss: 7.8221\n",
      "Iteration 877/1072, Loss: 7.8710\n",
      "Iteration 878/1072, Loss: 7.8230\n",
      "Iteration 879/1072, Loss: 7.8438\n",
      "Iteration 880/1072, Loss: 7.7788\n",
      "Iteration 881/1072, Loss: 7.9114\n",
      "Iteration 882/1072, Loss: 7.8412\n",
      "Iteration 883/1072, Loss: 7.7948\n",
      "Iteration 884/1072, Loss: 7.8560\n",
      "Iteration 885/1072, Loss: 7.8534\n",
      "Iteration 886/1072, Loss: 7.7786\n",
      "Iteration 887/1072, Loss: 7.7586\n",
      "Iteration 888/1072, Loss: 7.7314\n",
      "Iteration 889/1072, Loss: 7.8806\n",
      "Iteration 890/1072, Loss: 7.8403\n",
      "Iteration 891/1072, Loss: 7.7574\n",
      "Iteration 892/1072, Loss: 7.8433\n",
      "Iteration 893/1072, Loss: 7.7060\n",
      "Iteration 894/1072, Loss: 7.7859\n",
      "Iteration 895/1072, Loss: 7.7817\n",
      "Iteration 896/1072, Loss: 7.7963\n",
      "Iteration 897/1072, Loss: 7.8686\n",
      "Iteration 898/1072, Loss: 7.8502\n",
      "Iteration 899/1072, Loss: 7.8352\n",
      "Iteration 900/1072, Loss: 7.7809\n",
      "Iteration 901/1072, Loss: 7.7221\n",
      "Iteration 902/1072, Loss: 7.8185\n",
      "Iteration 903/1072, Loss: 7.7871\n",
      "Iteration 904/1072, Loss: 7.7536\n",
      "Iteration 905/1072, Loss: 7.8004\n",
      "Iteration 906/1072, Loss: 7.8460\n",
      "Iteration 907/1072, Loss: 7.8158\n",
      "Iteration 908/1072, Loss: 7.8836\n",
      "Iteration 909/1072, Loss: 7.8168\n",
      "Iteration 910/1072, Loss: 7.7583\n",
      "Iteration 911/1072, Loss: 7.7713\n",
      "Iteration 912/1072, Loss: 7.7937\n",
      "Iteration 913/1072, Loss: 7.8382\n",
      "Iteration 914/1072, Loss: 7.8043\n",
      "Iteration 915/1072, Loss: 7.8847\n",
      "Iteration 916/1072, Loss: 7.7840\n",
      "Iteration 917/1072, Loss: 7.7766\n",
      "Iteration 918/1072, Loss: 7.8455\n",
      "Iteration 919/1072, Loss: 7.7652\n",
      "Iteration 920/1072, Loss: 7.8453\n",
      "Iteration 921/1072, Loss: 7.8038\n",
      "Iteration 922/1072, Loss: 7.8580\n",
      "Iteration 923/1072, Loss: 7.7826\n",
      "Iteration 924/1072, Loss: 7.7307\n",
      "Iteration 925/1072, Loss: 7.7903\n",
      "Iteration 926/1072, Loss: 7.6691\n",
      "Iteration 927/1072, Loss: 7.8069\n",
      "Iteration 928/1072, Loss: 7.7875\n",
      "Iteration 929/1072, Loss: 7.7446\n",
      "Iteration 930/1072, Loss: 7.8506\n",
      "Iteration 931/1072, Loss: 7.8146\n",
      "Iteration 932/1072, Loss: 7.7575\n",
      "Iteration 933/1072, Loss: 7.8328\n",
      "Iteration 934/1072, Loss: 7.7746\n",
      "Iteration 935/1072, Loss: 7.8255\n",
      "Iteration 936/1072, Loss: 7.8053\n",
      "Iteration 937/1072, Loss: 7.8163\n",
      "Iteration 938/1072, Loss: 7.7173\n",
      "Iteration 939/1072, Loss: 7.7785\n",
      "Iteration 940/1072, Loss: 7.8418\n",
      "Iteration 941/1072, Loss: 7.7926\n",
      "Iteration 942/1072, Loss: 7.8949\n",
      "Iteration 943/1072, Loss: 7.8595\n",
      "Iteration 944/1072, Loss: 7.7944\n",
      "Iteration 945/1072, Loss: 7.8415\n",
      "Iteration 946/1072, Loss: 7.7840\n",
      "Iteration 947/1072, Loss: 7.7235\n",
      "Iteration 948/1072, Loss: 7.8112\n",
      "Iteration 949/1072, Loss: 7.7940\n",
      "Iteration 950/1072, Loss: 7.7837\n",
      "Iteration 951/1072, Loss: 7.7659\n",
      "Iteration 952/1072, Loss: 7.8158\n",
      "Iteration 953/1072, Loss: 7.8995\n",
      "Iteration 954/1072, Loss: 7.8370\n",
      "Iteration 955/1072, Loss: 7.7827\n",
      "Iteration 956/1072, Loss: 7.8804\n",
      "Iteration 957/1072, Loss: 7.7166\n",
      "Iteration 958/1072, Loss: 7.7853\n",
      "Iteration 959/1072, Loss: 7.8245\n",
      "Iteration 960/1072, Loss: 7.8497\n",
      "Iteration 961/1072, Loss: 7.8518\n",
      "Iteration 962/1072, Loss: 7.8403\n",
      "Iteration 963/1072, Loss: 7.9134\n",
      "Iteration 964/1072, Loss: 7.7893\n",
      "Iteration 965/1072, Loss: 7.8424\n",
      "Iteration 966/1072, Loss: 7.8362\n",
      "Iteration 967/1072, Loss: 7.8132\n",
      "Iteration 968/1072, Loss: 7.6678\n",
      "Iteration 969/1072, Loss: 7.8598\n",
      "Iteration 970/1072, Loss: 7.7667\n",
      "Iteration 971/1072, Loss: 7.8817\n",
      "Iteration 972/1072, Loss: 7.8739\n",
      "Iteration 973/1072, Loss: 7.8307\n",
      "Iteration 974/1072, Loss: 7.7943\n",
      "Iteration 975/1072, Loss: 7.8128\n",
      "Iteration 976/1072, Loss: 7.8300\n",
      "Iteration 977/1072, Loss: 7.7817\n",
      "Iteration 978/1072, Loss: 7.8659\n",
      "Iteration 979/1072, Loss: 7.7152\n",
      "Iteration 980/1072, Loss: 7.8111\n",
      "Iteration 981/1072, Loss: 7.7760\n",
      "Iteration 982/1072, Loss: 7.8288\n",
      "Iteration 983/1072, Loss: 7.8100\n",
      "Iteration 984/1072, Loss: 7.7485\n",
      "Iteration 985/1072, Loss: 7.7742\n",
      "Iteration 986/1072, Loss: 7.8723\n",
      "Iteration 987/1072, Loss: 7.8075\n",
      "Iteration 988/1072, Loss: 7.7942\n",
      "Iteration 989/1072, Loss: 7.7482\n",
      "Iteration 990/1072, Loss: 7.7437\n",
      "Iteration 991/1072, Loss: 7.7570\n",
      "Iteration 992/1072, Loss: 7.8287\n",
      "Iteration 993/1072, Loss: 7.8420\n",
      "Iteration 994/1072, Loss: 7.8647\n",
      "Iteration 995/1072, Loss: 7.8840\n",
      "Iteration 996/1072, Loss: 7.6741\n",
      "Iteration 997/1072, Loss: 7.8726\n",
      "Iteration 998/1072, Loss: 7.7644\n",
      "Iteration 999/1072, Loss: 7.8418\n",
      "Iteration 1000/1072, Loss: 7.8570\n",
      "Iteration 1001/1072, Loss: 7.8002\n",
      "Iteration 1002/1072, Loss: 7.8842\n",
      "Iteration 1003/1072, Loss: 7.7055\n",
      "Iteration 1004/1072, Loss: 7.7847\n",
      "Iteration 1005/1072, Loss: 7.8856\n",
      "Iteration 1006/1072, Loss: 7.7956\n",
      "Iteration 1007/1072, Loss: 7.7811\n",
      "Iteration 1008/1072, Loss: 7.7343\n",
      "Iteration 1009/1072, Loss: 7.8511\n",
      "Iteration 1010/1072, Loss: 7.8533\n",
      "Iteration 1011/1072, Loss: 7.8168\n",
      "Iteration 1012/1072, Loss: 7.8135\n",
      "Iteration 1013/1072, Loss: 7.8253\n",
      "Iteration 1014/1072, Loss: 7.7838\n",
      "Iteration 1015/1072, Loss: 7.7272\n",
      "Iteration 1016/1072, Loss: 7.8022\n",
      "Iteration 1017/1072, Loss: 7.8454\n",
      "Iteration 1018/1072, Loss: 7.9244\n",
      "Iteration 1019/1072, Loss: 7.7939\n",
      "Iteration 1020/1072, Loss: 7.7713\n",
      "Iteration 1021/1072, Loss: 7.7757\n",
      "Iteration 1022/1072, Loss: 7.7742\n",
      "Iteration 1023/1072, Loss: 7.8233\n",
      "Iteration 1024/1072, Loss: 7.7838\n",
      "Iteration 1025/1072, Loss: 7.7939\n",
      "Iteration 1026/1072, Loss: 7.7399\n",
      "Iteration 1027/1072, Loss: 7.8373\n",
      "Iteration 1028/1072, Loss: 7.7841\n",
      "Iteration 1029/1072, Loss: 7.8041\n",
      "Iteration 1030/1072, Loss: 7.7562\n",
      "Iteration 1031/1072, Loss: 7.7911\n",
      "Iteration 1032/1072, Loss: 7.8155\n",
      "Iteration 1033/1072, Loss: 7.7824\n",
      "Iteration 1034/1072, Loss: 7.8982\n",
      "Iteration 1035/1072, Loss: 7.7906\n",
      "Iteration 1036/1072, Loss: 7.8410\n",
      "Iteration 1037/1072, Loss: 7.7482\n",
      "Iteration 1038/1072, Loss: 7.7188\n",
      "Iteration 1039/1072, Loss: 7.7905\n",
      "Iteration 1040/1072, Loss: 7.6878\n",
      "Iteration 1041/1072, Loss: 7.7735\n",
      "Iteration 1042/1072, Loss: 7.7540\n",
      "Iteration 1043/1072, Loss: 7.8083\n",
      "Iteration 1044/1072, Loss: 7.7316\n",
      "Iteration 1045/1072, Loss: 7.7598\n",
      "Iteration 1046/1072, Loss: 7.8083\n",
      "Iteration 1047/1072, Loss: 7.8029\n",
      "Iteration 1048/1072, Loss: 7.7141\n",
      "Iteration 1049/1072, Loss: 7.7987\n",
      "Iteration 1050/1072, Loss: 7.8120\n",
      "Iteration 1051/1072, Loss: 7.7914\n",
      "Iteration 1052/1072, Loss: 7.8554\n",
      "Iteration 1053/1072, Loss: 7.7800\n",
      "Iteration 1054/1072, Loss: 7.8572\n",
      "Iteration 1055/1072, Loss: 7.7963\n",
      "Iteration 1056/1072, Loss: 7.7751\n",
      "Iteration 1057/1072, Loss: 7.8065\n",
      "Iteration 1058/1072, Loss: 7.8225\n",
      "Iteration 1059/1072, Loss: 7.7684\n",
      "Iteration 1060/1072, Loss: 7.8878\n",
      "Iteration 1061/1072, Loss: 7.7924\n",
      "Iteration 1062/1072, Loss: 7.7825\n",
      "Iteration 1063/1072, Loss: 7.7455\n",
      "Iteration 1064/1072, Loss: 7.8452\n",
      "Iteration 1065/1072, Loss: 7.8301\n",
      "Iteration 1066/1072, Loss: 7.8058\n",
      "Iteration 1067/1072, Loss: 7.8712\n",
      "Iteration 1068/1072, Loss: 7.8254\n",
      "Iteration 1069/1072, Loss: 7.8532\n",
      "Iteration 1070/1072, Loss: 7.8420\n",
      "Iteration 1071/1072, Loss: 7.7977\n",
      "Iteration 1072/1072, Loss: 8.0007\n",
      "Epoch 5/10, Loss: 7.8226\n",
      "Checkpoint saved at checkpoints/efficientnet-b4_epoch_5.pth\n",
      "Validation Accuracy: 1.73%\n",
      "Iteration 1/1072, Loss: 7.8091\n",
      "Iteration 2/1072, Loss: 7.7761\n",
      "Iteration 3/1072, Loss: 7.7182\n",
      "Iteration 4/1072, Loss: 7.8364\n",
      "Iteration 5/1072, Loss: 7.7116\n",
      "Iteration 6/1072, Loss: 7.7929\n",
      "Iteration 7/1072, Loss: 7.7402\n",
      "Iteration 8/1072, Loss: 7.7887\n",
      "Iteration 9/1072, Loss: 7.7647\n",
      "Iteration 10/1072, Loss: 7.7138\n",
      "Iteration 11/1072, Loss: 7.8181\n",
      "Iteration 12/1072, Loss: 7.6843\n",
      "Iteration 13/1072, Loss: 7.7381\n",
      "Iteration 58/1072, Loss: 7.7282\n",
      "Iteration 59/1072, Loss: 7.7580\n",
      "Iteration 60/1072, Loss: 7.6286\n",
      "Iteration 61/1072, Loss: 7.7510\n",
      "Iteration 62/1072, Loss: 7.8538\n",
      "Iteration 63/1072, Loss: 7.7667\n",
      "Iteration 64/1072, Loss: 7.7547\n",
      "Iteration 65/1072, Loss: 7.8113\n",
      "Iteration 66/1072, Loss: 7.7705\n",
      "Iteration 67/1072, Loss: 7.8043\n",
      "Iteration 68/1072, Loss: 7.8152\n",
      "Iteration 69/1072, Loss: 7.6913\n",
      "Iteration 70/1072, Loss: 7.8015\n",
      "Iteration 71/1072, Loss: 7.7299\n",
      "Iteration 72/1072, Loss: 7.7634\n",
      "Iteration 73/1072, Loss: 7.7630\n",
      "Iteration 74/1072, Loss: 7.8205\n",
      "Iteration 75/1072, Loss: 7.7884\n",
      "Iteration 76/1072, Loss: 7.8666\n",
      "Iteration 77/1072, Loss: 7.7372\n",
      "Iteration 78/1072, Loss: 7.7706\n",
      "Iteration 79/1072, Loss: 7.8071\n",
      "Iteration 80/1072, Loss: 7.7267\n",
      "Iteration 81/1072, Loss: 7.7174\n",
      "Iteration 82/1072, Loss: 7.7814\n",
      "Iteration 83/1072, Loss: 7.6984\n",
      "Iteration 84/1072, Loss: 7.8468\n",
      "Iteration 85/1072, Loss: 7.7358\n",
      "Iteration 86/1072, Loss: 7.6494\n",
      "Iteration 87/1072, Loss: 7.7936\n",
      "Iteration 88/1072, Loss: 7.8314\n",
      "Iteration 89/1072, Loss: 7.8130\n",
      "Iteration 90/1072, Loss: 7.7842\n",
      "Iteration 91/1072, Loss: 7.6664\n",
      "Iteration 92/1072, Loss: 7.7358\n",
      "Iteration 93/1072, Loss: 7.7254\n",
      "Iteration 94/1072, Loss: 7.7865\n",
      "Iteration 95/1072, Loss: 7.7423\n",
      "Iteration 96/1072, Loss: 7.7494\n",
      "Iteration 97/1072, Loss: 7.7568\n",
      "Iteration 98/1072, Loss: 7.7032\n",
      "Iteration 99/1072, Loss: 7.7539\n",
      "Iteration 100/1072, Loss: 7.8037\n",
      "Iteration 101/1072, Loss: 7.7683\n",
      "Iteration 102/1072, Loss: 7.8262\n",
      "Iteration 103/1072, Loss: 7.8238\n",
      "Iteration 104/1072, Loss: 7.7958\n",
      "Iteration 105/1072, Loss: 7.8210\n",
      "Iteration 106/1072, Loss: 7.8250\n",
      "Iteration 107/1072, Loss: 7.7328\n",
      "Iteration 108/1072, Loss: 7.8121\n",
      "Iteration 109/1072, Loss: 7.7804\n",
      "Iteration 110/1072, Loss: 7.7811\n",
      "Iteration 111/1072, Loss: 7.7722\n",
      "Iteration 112/1072, Loss: 7.7981\n",
      "Iteration 113/1072, Loss: 7.6863\n",
      "Iteration 114/1072, Loss: 7.7030\n",
      "Iteration 115/1072, Loss: 7.7853\n",
      "Iteration 116/1072, Loss: 7.7340\n",
      "Iteration 117/1072, Loss: 7.7246\n",
      "Iteration 118/1072, Loss: 7.7326\n",
      "Iteration 119/1072, Loss: 7.7561\n",
      "Iteration 120/1072, Loss: 7.8192\n",
      "Iteration 121/1072, Loss: 7.8363\n",
      "Iteration 122/1072, Loss: 7.7529\n",
      "Iteration 123/1072, Loss: 7.7377\n",
      "Iteration 124/1072, Loss: 7.7407\n",
      "Iteration 125/1072, Loss: 7.7219\n",
      "Iteration 126/1072, Loss: 7.7479\n",
      "Iteration 127/1072, Loss: 7.8388\n",
      "Iteration 128/1072, Loss: 7.7546\n",
      "Iteration 129/1072, Loss: 7.7353\n",
      "Iteration 130/1072, Loss: 7.7626\n",
      "Iteration 131/1072, Loss: 7.6927\n",
      "Iteration 132/1072, Loss: 7.7304\n",
      "Iteration 175/1072, Loss: 7.7713\n",
      "Iteration 176/1072, Loss: 7.7557\n",
      "Iteration 177/1072, Loss: 7.8138\n",
      "Iteration 178/1072, Loss: 7.6473\n",
      "Iteration 179/1072, Loss: 7.7669\n",
      "Iteration 180/1072, Loss: 7.7456\n",
      "Iteration 181/1072, Loss: 7.7696\n",
      "Iteration 182/1072, Loss: 7.7629\n",
      "Iteration 183/1072, Loss: 7.8036\n",
      "Iteration 184/1072, Loss: 7.7511\n",
      "Iteration 185/1072, Loss: 7.8309\n",
      "Iteration 186/1072, Loss: 7.7634\n",
      "Iteration 187/1072, Loss: 7.7385\n",
      "Iteration 188/1072, Loss: 7.7307\n",
      "Iteration 189/1072, Loss: 7.6907\n",
      "Iteration 190/1072, Loss: 7.8177\n",
      "Iteration 191/1072, Loss: 7.7364\n",
      "Iteration 192/1072, Loss: 7.6868\n",
      "Iteration 193/1072, Loss: 7.8579\n",
      "Iteration 194/1072, Loss: 7.7017\n",
      "Iteration 195/1072, Loss: 7.7614\n",
      "Iteration 196/1072, Loss: 7.7237\n",
      "Iteration 197/1072, Loss: 7.6969\n",
      "Iteration 198/1072, Loss: 7.7610\n",
      "Iteration 199/1072, Loss: 7.7591\n",
      "Iteration 200/1072, Loss: 7.6726\n",
      "Iteration 201/1072, Loss: 7.8079\n",
      "Iteration 202/1072, Loss: 7.7909\n",
      "Iteration 203/1072, Loss: 7.7852\n",
      "Iteration 204/1072, Loss: 7.7358\n",
      "Iteration 205/1072, Loss: 7.7176\n",
      "Iteration 206/1072, Loss: 7.8063\n",
      "Iteration 207/1072, Loss: 7.8051\n",
      "Iteration 208/1072, Loss: 7.7396\n",
      "Iteration 209/1072, Loss: 7.8357\n",
      "Iteration 210/1072, Loss: 7.7509\n",
      "Iteration 211/1072, Loss: 7.7702\n",
      "Iteration 212/1072, Loss: 7.8064\n",
      "Iteration 213/1072, Loss: 7.6797\n",
      "Iteration 214/1072, Loss: 7.7757\n",
      "Iteration 215/1072, Loss: 7.7956\n",
      "Iteration 216/1072, Loss: 7.8374\n",
      "Iteration 217/1072, Loss: 7.7115\n",
      "Iteration 218/1072, Loss: 7.7480\n",
      "Iteration 219/1072, Loss: 7.7119\n",
      "Iteration 220/1072, Loss: 7.7864\n",
      "Iteration 221/1072, Loss: 7.6895\n",
      "Iteration 222/1072, Loss: 7.7558\n",
      "Iteration 223/1072, Loss: 7.7832\n",
      "Iteration 224/1072, Loss: 7.7877\n",
      "Iteration 225/1072, Loss: 7.7596\n",
      "Iteration 226/1072, Loss: 7.7386\n",
      "Iteration 227/1072, Loss: 7.7838\n",
      "Iteration 228/1072, Loss: 7.8222\n",
      "Iteration 229/1072, Loss: 7.7843\n",
      "Iteration 230/1072, Loss: 7.7782\n",
      "Iteration 231/1072, Loss: 7.7775\n",
      "Iteration 232/1072, Loss: 7.8130\n",
      "Iteration 233/1072, Loss: 7.7939\n",
      "Iteration 234/1072, Loss: 7.7836\n",
      "Iteration 235/1072, Loss: 7.6610\n",
      "Iteration 236/1072, Loss: 7.7682\n",
      "Iteration 237/1072, Loss: 7.7537\n",
      "Iteration 238/1072, Loss: 7.7988\n",
      "Iteration 239/1072, Loss: 7.8044\n",
      "Iteration 240/1072, Loss: 7.8110\n",
      "Iteration 241/1072, Loss: 7.7267\n",
      "Iteration 242/1072, Loss: 7.6980\n",
      "Iteration 243/1072, Loss: 7.7021\n",
      "Iteration 244/1072, Loss: 7.7071\n",
      "Iteration 245/1072, Loss: 7.8119\n",
      "Iteration 246/1072, Loss: 7.8024\n",
      "Iteration 247/1072, Loss: 7.7565\n",
      "Iteration 248/1072, Loss: 7.6557\n",
      "Iteration 249/1072, Loss: 7.7447\n",
      "Iteration 250/1072, Loss: 7.7614\n",
      "Iteration 251/1072, Loss: 7.8096\n",
      "Iteration 252/1072, Loss: 7.7546\n",
      "Iteration 253/1072, Loss: 7.8126\n",
      "Iteration 254/1072, Loss: 7.7584\n",
      "Iteration 255/1072, Loss: 7.7351\n",
      "Iteration 256/1072, Loss: 7.8668\n",
      "Iteration 257/1072, Loss: 7.7417\n",
      "Iteration 258/1072, Loss: 7.6987\n",
      "Iteration 259/1072, Loss: 7.7728\n",
      "Iteration 260/1072, Loss: 7.7726\n",
      "Iteration 261/1072, Loss: 7.7094\n",
      "Iteration 262/1072, Loss: 7.7255\n",
      "Iteration 263/1072, Loss: 7.8009\n",
      "Iteration 264/1072, Loss: 7.7348\n",
      "Iteration 265/1072, Loss: 7.7674\n",
      "Iteration 266/1072, Loss: 7.8037\n",
      "Iteration 267/1072, Loss: 7.7734\n",
      "Iteration 268/1072, Loss: 7.7490\n",
      "Iteration 269/1072, Loss: 7.7143\n",
      "Iteration 270/1072, Loss: 7.7336\n",
      "Iteration 271/1072, Loss: 7.7739\n",
      "Iteration 272/1072, Loss: 7.7079\n",
      "Iteration 273/1072, Loss: 7.8032\n",
      "Iteration 274/1072, Loss: 7.6112\n",
      "Iteration 275/1072, Loss: 7.7514\n",
      "Iteration 276/1072, Loss: 7.8680\n",
      "Iteration 277/1072, Loss: 7.6658\n",
      "Iteration 278/1072, Loss: 7.7987\n",
      "Iteration 279/1072, Loss: 7.7737\n",
      "Iteration 280/1072, Loss: 7.8128\n",
      "Iteration 281/1072, Loss: 7.7937\n",
      "Iteration 282/1072, Loss: 7.6275\n",
      "Iteration 283/1072, Loss: 7.7019\n",
      "Iteration 284/1072, Loss: 7.6920\n",
      "Iteration 285/1072, Loss: 7.7531\n",
      "Iteration 286/1072, Loss: 7.7528\n",
      "Iteration 287/1072, Loss: 7.7832\n",
      "Iteration 288/1072, Loss: 7.7577\n",
      "Iteration 289/1072, Loss: 7.6915\n",
      "Iteration 290/1072, Loss: 7.7610\n",
      "Iteration 291/1072, Loss: 7.7976\n",
      "Iteration 292/1072, Loss: 7.7075\n",
      "Iteration 293/1072, Loss: 7.7107\n",
      "Iteration 294/1072, Loss: 7.7345\n",
      "Iteration 295/1072, Loss: 7.7496\n",
      "Iteration 296/1072, Loss: 7.6970\n",
      "Iteration 297/1072, Loss: 7.7495\n",
      "Iteration 298/1072, Loss: 7.7341\n",
      "Iteration 299/1072, Loss: 7.7663\n",
      "Iteration 300/1072, Loss: 7.8094\n",
      "Iteration 301/1072, Loss: 7.8058\n",
      "Iteration 302/1072, Loss: 7.7125\n",
      "Iteration 303/1072, Loss: 7.6461\n",
      "Iteration 304/1072, Loss: 7.7944\n",
      "Iteration 305/1072, Loss: 7.7767\n",
      "Iteration 306/1072, Loss: 7.7561\n",
      "Iteration 307/1072, Loss: 7.7959\n",
      "Iteration 308/1072, Loss: 7.7418\n",
      "Iteration 309/1072, Loss: 7.7597\n",
      "Iteration 310/1072, Loss: 7.7140\n",
      "Iteration 311/1072, Loss: 7.7839\n",
      "Iteration 312/1072, Loss: 7.8123\n",
      "Iteration 313/1072, Loss: 7.7784\n",
      "Iteration 314/1072, Loss: 7.8448\n",
      "Iteration 315/1072, Loss: 7.7962\n",
      "Iteration 316/1072, Loss: 7.7908\n",
      "Iteration 317/1072, Loss: 7.7356\n",
      "Iteration 318/1072, Loss: 7.8324\n",
      "Iteration 319/1072, Loss: 7.7427\n",
      "Iteration 320/1072, Loss: 7.7674\n",
      "Iteration 321/1072, Loss: 7.7422\n",
      "Iteration 322/1072, Loss: 7.7582\n",
      "Iteration 323/1072, Loss: 7.7155\n",
      "Iteration 324/1072, Loss: 7.6383\n",
      "Iteration 325/1072, Loss: 7.7788\n",
      "Iteration 326/1072, Loss: 7.7625\n",
      "Iteration 327/1072, Loss: 7.7525\n",
      "Iteration 328/1072, Loss: 7.7853\n",
      "Iteration 329/1072, Loss: 7.7173\n",
      "Iteration 330/1072, Loss: 7.8354\n",
      "Iteration 331/1072, Loss: 7.7172\n",
      "Iteration 332/1072, Loss: 7.7534\n",
      "Iteration 333/1072, Loss: 7.6823\n",
      "Iteration 334/1072, Loss: 7.7457\n",
      "Iteration 335/1072, Loss: 7.6997\n",
      "Iteration 336/1072, Loss: 7.7251\n",
      "Iteration 337/1072, Loss: 7.7378\n",
      "Iteration 338/1072, Loss: 7.7209\n",
      "Iteration 339/1072, Loss: 7.7282\n",
      "Iteration 340/1072, Loss: 7.7378\n",
      "Iteration 341/1072, Loss: 7.7082\n",
      "Iteration 342/1072, Loss: 7.8240\n",
      "Iteration 343/1072, Loss: 7.7503\n",
      "Iteration 344/1072, Loss: 7.7486\n",
      "Iteration 345/1072, Loss: 7.6408\n",
      "Iteration 346/1072, Loss: 7.7198\n",
      "Iteration 347/1072, Loss: 7.7539\n",
      "Iteration 348/1072, Loss: 7.8057\n",
      "Iteration 349/1072, Loss: 7.7999\n",
      "Iteration 350/1072, Loss: 7.7211\n",
      "Iteration 351/1072, Loss: 7.8160\n",
      "Iteration 352/1072, Loss: 7.7390\n",
      "Iteration 353/1072, Loss: 7.6882\n",
      "Iteration 354/1072, Loss: 7.7409\n",
      "Iteration 355/1072, Loss: 7.7992\n",
      "Iteration 356/1072, Loss: 7.8457\n",
      "Iteration 357/1072, Loss: 7.8337\n",
      "Iteration 358/1072, Loss: 7.8047\n",
      "Iteration 359/1072, Loss: 7.7421\n",
      "Iteration 360/1072, Loss: 7.8119\n",
      "Iteration 361/1072, Loss: 7.7500\n",
      "Iteration 362/1072, Loss: 7.7496\n",
      "Iteration 363/1072, Loss: 7.7458\n",
      "Iteration 364/1072, Loss: 7.7424\n",
      "Iteration 365/1072, Loss: 7.7469\n",
      "Iteration 366/1072, Loss: 7.7096\n",
      "Iteration 367/1072, Loss: 7.7264\n",
      "Iteration 368/1072, Loss: 7.7145\n",
      "Iteration 369/1072, Loss: 7.7820\n",
      "Iteration 370/1072, Loss: 7.7378\n",
      "Iteration 371/1072, Loss: 7.7858\n",
      "Iteration 372/1072, Loss: 7.8147\n",
      "Iteration 373/1072, Loss: 7.7932\n",
      "Iteration 374/1072, Loss: 7.7638\n",
      "Iteration 375/1072, Loss: 7.6902\n",
      "Iteration 376/1072, Loss: 7.7660\n",
      "Iteration 377/1072, Loss: 7.8267\n",
      "Iteration 378/1072, Loss: 7.7240\n",
      "Iteration 379/1072, Loss: 7.7504\n",
      "Iteration 380/1072, Loss: 7.6953\n",
      "Iteration 381/1072, Loss: 7.7966\n",
      "Iteration 382/1072, Loss: 7.6828\n",
      "Iteration 383/1072, Loss: 7.7855\n",
      "Iteration 384/1072, Loss: 7.6602\n",
      "Iteration 385/1072, Loss: 7.8257\n",
      "Iteration 386/1072, Loss: 7.8425\n",
      "Iteration 387/1072, Loss: 7.7518\n",
      "Iteration 388/1072, Loss: 7.8045\n",
      "Iteration 389/1072, Loss: 7.7056\n",
      "Iteration 390/1072, Loss: 7.7078\n",
      "Iteration 391/1072, Loss: 7.7164\n",
      "Iteration 392/1072, Loss: 7.8257\n",
      "Iteration 393/1072, Loss: 7.7752\n",
      "Iteration 394/1072, Loss: 7.8700\n",
      "Iteration 395/1072, Loss: 7.6559\n",
      "Iteration 396/1072, Loss: 7.7634\n",
      "Iteration 397/1072, Loss: 7.7239\n",
      "Iteration 398/1072, Loss: 7.8035\n",
      "Iteration 399/1072, Loss: 7.7164\n",
      "Iteration 400/1072, Loss: 7.8036\n",
      "Iteration 401/1072, Loss: 7.7000\n",
      "Iteration 402/1072, Loss: 7.7859\n",
      "Iteration 403/1072, Loss: 7.7924\n",
      "Iteration 404/1072, Loss: 7.8006\n",
      "Iteration 405/1072, Loss: 7.6777\n",
      "Iteration 406/1072, Loss: 7.6553\n",
      "Iteration 407/1072, Loss: 7.8094\n",
      "Iteration 408/1072, Loss: 7.7573\n",
      "Iteration 409/1072, Loss: 7.7206\n",
      "Iteration 410/1072, Loss: 7.7044\n",
      "Iteration 411/1072, Loss: 7.7720\n",
      "Iteration 412/1072, Loss: 7.8398\n",
      "Iteration 413/1072, Loss: 7.7836\n",
      "Iteration 414/1072, Loss: 7.6823\n",
      "Iteration 415/1072, Loss: 7.6629\n",
      "Iteration 416/1072, Loss: 7.8199\n",
      "Iteration 417/1072, Loss: 7.7210\n",
      "Iteration 418/1072, Loss: 7.7908\n",
      "Iteration 419/1072, Loss: 7.7549\n",
      "Iteration 420/1072, Loss: 7.8266\n",
      "Iteration 421/1072, Loss: 7.7295\n",
      "Iteration 422/1072, Loss: 7.7477\n",
      "Iteration 423/1072, Loss: 7.7604\n",
      "Iteration 424/1072, Loss: 7.6805\n",
      "Iteration 425/1072, Loss: 7.7139\n",
      "Iteration 426/1072, Loss: 7.7621\n",
      "Iteration 427/1072, Loss: 7.7616\n",
      "Iteration 428/1072, Loss: 7.6959\n",
      "Iteration 429/1072, Loss: 7.7662\n",
      "Iteration 430/1072, Loss: 7.6949\n",
      "Iteration 431/1072, Loss: 7.7298\n",
      "Iteration 432/1072, Loss: 7.7469\n",
      "Iteration 433/1072, Loss: 7.7104\n",
      "Iteration 434/1072, Loss: 7.8153\n",
      "Iteration 435/1072, Loss: 7.8348\n",
      "Iteration 436/1072, Loss: 7.7448\n",
      "Iteration 437/1072, Loss: 7.7041\n",
      "Iteration 438/1072, Loss: 7.7226\n",
      "Iteration 439/1072, Loss: 7.8105\n",
      "Iteration 440/1072, Loss: 7.7099\n",
      "Iteration 441/1072, Loss: 7.7813\n",
      "Iteration 442/1072, Loss: 7.7637\n",
      "Iteration 443/1072, Loss: 7.7600\n",
      "Iteration 444/1072, Loss: 7.7771\n",
      "Iteration 445/1072, Loss: 7.8097\n",
      "Iteration 446/1072, Loss: 7.7197\n",
      "Iteration 447/1072, Loss: 7.6461\n",
      "Iteration 448/1072, Loss: 7.6722\n",
      "Iteration 449/1072, Loss: 7.8302\n",
      "Iteration 450/1072, Loss: 7.8315\n",
      "Iteration 451/1072, Loss: 7.7498\n",
      "Iteration 452/1072, Loss: 7.6946\n",
      "Iteration 453/1072, Loss: 7.8272\n",
      "Iteration 454/1072, Loss: 7.7097\n",
      "Iteration 455/1072, Loss: 7.6899\n",
      "Iteration 456/1072, Loss: 7.8072\n",
      "Iteration 457/1072, Loss: 7.7574\n",
      "Iteration 458/1072, Loss: 7.8046\n",
      "Iteration 459/1072, Loss: 7.8254\n",
      "Iteration 460/1072, Loss: 7.8367\n",
      "Iteration 461/1072, Loss: 7.7251\n",
      "Iteration 462/1072, Loss: 7.7814\n",
      "Iteration 463/1072, Loss: 7.6817\n",
      "Iteration 464/1072, Loss: 7.7077\n",
      "Iteration 465/1072, Loss: 7.7439\n",
      "Iteration 466/1072, Loss: 7.6880\n",
      "Iteration 467/1072, Loss: 7.6960\n",
      "Iteration 468/1072, Loss: 7.7563\n",
      "Iteration 469/1072, Loss: 7.7257\n",
      "Iteration 470/1072, Loss: 7.7935\n",
      "Iteration 471/1072, Loss: 7.7031\n",
      "Iteration 472/1072, Loss: 7.7227\n",
      "Iteration 473/1072, Loss: 7.7294\n",
      "Iteration 474/1072, Loss: 7.8082\n",
      "Iteration 475/1072, Loss: 7.7255\n",
      "Iteration 476/1072, Loss: 7.7539\n",
      "Iteration 477/1072, Loss: 7.8324\n",
      "Iteration 478/1072, Loss: 7.7419\n",
      "Iteration 479/1072, Loss: 7.7667\n",
      "Iteration 480/1072, Loss: 7.7546\n",
      "Iteration 481/1072, Loss: 7.8289\n",
      "Iteration 482/1072, Loss: 7.8122\n",
      "Iteration 483/1072, Loss: 7.7166\n",
      "Iteration 484/1072, Loss: 7.6343\n",
      "Iteration 485/1072, Loss: 7.7474\n",
      "Iteration 486/1072, Loss: 7.7101\n",
      "Iteration 487/1072, Loss: 7.7074\n",
      "Iteration 488/1072, Loss: 7.7405\n",
      "Iteration 489/1072, Loss: 7.7710\n",
      "Iteration 490/1072, Loss: 7.7646\n",
      "Iteration 491/1072, Loss: 7.7777\n",
      "Iteration 492/1072, Loss: 7.7559\n",
      "Iteration 493/1072, Loss: 7.7235\n",
      "Iteration 494/1072, Loss: 7.8150\n",
      "Iteration 495/1072, Loss: 7.7289\n",
      "Iteration 496/1072, Loss: 7.6440\n",
      "Iteration 497/1072, Loss: 7.7617\n",
      "Iteration 498/1072, Loss: 7.7286\n",
      "Iteration 499/1072, Loss: 7.8016\n",
      "Iteration 500/1072, Loss: 7.6568\n",
      "Iteration 501/1072, Loss: 7.7507\n",
      "Iteration 502/1072, Loss: 7.7382\n",
      "Iteration 503/1072, Loss: 7.7316\n",
      "Iteration 504/1072, Loss: 7.7104\n",
      "Iteration 505/1072, Loss: 7.7655\n",
      "Iteration 506/1072, Loss: 7.8213\n",
      "Iteration 507/1072, Loss: 7.7418\n",
      "Iteration 508/1072, Loss: 7.7101\n",
      "Iteration 509/1072, Loss: 7.7857\n",
      "Iteration 510/1072, Loss: 7.7326\n",
      "Iteration 511/1072, Loss: 7.7552\n",
      "Iteration 512/1072, Loss: 7.7520\n",
      "Iteration 513/1072, Loss: 7.7143\n",
      "Iteration 514/1072, Loss: 7.7630\n",
      "Iteration 515/1072, Loss: 7.7936\n",
      "Iteration 516/1072, Loss: 7.7693\n",
      "Iteration 517/1072, Loss: 7.6327\n",
      "Iteration 518/1072, Loss: 7.6785\n",
      "Iteration 519/1072, Loss: 7.6870\n",
      "Iteration 520/1072, Loss: 7.7811\n",
      "Iteration 521/1072, Loss: 7.8032\n",
      "Iteration 522/1072, Loss: 7.8144\n",
      "Iteration 523/1072, Loss: 7.7811\n",
      "Iteration 524/1072, Loss: 7.7714\n",
      "Iteration 525/1072, Loss: 7.7816\n",
      "Iteration 526/1072, Loss: 7.7975\n",
      "Iteration 527/1072, Loss: 7.7675\n",
      "Iteration 528/1072, Loss: 7.8296\n",
      "Iteration 529/1072, Loss: 7.7712\n",
      "Iteration 530/1072, Loss: 7.8098\n",
      "Iteration 531/1072, Loss: 7.7959\n",
      "Iteration 532/1072, Loss: 7.7352\n",
      "Iteration 533/1072, Loss: 7.7733\n",
      "Iteration 534/1072, Loss: 7.7271\n",
      "Iteration 535/1072, Loss: 7.8483\n",
      "Iteration 536/1072, Loss: 7.6746\n",
      "Iteration 537/1072, Loss: 7.8312\n",
      "Iteration 538/1072, Loss: 7.7494\n",
      "Iteration 539/1072, Loss: 7.7412\n",
      "Iteration 540/1072, Loss: 7.7546\n",
      "Iteration 541/1072, Loss: 7.6555\n",
      "Iteration 542/1072, Loss: 7.8505\n",
      "Iteration 543/1072, Loss: 7.8609\n",
      "Iteration 544/1072, Loss: 7.6838\n",
      "Iteration 545/1072, Loss: 7.6327\n",
      "Iteration 546/1072, Loss: 7.7480\n",
      "Iteration 547/1072, Loss: 7.7705\n",
      "Iteration 548/1072, Loss: 7.7199\n",
      "Iteration 549/1072, Loss: 7.7754\n",
      "Iteration 550/1072, Loss: 7.7322\n",
      "Iteration 551/1072, Loss: 7.8179\n",
      "Iteration 552/1072, Loss: 7.7661\n",
      "Iteration 553/1072, Loss: 7.7445\n",
      "Iteration 554/1072, Loss: 7.7273\n",
      "Iteration 555/1072, Loss: 7.7506\n",
      "Iteration 556/1072, Loss: 7.7840\n",
      "Iteration 557/1072, Loss: 7.7341\n",
      "Iteration 558/1072, Loss: 7.7100\n",
      "Iteration 559/1072, Loss: 7.7992\n",
      "Iteration 560/1072, Loss: 7.6531\n",
      "Iteration 561/1072, Loss: 7.6740\n",
      "Iteration 562/1072, Loss: 7.7837\n",
      "Iteration 563/1072, Loss: 7.6978\n",
      "Iteration 564/1072, Loss: 7.7903\n",
      "Iteration 565/1072, Loss: 7.6978\n",
      "Iteration 566/1072, Loss: 7.8239\n",
      "Iteration 567/1072, Loss: 7.7046\n",
      "Iteration 568/1072, Loss: 7.7474\n",
      "Iteration 569/1072, Loss: 7.7325\n",
      "Iteration 570/1072, Loss: 7.7800\n",
      "Iteration 571/1072, Loss: 7.8206\n",
      "Iteration 572/1072, Loss: 7.6891\n",
      "Iteration 573/1072, Loss: 7.7252\n",
      "Iteration 574/1072, Loss: 7.7682\n",
      "Iteration 575/1072, Loss: 7.7589\n",
      "Iteration 576/1072, Loss: 7.7516\n",
      "Iteration 577/1072, Loss: 7.7990\n",
      "Iteration 578/1072, Loss: 7.6374\n",
      "Iteration 579/1072, Loss: 7.7232\n",
      "Iteration 580/1072, Loss: 7.7077\n",
      "Iteration 581/1072, Loss: 7.8388\n",
      "Iteration 582/1072, Loss: 7.7025\n",
      "Iteration 583/1072, Loss: 7.7052\n",
      "Iteration 584/1072, Loss: 7.7833\n",
      "Iteration 585/1072, Loss: 7.7333\n",
      "Iteration 586/1072, Loss: 7.6906\n",
      "Iteration 587/1072, Loss: 7.7544\n",
      "Iteration 588/1072, Loss: 7.7406\n",
      "Iteration 589/1072, Loss: 7.7552\n",
      "Iteration 590/1072, Loss: 7.7004\n",
      "Iteration 591/1072, Loss: 7.7402\n",
      "Iteration 592/1072, Loss: 7.7446\n",
      "Iteration 593/1072, Loss: 7.6402\n",
      "Iteration 594/1072, Loss: 7.6725\n",
      "Iteration 595/1072, Loss: 7.7317\n",
      "Iteration 596/1072, Loss: 7.7051\n",
      "Iteration 597/1072, Loss: 7.7633\n",
      "Iteration 598/1072, Loss: 7.7914\n",
      "Iteration 599/1072, Loss: 7.6786\n",
      "Iteration 600/1072, Loss: 7.7580\n",
      "Iteration 601/1072, Loss: 7.7060\n",
      "Iteration 602/1072, Loss: 7.7836\n",
      "Iteration 603/1072, Loss: 7.8250\n",
      "Iteration 604/1072, Loss: 7.7931\n",
      "Iteration 605/1072, Loss: 7.8513\n",
      "Iteration 606/1072, Loss: 7.7225\n",
      "Iteration 607/1072, Loss: 7.8309\n",
      "Iteration 608/1072, Loss: 7.7640\n",
      "Iteration 609/1072, Loss: 7.7413\n",
      "Iteration 610/1072, Loss: 7.7773\n",
      "Iteration 611/1072, Loss: 7.6859\n",
      "Iteration 612/1072, Loss: 7.7910\n",
      "Iteration 613/1072, Loss: 7.7898\n",
      "Iteration 614/1072, Loss: 7.6829\n",
      "Iteration 615/1072, Loss: 7.7344\n",
      "Iteration 616/1072, Loss: 7.7539\n",
      "Iteration 617/1072, Loss: 7.7344\n",
      "Iteration 618/1072, Loss: 7.6926\n",
      "Iteration 619/1072, Loss: 7.7410\n",
      "Iteration 620/1072, Loss: 7.6981\n",
      "Iteration 621/1072, Loss: 7.7848\n",
      "Iteration 622/1072, Loss: 7.6762\n",
      "Iteration 623/1072, Loss: 7.6519\n",
      "Iteration 624/1072, Loss: 7.7720\n",
      "Iteration 625/1072, Loss: 7.7525\n",
      "Iteration 626/1072, Loss: 7.8079\n",
      "Iteration 627/1072, Loss: 7.7266\n",
      "Iteration 628/1072, Loss: 7.7141\n",
      "Iteration 629/1072, Loss: 7.7707\n",
      "Iteration 630/1072, Loss: 7.6768\n",
      "Iteration 631/1072, Loss: 7.7482\n",
      "Iteration 632/1072, Loss: 7.7733\n",
      "Iteration 633/1072, Loss: 7.7202\n",
      "Iteration 634/1072, Loss: 7.6948\n",
      "Iteration 635/1072, Loss: 7.7674\n",
      "Iteration 636/1072, Loss: 7.7146\n",
      "Iteration 637/1072, Loss: 7.7625\n",
      "Iteration 638/1072, Loss: 7.7672\n",
      "Iteration 639/1072, Loss: 7.8197\n",
      "Iteration 640/1072, Loss: 7.7218\n",
      "Iteration 641/1072, Loss: 7.6614\n",
      "Iteration 642/1072, Loss: 7.7441\n",
      "Iteration 643/1072, Loss: 7.6683\n",
      "Iteration 644/1072, Loss: 7.7783\n",
      "Iteration 645/1072, Loss: 7.7880\n",
      "Iteration 646/1072, Loss: 7.7823\n",
      "Iteration 647/1072, Loss: 7.7754\n",
      "Iteration 648/1072, Loss: 7.8140\n",
      "Iteration 649/1072, Loss: 7.6933\n",
      "Iteration 650/1072, Loss: 7.6694\n",
      "Iteration 651/1072, Loss: 7.8361\n",
      "Iteration 652/1072, Loss: 7.5858\n",
      "Iteration 653/1072, Loss: 7.6841\n",
      "Iteration 654/1072, Loss: 7.7354\n",
      "Iteration 655/1072, Loss: 7.8040\n",
      "Iteration 656/1072, Loss: 7.6588\n",
      "Iteration 657/1072, Loss: 7.7941\n",
      "Iteration 658/1072, Loss: 7.7577\n",
      "Iteration 659/1072, Loss: 7.7850\n",
      "Iteration 660/1072, Loss: 7.7162\n",
      "Iteration 661/1072, Loss: 7.7057\n",
      "Iteration 662/1072, Loss: 7.7912\n",
      "Iteration 663/1072, Loss: 7.7203\n",
      "Iteration 664/1072, Loss: 7.7825\n",
      "Iteration 665/1072, Loss: 7.7374\n",
      "Iteration 666/1072, Loss: 7.7671\n",
      "Iteration 667/1072, Loss: 7.6556\n",
      "Iteration 668/1072, Loss: 7.6652\n",
      "Iteration 669/1072, Loss: 7.7302\n",
      "Iteration 670/1072, Loss: 7.7575\n",
      "Iteration 671/1072, Loss: 7.8422\n",
      "Iteration 672/1072, Loss: 7.8771\n",
      "Iteration 673/1072, Loss: 7.6654\n",
      "Iteration 674/1072, Loss: 7.6605\n",
      "Iteration 675/1072, Loss: 7.7963\n",
      "Iteration 676/1072, Loss: 7.7976\n",
      "Iteration 677/1072, Loss: 7.7446\n",
      "Iteration 678/1072, Loss: 7.6601\n",
      "Iteration 679/1072, Loss: 7.7481\n",
      "Iteration 680/1072, Loss: 7.7907\n",
      "Iteration 681/1072, Loss: 7.6980\n",
      "Iteration 682/1072, Loss: 7.7464\n",
      "Iteration 683/1072, Loss: 7.7617\n",
      "Iteration 684/1072, Loss: 7.7584\n",
      "Iteration 685/1072, Loss: 7.6089\n",
      "Iteration 686/1072, Loss: 7.5884\n",
      "Iteration 687/1072, Loss: 7.7816\n",
      "Iteration 688/1072, Loss: 7.7136\n",
      "Iteration 689/1072, Loss: 7.6988\n",
      "Iteration 690/1072, Loss: 7.7976\n",
      "Iteration 691/1072, Loss: 7.7137\n",
      "Iteration 692/1072, Loss: 7.6841\n",
      "Iteration 693/1072, Loss: 7.6593\n",
      "Iteration 694/1072, Loss: 7.6960\n",
      "Iteration 695/1072, Loss: 7.6794\n",
      "Iteration 696/1072, Loss: 7.7362\n",
      "Iteration 697/1072, Loss: 7.7699\n",
      "Iteration 698/1072, Loss: 7.6903\n",
      "Iteration 699/1072, Loss: 7.7380\n",
      "Iteration 700/1072, Loss: 7.7522\n",
      "Iteration 701/1072, Loss: 7.7263\n",
      "Iteration 702/1072, Loss: 7.6564\n",
      "Iteration 703/1072, Loss: 7.7789\n",
      "Iteration 704/1072, Loss: 7.7084\n",
      "Iteration 705/1072, Loss: 7.7076\n",
      "Iteration 706/1072, Loss: 7.8106\n",
      "Iteration 707/1072, Loss: 7.7639\n",
      "Iteration 708/1072, Loss: 7.5865\n",
      "Iteration 709/1072, Loss: 7.7506\n",
      "Iteration 710/1072, Loss: 7.6633\n",
      "Iteration 711/1072, Loss: 7.7856\n",
      "Iteration 712/1072, Loss: 7.7115\n",
      "Iteration 713/1072, Loss: 7.7679\n",
      "Iteration 714/1072, Loss: 7.7373\n",
      "Iteration 715/1072, Loss: 7.7341\n",
      "Iteration 716/1072, Loss: 7.7691\n",
      "Iteration 717/1072, Loss: 7.6871\n",
      "Iteration 718/1072, Loss: 7.7308\n",
      "Iteration 719/1072, Loss: 7.7769\n",
      "Iteration 720/1072, Loss: 7.6881\n",
      "Iteration 721/1072, Loss: 7.7234\n",
      "Iteration 722/1072, Loss: 7.6824\n",
      "Iteration 723/1072, Loss: 7.8037\n",
      "Iteration 724/1072, Loss: 7.6012\n",
      "Iteration 725/1072, Loss: 7.7191\n",
      "Iteration 726/1072, Loss: 7.6631\n",
      "Iteration 727/1072, Loss: 7.7203\n",
      "Iteration 728/1072, Loss: 7.7118\n",
      "Iteration 729/1072, Loss: 7.6434\n",
      "Iteration 730/1072, Loss: 7.7200\n",
      "Iteration 731/1072, Loss: 7.6646\n",
      "Iteration 732/1072, Loss: 7.8772\n",
      "Iteration 733/1072, Loss: 7.7631\n",
      "Iteration 734/1072, Loss: 7.6938\n",
      "Iteration 735/1072, Loss: 7.5851\n",
      "Iteration 736/1072, Loss: 7.7390\n",
      "Iteration 737/1072, Loss: 7.6960\n",
      "Iteration 738/1072, Loss: 7.7178\n",
      "Iteration 739/1072, Loss: 7.7069\n",
      "Iteration 740/1072, Loss: 7.7443\n",
      "Iteration 741/1072, Loss: 7.7607\n",
      "Iteration 742/1072, Loss: 7.7425\n",
      "Iteration 743/1072, Loss: 7.7108\n",
      "Iteration 744/1072, Loss: 7.7504\n",
      "Iteration 745/1072, Loss: 7.7004\n",
      "Iteration 746/1072, Loss: 7.7818\n",
      "Iteration 747/1072, Loss: 7.7310\n",
      "Iteration 748/1072, Loss: 7.7590\n",
      "Iteration 749/1072, Loss: 7.6840\n",
      "Iteration 750/1072, Loss: 7.7859\n",
      "Iteration 751/1072, Loss: 7.7477\n",
      "Iteration 752/1072, Loss: 7.6925\n",
      "Iteration 753/1072, Loss: 7.7253\n",
      "Iteration 754/1072, Loss: 7.7285\n",
      "Iteration 755/1072, Loss: 7.7344\n",
      "Iteration 756/1072, Loss: 7.6913\n",
      "Iteration 757/1072, Loss: 7.8162\n",
      "Iteration 758/1072, Loss: 7.7722\n",
      "Iteration 759/1072, Loss: 7.7913\n",
      "Iteration 760/1072, Loss: 7.6733\n",
      "Iteration 761/1072, Loss: 7.7677\n",
      "Iteration 762/1072, Loss: 7.6969\n",
      "Iteration 763/1072, Loss: 7.7308\n",
      "Iteration 764/1072, Loss: 7.8083\n",
      "Iteration 765/1072, Loss: 7.7431\n",
      "Iteration 766/1072, Loss: 7.7755\n",
      "Iteration 767/1072, Loss: 7.8128\n",
      "Iteration 768/1072, Loss: 7.7515\n",
      "Iteration 769/1072, Loss: 7.7304\n",
      "Iteration 770/1072, Loss: 7.6634\n",
      "Iteration 771/1072, Loss: 7.7258\n",
      "Iteration 772/1072, Loss: 7.7309\n",
      "Iteration 773/1072, Loss: 7.7217\n",
      "Iteration 774/1072, Loss: 7.6933\n",
      "Iteration 775/1072, Loss: 7.8305\n",
      "Iteration 776/1072, Loss: 7.7860\n",
      "Iteration 777/1072, Loss: 7.7648\n",
      "Iteration 778/1072, Loss: 7.7390\n",
      "Iteration 779/1072, Loss: 7.7958\n",
      "Iteration 780/1072, Loss: 7.7673\n",
      "Iteration 781/1072, Loss: 7.7193\n",
      "Iteration 782/1072, Loss: 7.7642\n",
      "Iteration 783/1072, Loss: 7.7946\n",
      "Iteration 784/1072, Loss: 7.7558\n",
      "Iteration 785/1072, Loss: 7.6696\n",
      "Iteration 786/1072, Loss: 7.7264\n",
      "Iteration 787/1072, Loss: 7.7734\n",
      "Iteration 788/1072, Loss: 7.6833\n",
      "Iteration 789/1072, Loss: 7.8054\n",
      "Iteration 790/1072, Loss: 7.7185\n",
      "Iteration 791/1072, Loss: 7.7734\n",
      "Iteration 792/1072, Loss: 7.6348\n",
      "Iteration 793/1072, Loss: 7.7656\n",
      "Iteration 794/1072, Loss: 7.7196\n",
      "Iteration 795/1072, Loss: 7.7333\n",
      "Iteration 796/1072, Loss: 7.7152\n",
      "Iteration 797/1072, Loss: 7.7109\n",
      "Iteration 798/1072, Loss: 7.6913\n",
      "Iteration 799/1072, Loss: 7.7361\n",
      "Iteration 800/1072, Loss: 7.8315\n",
      "Iteration 801/1072, Loss: 7.7818\n",
      "Iteration 802/1072, Loss: 7.7837\n",
      "Iteration 803/1072, Loss: 7.8235\n",
      "Iteration 804/1072, Loss: 7.6821\n",
      "Iteration 805/1072, Loss: 7.7436\n",
      "Iteration 806/1072, Loss: 7.7055\n",
      "Iteration 807/1072, Loss: 7.6787\n",
      "Iteration 808/1072, Loss: 7.7421\n",
      "Iteration 809/1072, Loss: 7.7709\n",
      "Iteration 810/1072, Loss: 7.7675\n",
      "Iteration 811/1072, Loss: 7.6290\n",
      "Iteration 812/1072, Loss: 7.7315\n",
      "Iteration 813/1072, Loss: 7.6473\n",
      "Iteration 814/1072, Loss: 7.7961\n",
      "Iteration 815/1072, Loss: 7.8259\n",
      "Iteration 816/1072, Loss: 7.7859\n",
      "Iteration 817/1072, Loss: 7.6957\n",
      "Iteration 818/1072, Loss: 7.7485\n",
      "Iteration 819/1072, Loss: 7.8023\n",
      "Iteration 820/1072, Loss: 7.7343\n",
      "Iteration 821/1072, Loss: 7.6171\n",
      "Iteration 822/1072, Loss: 7.7153\n",
      "Iteration 823/1072, Loss: 7.6800\n",
      "Iteration 824/1072, Loss: 7.7140\n",
      "Iteration 825/1072, Loss: 7.6852\n",
      "Iteration 826/1072, Loss: 7.7215\n",
      "Iteration 827/1072, Loss: 7.7303\n",
      "Iteration 828/1072, Loss: 7.7542\n",
      "Iteration 829/1072, Loss: 7.6320\n",
      "Iteration 830/1072, Loss: 7.6846\n",
      "Iteration 831/1072, Loss: 7.7597\n",
      "Iteration 832/1072, Loss: 7.7692\n",
      "Iteration 833/1072, Loss: 7.6871\n",
      "Iteration 834/1072, Loss: 7.7931\n",
      "Iteration 835/1072, Loss: 7.7897\n",
      "Iteration 836/1072, Loss: 7.8011\n",
      "Iteration 837/1072, Loss: 7.7204\n",
      "Iteration 838/1072, Loss: 7.6680\n",
      "Iteration 839/1072, Loss: 7.7540\n",
      "Iteration 840/1072, Loss: 7.7703\n",
      "Iteration 841/1072, Loss: 7.7013\n",
      "Iteration 842/1072, Loss: 7.6633\n",
      "Iteration 843/1072, Loss: 7.7814\n",
      "Iteration 844/1072, Loss: 7.7736\n",
      "Iteration 845/1072, Loss: 7.7772\n",
      "Iteration 846/1072, Loss: 7.7755\n",
      "Iteration 847/1072, Loss: 7.5875\n",
      "Iteration 848/1072, Loss: 7.6967\n",
      "Iteration 849/1072, Loss: 7.7175\n",
      "Iteration 850/1072, Loss: 7.7333\n",
      "Iteration 851/1072, Loss: 7.6952\n",
      "Iteration 852/1072, Loss: 7.6727\n",
      "Iteration 853/1072, Loss: 7.7394\n",
      "Iteration 854/1072, Loss: 7.8221\n",
      "Iteration 855/1072, Loss: 7.7862\n",
      "Iteration 856/1072, Loss: 7.6613\n",
      "Iteration 857/1072, Loss: 7.7790\n",
      "Iteration 858/1072, Loss: 7.7489\n",
      "Iteration 859/1072, Loss: 7.7229\n",
      "Iteration 860/1072, Loss: 7.6994\n",
      "Iteration 861/1072, Loss: 7.6303\n",
      "Iteration 862/1072, Loss: 7.7118\n",
      "Iteration 863/1072, Loss: 7.7461\n",
      "Iteration 864/1072, Loss: 7.8133\n",
      "Iteration 865/1072, Loss: 7.7571\n",
      "Iteration 866/1072, Loss: 7.6922\n",
      "Iteration 867/1072, Loss: 7.8050\n",
      "Iteration 868/1072, Loss: 7.6331\n",
      "Iteration 869/1072, Loss: 7.6528\n",
      "Iteration 870/1072, Loss: 7.6746\n",
      "Iteration 871/1072, Loss: 7.7499\n",
      "Iteration 872/1072, Loss: 7.8051\n",
      "Iteration 873/1072, Loss: 7.6918\n",
      "Iteration 874/1072, Loss: 7.8067\n",
      "Iteration 875/1072, Loss: 7.6943\n",
      "Iteration 876/1072, Loss: 7.7298\n",
      "Iteration 877/1072, Loss: 7.7450\n",
      "Iteration 878/1072, Loss: 7.5992\n",
      "Iteration 879/1072, Loss: 7.7760\n",
      "Iteration 880/1072, Loss: 7.7312\n",
      "Iteration 881/1072, Loss: 7.7941\n",
      "Iteration 882/1072, Loss: 7.6859\n",
      "Iteration 883/1072, Loss: 7.6189\n",
      "Iteration 884/1072, Loss: 7.7138\n",
      "Iteration 885/1072, Loss: 7.7396\n",
      "Iteration 886/1072, Loss: 7.7251\n",
      "Iteration 887/1072, Loss: 7.7666\n",
      "Iteration 888/1072, Loss: 7.7397\n",
      "Iteration 889/1072, Loss: 7.6894\n",
      "Iteration 890/1072, Loss: 7.7040\n",
      "Iteration 891/1072, Loss: 7.7384\n",
      "Iteration 892/1072, Loss: 7.7115\n",
      "Iteration 893/1072, Loss: 7.6790\n",
      "Iteration 894/1072, Loss: 7.6280\n",
      "Iteration 895/1072, Loss: 7.7820\n",
      "Iteration 896/1072, Loss: 7.7074\n",
      "Iteration 897/1072, Loss: 7.7996\n",
      "Iteration 898/1072, Loss: 7.7258\n",
      "Iteration 899/1072, Loss: 7.8267\n",
      "Iteration 900/1072, Loss: 7.7529\n",
      "Iteration 901/1072, Loss: 7.8066\n",
      "Iteration 902/1072, Loss: 7.7353\n",
      "Iteration 903/1072, Loss: 7.6729\n",
      "Iteration 904/1072, Loss: 7.7966\n",
      "Iteration 905/1072, Loss: 7.8075\n",
      "Iteration 906/1072, Loss: 7.8525\n",
      "Iteration 907/1072, Loss: 7.7963\n",
      "Iteration 908/1072, Loss: 7.7470\n",
      "Iteration 909/1072, Loss: 7.7061\n",
      "Iteration 910/1072, Loss: 7.8268\n",
      "Iteration 911/1072, Loss: 7.7897\n",
      "Iteration 912/1072, Loss: 7.7269\n",
      "Iteration 913/1072, Loss: 7.7054\n",
      "Iteration 914/1072, Loss: 7.6826\n",
      "Iteration 915/1072, Loss: 7.7502\n",
      "Iteration 916/1072, Loss: 7.6882\n",
      "Iteration 917/1072, Loss: 7.7106\n",
      "Iteration 918/1072, Loss: 7.7133\n",
      "Iteration 919/1072, Loss: 7.6200\n",
      "Iteration 920/1072, Loss: 7.8072\n",
      "Iteration 921/1072, Loss: 7.7134\n",
      "Iteration 922/1072, Loss: 7.6673\n",
      "Iteration 923/1072, Loss: 7.7068\n",
      "Iteration 924/1072, Loss: 7.7220\n",
      "Iteration 925/1072, Loss: 7.7796\n",
      "Iteration 926/1072, Loss: 7.7512\n",
      "Iteration 927/1072, Loss: 7.6723\n",
      "Iteration 928/1072, Loss: 7.6520\n",
      "Iteration 929/1072, Loss: 7.6482\n",
      "Iteration 930/1072, Loss: 7.7634\n",
      "Iteration 931/1072, Loss: 7.7108\n",
      "Iteration 932/1072, Loss: 7.7381\n",
      "Iteration 933/1072, Loss: 7.6684\n",
      "Iteration 934/1072, Loss: 7.7095\n",
      "Iteration 935/1072, Loss: 7.7104\n",
      "Iteration 936/1072, Loss: 7.6902\n",
      "Iteration 937/1072, Loss: 7.8151\n",
      "Iteration 938/1072, Loss: 7.8041\n",
      "Iteration 939/1072, Loss: 7.7017\n",
      "Iteration 940/1072, Loss: 7.7495\n",
      "Iteration 941/1072, Loss: 7.7036\n",
      "Iteration 942/1072, Loss: 7.7493\n",
      "Iteration 943/1072, Loss: 7.7722\n",
      "Iteration 944/1072, Loss: 7.7664\n",
      "Iteration 945/1072, Loss: 7.6495\n",
      "Iteration 946/1072, Loss: 7.6364\n",
      "Iteration 947/1072, Loss: 7.6426\n",
      "Iteration 948/1072, Loss: 7.7039\n",
      "Iteration 949/1072, Loss: 7.7424\n",
      "Iteration 950/1072, Loss: 7.7394\n",
      "Iteration 951/1072, Loss: 7.7002\n",
      "Iteration 952/1072, Loss: 7.7216\n",
      "Iteration 953/1072, Loss: 7.7591\n",
      "Iteration 954/1072, Loss: 7.7253\n",
      "Iteration 955/1072, Loss: 7.7103\n",
      "Iteration 956/1072, Loss: 7.6871\n",
      "Iteration 957/1072, Loss: 7.7455\n",
      "Iteration 958/1072, Loss: 7.7546\n",
      "Iteration 959/1072, Loss: 7.7018\n",
      "Iteration 960/1072, Loss: 7.7082\n",
      "Iteration 961/1072, Loss: 7.7201\n",
      "Iteration 962/1072, Loss: 7.6242\n",
      "Iteration 963/1072, Loss: 7.7149\n",
      "Iteration 964/1072, Loss: 7.7343\n",
      "Iteration 965/1072, Loss: 7.7631\n",
      "Iteration 966/1072, Loss: 7.6730\n",
      "Iteration 967/1072, Loss: 7.6905\n",
      "Iteration 968/1072, Loss: 7.7344\n",
      "Iteration 969/1072, Loss: 7.7007\n",
      "Iteration 970/1072, Loss: 7.7382\n",
      "Iteration 971/1072, Loss: 7.7208\n",
      "Iteration 972/1072, Loss: 7.7141\n",
      "Iteration 973/1072, Loss: 7.8236\n",
      "Iteration 974/1072, Loss: 7.7268\n",
      "Iteration 975/1072, Loss: 7.7826\n",
      "Iteration 976/1072, Loss: 7.7088\n",
      "Iteration 977/1072, Loss: 7.7436\n",
      "Iteration 978/1072, Loss: 7.7331\n",
      "Iteration 979/1072, Loss: 7.7276\n",
      "Iteration 980/1072, Loss: 7.7554\n",
      "Iteration 981/1072, Loss: 7.6845\n",
      "Iteration 982/1072, Loss: 7.6524\n",
      "Iteration 983/1072, Loss: 7.8177\n",
      "Iteration 984/1072, Loss: 7.7290\n",
      "Iteration 985/1072, Loss: 7.7499\n",
      "Iteration 986/1072, Loss: 7.7604\n",
      "Iteration 987/1072, Loss: 7.7089\n",
      "Iteration 988/1072, Loss: 7.8178\n",
      "Iteration 989/1072, Loss: 7.7037\n",
      "Iteration 990/1072, Loss: 7.7180\n",
      "Iteration 991/1072, Loss: 7.6676\n",
      "Iteration 992/1072, Loss: 7.6263\n",
      "Iteration 993/1072, Loss: 7.6005\n",
      "Iteration 994/1072, Loss: 7.7588\n",
      "Iteration 995/1072, Loss: 7.6934\n",
      "Iteration 996/1072, Loss: 7.7597\n",
      "Iteration 997/1072, Loss: 7.7372\n",
      "Iteration 998/1072, Loss: 7.7011\n",
      "Iteration 999/1072, Loss: 7.7098\n",
      "Iteration 1000/1072, Loss: 7.6397\n",
      "Iteration 1001/1072, Loss: 7.7185\n",
      "Iteration 1002/1072, Loss: 7.7194\n",
      "Iteration 1003/1072, Loss: 7.6541\n",
      "Iteration 1004/1072, Loss: 7.7136\n",
      "Iteration 1005/1072, Loss: 7.6571\n",
      "Iteration 1006/1072, Loss: 7.7476\n",
      "Iteration 1007/1072, Loss: 7.7075\n",
      "Iteration 1008/1072, Loss: 7.7566\n",
      "Iteration 1009/1072, Loss: 7.7047\n",
      "Iteration 1010/1072, Loss: 7.7346\n",
      "Iteration 1011/1072, Loss: 7.6528\n",
      "Iteration 1012/1072, Loss: 7.7211\n",
      "Iteration 1013/1072, Loss: 7.7261\n",
      "Iteration 1014/1072, Loss: 7.7706\n",
      "Iteration 1015/1072, Loss: 7.7412\n",
      "Iteration 1016/1072, Loss: 7.6818\n",
      "Iteration 1017/1072, Loss: 7.7446\n",
      "Iteration 1018/1072, Loss: 7.8682\n",
      "Iteration 1019/1072, Loss: 7.7324\n",
      "Iteration 1020/1072, Loss: 7.8007\n",
      "Iteration 1021/1072, Loss: 7.7558\n",
      "Iteration 1022/1072, Loss: 7.6647\n",
      "Iteration 1023/1072, Loss: 7.7364\n",
      "Iteration 1024/1072, Loss: 7.6911\n",
      "Iteration 1025/1072, Loss: 7.8075\n",
      "Iteration 1026/1072, Loss: 7.7295\n",
      "Iteration 1027/1072, Loss: 7.6546\n",
      "Iteration 1028/1072, Loss: 7.7048\n",
      "Iteration 1029/1072, Loss: 7.7432\n",
      "Iteration 1030/1072, Loss: 7.7594\n",
      "Iteration 1031/1072, Loss: 7.7874\n",
      "Iteration 1032/1072, Loss: 7.7292\n",
      "Iteration 1033/1072, Loss: 7.7009\n",
      "Iteration 1034/1072, Loss: 7.7069\n",
      "Iteration 1035/1072, Loss: 7.6985\n",
      "Iteration 1036/1072, Loss: 7.8244\n",
      "Iteration 1037/1072, Loss: 7.6680\n",
      "Iteration 1038/1072, Loss: 7.7190\n",
      "Iteration 1039/1072, Loss: 7.6890\n",
      "Iteration 1040/1072, Loss: 7.5919\n",
      "Iteration 1041/1072, Loss: 7.8046\n",
      "Iteration 1042/1072, Loss: 7.7330\n",
      "Iteration 1043/1072, Loss: 7.7703\n",
      "Iteration 1044/1072, Loss: 7.7386\n",
      "Iteration 1045/1072, Loss: 7.7278\n",
      "Iteration 1046/1072, Loss: 7.7542\n",
      "Iteration 1047/1072, Loss: 7.6739\n",
      "Iteration 1048/1072, Loss: 7.7249\n",
      "Iteration 1049/1072, Loss: 7.7239\n",
      "Iteration 1050/1072, Loss: 7.7376\n",
      "Iteration 1051/1072, Loss: 7.6895\n",
      "Iteration 1052/1072, Loss: 7.7061\n",
      "Iteration 1053/1072, Loss: 7.7662\n",
      "Iteration 1054/1072, Loss: 7.7103\n",
      "Iteration 1055/1072, Loss: 7.7860\n",
      "Iteration 1056/1072, Loss: 7.6559\n",
      "Iteration 1057/1072, Loss: 7.7154\n",
      "Iteration 1058/1072, Loss: 7.6067\n",
      "Iteration 1059/1072, Loss: 7.7409\n",
      "Iteration 1060/1072, Loss: 7.7772\n",
      "Iteration 1061/1072, Loss: 7.6757\n",
      "Iteration 1062/1072, Loss: 7.7384\n",
      "Iteration 1063/1072, Loss: 7.7624\n",
      "Iteration 1064/1072, Loss: 7.6930\n",
      "Iteration 1065/1072, Loss: 7.6169\n",
      "Iteration 1066/1072, Loss: 7.6463\n",
      "Iteration 1067/1072, Loss: 7.7216\n",
      "Iteration 1068/1072, Loss: 7.7462\n",
      "Iteration 1069/1072, Loss: 7.6393\n",
      "Iteration 1070/1072, Loss: 7.7484\n",
      "Iteration 1071/1072, Loss: 7.7074\n",
      "Iteration 1072/1072, Loss: 7.8393\n",
      "Epoch 6/10, Loss: 7.7442\n",
      "Checkpoint saved at checkpoints/efficientnet-b4_epoch_6.pth\n",
      "Validation Accuracy: 2.60%\n",
      "Iteration 1/1072, Loss: 7.6402\n",
      "Iteration 2/1072, Loss: 7.7080\n",
      "Iteration 3/1072, Loss: 7.6141\n",
      "Iteration 4/1072, Loss: 7.6449\n",
      "Iteration 5/1072, Loss: 7.7108\n",
      "Iteration 6/1072, Loss: 7.6766\n",
      "Iteration 7/1072, Loss: 7.6316\n",
      "Iteration 8/1072, Loss: 7.6858\n",
      "Iteration 9/1072, Loss: 7.7498\n",
      "Iteration 10/1072, Loss: 7.5995\n",
      "Iteration 11/1072, Loss: 7.6926\n",
      "Iteration 12/1072, Loss: 7.7111\n",
      "Iteration 13/1072, Loss: 7.5633\n",
      "Iteration 14/1072, Loss: 7.7094\n",
      "Iteration 15/1072, Loss: 7.7099\n",
      "Iteration 16/1072, Loss: 7.6799\n",
      "Iteration 17/1072, Loss: 7.7282\n",
      "Iteration 18/1072, Loss: 7.5836\n",
      "Iteration 19/1072, Loss: 7.7115\n",
      "Iteration 20/1072, Loss: 7.6618\n",
      "Iteration 21/1072, Loss: 7.6139\n",
      "Iteration 22/1072, Loss: 7.6394\n",
      "Iteration 23/1072, Loss: 7.6568\n",
      "Iteration 24/1072, Loss: 7.6749\n",
      "Iteration 25/1072, Loss: 7.6752\n",
      "Iteration 26/1072, Loss: 7.7494\n",
      "Iteration 27/1072, Loss: 7.6895\n",
      "Iteration 28/1072, Loss: 7.6134\n",
      "Iteration 29/1072, Loss: 7.7262\n",
      "Iteration 30/1072, Loss: 7.6667\n",
      "Iteration 31/1072, Loss: 7.6762\n",
      "Iteration 32/1072, Loss: 7.6167\n",
      "Iteration 33/1072, Loss: 7.6887\n",
      "Iteration 34/1072, Loss: 7.6323\n",
      "Iteration 35/1072, Loss: 7.7612\n",
      "Iteration 36/1072, Loss: 7.6245\n",
      "Iteration 37/1072, Loss: 7.6048\n",
      "Iteration 38/1072, Loss: 7.5921\n",
      "Iteration 39/1072, Loss: 7.6667\n",
      "Iteration 40/1072, Loss: 7.7147\n",
      "Iteration 41/1072, Loss: 7.7220\n",
      "Iteration 42/1072, Loss: 7.6711\n",
      "Iteration 43/1072, Loss: 7.6182\n",
      "Iteration 44/1072, Loss: 7.6382\n",
      "Iteration 45/1072, Loss: 7.7418\n",
      "Iteration 46/1072, Loss: 7.6595\n",
      "Iteration 47/1072, Loss: 7.7477\n",
      "Iteration 48/1072, Loss: 7.5929\n",
      "Iteration 49/1072, Loss: 7.7069\n",
      "Iteration 50/1072, Loss: 7.7594\n",
      "Iteration 51/1072, Loss: 7.7213\n",
      "Iteration 52/1072, Loss: 7.6888\n",
      "Iteration 53/1072, Loss: 7.6826\n",
      "Iteration 54/1072, Loss: 7.6066\n",
      "Iteration 55/1072, Loss: 7.6711\n",
      "Iteration 56/1072, Loss: 7.6655\n",
      "Iteration 57/1072, Loss: 7.6009\n",
      "Iteration 58/1072, Loss: 7.6324\n",
      "Iteration 59/1072, Loss: 7.6147\n",
      "Iteration 60/1072, Loss: 7.7320\n",
      "Iteration 61/1072, Loss: 7.6404\n",
      "Iteration 62/1072, Loss: 7.7006\n",
      "Iteration 63/1072, Loss: 7.7145\n",
      "Iteration 64/1072, Loss: 7.5850\n",
      "Iteration 65/1072, Loss: 7.7557\n",
      "Iteration 66/1072, Loss: 7.6647\n",
      "Iteration 67/1072, Loss: 7.6748\n",
      "Iteration 68/1072, Loss: 7.6830\n",
      "Iteration 69/1072, Loss: 7.6725\n",
      "Iteration 70/1072, Loss: 7.5912\n",
      "Iteration 71/1072, Loss: 7.6898\n",
      "Iteration 72/1072, Loss: 7.6894\n",
      "Iteration 73/1072, Loss: 7.7292\n",
      "Iteration 74/1072, Loss: 7.7237\n",
      "Iteration 75/1072, Loss: 7.6146\n",
      "Iteration 76/1072, Loss: 7.6353\n",
      "Iteration 77/1072, Loss: 7.6264\n",
      "Iteration 78/1072, Loss: 7.6207\n",
      "Iteration 79/1072, Loss: 7.6948\n",
      "Iteration 80/1072, Loss: 7.7106\n",
      "Iteration 81/1072, Loss: 7.7199\n",
      "Iteration 82/1072, Loss: 7.6714\n",
      "Iteration 83/1072, Loss: 7.6644\n",
      "Iteration 84/1072, Loss: 7.5767\n",
      "Iteration 85/1072, Loss: 7.6020\n",
      "Iteration 86/1072, Loss: 7.7084\n",
      "Iteration 87/1072, Loss: 7.7078\n",
      "Iteration 88/1072, Loss: 7.6526\n",
      "Iteration 89/1072, Loss: 7.7161\n",
      "Iteration 90/1072, Loss: 7.7984\n",
      "Iteration 91/1072, Loss: 7.7080\n",
      "Iteration 92/1072, Loss: 7.6076\n",
      "Iteration 93/1072, Loss: 7.7008\n",
      "Iteration 94/1072, Loss: 7.6998\n",
      "Iteration 95/1072, Loss: 7.6748\n",
      "Iteration 96/1072, Loss: 7.6495\n",
      "Iteration 97/1072, Loss: 7.6906\n",
      "Iteration 98/1072, Loss: 7.6941\n",
      "Iteration 99/1072, Loss: 7.6910\n",
      "Iteration 100/1072, Loss: 7.7852\n",
      "Iteration 101/1072, Loss: 7.7334\n",
      "Iteration 102/1072, Loss: 7.6551\n",
      "Iteration 103/1072, Loss: 7.6904\n",
      "Iteration 104/1072, Loss: 7.6966\n",
      "Iteration 105/1072, Loss: 7.6873\n",
      "Iteration 106/1072, Loss: 7.7066\n",
      "Iteration 107/1072, Loss: 7.6671\n",
      "Iteration 108/1072, Loss: 7.5436\n",
      "Iteration 109/1072, Loss: 7.6262\n",
      "Iteration 110/1072, Loss: 7.6611\n",
      "Iteration 111/1072, Loss: 7.6689\n",
      "Iteration 112/1072, Loss: 7.6808\n",
      "Iteration 113/1072, Loss: 7.6371\n",
      "Iteration 114/1072, Loss: 7.6524\n",
      "Iteration 115/1072, Loss: 7.7034\n",
      "Iteration 116/1072, Loss: 7.6320\n",
      "Iteration 117/1072, Loss: 7.6256\n",
      "Iteration 118/1072, Loss: 7.6879\n",
      "Iteration 119/1072, Loss: 7.6725\n",
      "Iteration 120/1072, Loss: 7.7308\n",
      "Iteration 121/1072, Loss: 7.7803\n",
      "Iteration 122/1072, Loss: 7.6568\n",
      "Iteration 123/1072, Loss: 7.5594\n",
      "Iteration 124/1072, Loss: 7.6964\n",
      "Iteration 125/1072, Loss: 7.6553\n",
      "Iteration 126/1072, Loss: 7.6262\n",
      "Iteration 127/1072, Loss: 7.6935\n",
      "Iteration 128/1072, Loss: 7.7757\n",
      "Iteration 129/1072, Loss: 7.7608\n",
      "Iteration 130/1072, Loss: 7.6869\n",
      "Iteration 131/1072, Loss: 7.6718\n",
      "Iteration 132/1072, Loss: 7.7215\n",
      "Iteration 133/1072, Loss: 7.6701\n",
      "Iteration 134/1072, Loss: 7.6231\n",
      "Iteration 135/1072, Loss: 7.6590\n",
      "Iteration 136/1072, Loss: 7.6724\n",
      "Iteration 137/1072, Loss: 7.6843\n",
      "Iteration 138/1072, Loss: 7.6896\n",
      "Iteration 139/1072, Loss: 7.6407\n",
      "Iteration 140/1072, Loss: 7.6890\n",
      "Iteration 141/1072, Loss: 7.6714\n",
      "Iteration 142/1072, Loss: 7.7039\n",
      "Iteration 143/1072, Loss: 7.6713\n",
      "Iteration 144/1072, Loss: 7.4986\n",
      "Iteration 145/1072, Loss: 7.6870\n",
      "Iteration 146/1072, Loss: 7.6813\n",
      "Iteration 147/1072, Loss: 7.6675\n",
      "Iteration 148/1072, Loss: 7.7456\n",
      "Iteration 149/1072, Loss: 7.6846\n",
      "Iteration 150/1072, Loss: 7.6930\n",
      "Iteration 151/1072, Loss: 7.8238\n",
      "Iteration 152/1072, Loss: 7.6505\n",
      "Iteration 153/1072, Loss: 7.7099\n",
      "Iteration 154/1072, Loss: 7.7153\n",
      "Iteration 155/1072, Loss: 7.6534\n",
      "Iteration 156/1072, Loss: 7.6496\n",
      "Iteration 157/1072, Loss: 7.5794\n",
      "Iteration 158/1072, Loss: 7.6218\n",
      "Iteration 159/1072, Loss: 7.6775\n",
      "Iteration 160/1072, Loss: 7.5313\n",
      "Iteration 161/1072, Loss: 7.6163\n",
      "Iteration 162/1072, Loss: 7.6257\n",
      "Iteration 163/1072, Loss: 7.6884\n",
      "Iteration 164/1072, Loss: 7.5559\n",
      "Iteration 165/1072, Loss: 7.6439\n",
      "Iteration 166/1072, Loss: 7.5919\n",
      "Iteration 167/1072, Loss: 7.6213\n",
      "Iteration 168/1072, Loss: 7.7243\n",
      "Iteration 169/1072, Loss: 7.7131\n",
      "Iteration 170/1072, Loss: 7.6802\n",
      "Iteration 171/1072, Loss: 7.6533\n",
      "Iteration 172/1072, Loss: 7.5602\n",
      "Iteration 173/1072, Loss: 7.7310\n",
      "Iteration 174/1072, Loss: 7.6796\n",
      "Iteration 175/1072, Loss: 7.7234\n",
      "Iteration 176/1072, Loss: 7.7973\n",
      "Iteration 177/1072, Loss: 7.7972\n",
      "Iteration 178/1072, Loss: 7.7413\n",
      "Iteration 179/1072, Loss: 7.6904\n",
      "Iteration 180/1072, Loss: 7.7068\n",
      "Iteration 181/1072, Loss: 7.7514\n",
      "Iteration 182/1072, Loss: 7.6812\n",
      "Iteration 183/1072, Loss: 7.7226\n",
      "Iteration 184/1072, Loss: 7.6187\n",
      "Iteration 185/1072, Loss: 7.6432\n",
      "Iteration 186/1072, Loss: 7.7643\n",
      "Iteration 187/1072, Loss: 7.6379\n",
      "Iteration 188/1072, Loss: 7.7096\n",
      "Iteration 189/1072, Loss: 7.7451\n",
      "Iteration 190/1072, Loss: 7.6869\n",
      "Iteration 191/1072, Loss: 7.6177\n",
      "Iteration 192/1072, Loss: 7.7224\n",
      "Iteration 193/1072, Loss: 7.7738\n",
      "Iteration 194/1072, Loss: 7.5715\n",
      "Iteration 195/1072, Loss: 7.6729\n",
      "Iteration 196/1072, Loss: 7.7196\n",
      "Iteration 197/1072, Loss: 7.7058\n",
      "Iteration 198/1072, Loss: 7.6751\n",
      "Iteration 199/1072, Loss: 7.6813\n",
      "Iteration 200/1072, Loss: 7.7307\n",
      "Iteration 201/1072, Loss: 7.6686\n",
      "Iteration 202/1072, Loss: 7.6554\n",
      "Iteration 203/1072, Loss: 7.7213\n",
      "Iteration 204/1072, Loss: 7.6996\n",
      "Iteration 205/1072, Loss: 7.6518\n",
      "Iteration 206/1072, Loss: 7.6830\n",
      "Iteration 207/1072, Loss: 7.6851\n",
      "Iteration 208/1072, Loss: 7.6854\n",
      "Iteration 209/1072, Loss: 7.5839\n",
      "Iteration 210/1072, Loss: 7.6213\n",
      "Iteration 211/1072, Loss: 7.6191\n",
      "Iteration 212/1072, Loss: 7.7200\n",
      "Iteration 213/1072, Loss: 7.6646\n",
      "Iteration 214/1072, Loss: 7.5876\n",
      "Iteration 215/1072, Loss: 7.6416\n",
      "Iteration 216/1072, Loss: 7.6832\n",
      "Iteration 217/1072, Loss: 7.6646\n",
      "Iteration 218/1072, Loss: 7.7391\n",
      "Iteration 219/1072, Loss: 7.6726\n",
      "Iteration 220/1072, Loss: 7.7121\n",
      "Iteration 221/1072, Loss: 7.6104\n",
      "Iteration 222/1072, Loss: 7.6172\n",
      "Iteration 223/1072, Loss: 7.6523\n",
      "Iteration 224/1072, Loss: 7.6938\n",
      "Iteration 225/1072, Loss: 7.7084\n",
      "Iteration 226/1072, Loss: 7.6072\n",
      "Iteration 227/1072, Loss: 7.6638\n",
      "Iteration 228/1072, Loss: 7.6695\n",
      "Iteration 229/1072, Loss: 7.6658\n",
      "Iteration 230/1072, Loss: 7.6903\n",
      "Iteration 231/1072, Loss: 7.6211\n",
      "Iteration 232/1072, Loss: 7.6740\n",
      "Iteration 233/1072, Loss: 7.6985\n",
      "Iteration 234/1072, Loss: 7.7049\n",
      "Iteration 235/1072, Loss: 7.5749\n",
      "Iteration 236/1072, Loss: 7.6358\n",
      "Iteration 237/1072, Loss: 7.6276\n",
      "Iteration 238/1072, Loss: 7.7329\n",
      "Iteration 239/1072, Loss: 7.6522\n",
      "Iteration 240/1072, Loss: 7.5929\n",
      "Iteration 241/1072, Loss: 7.7724\n",
      "Iteration 242/1072, Loss: 7.7197\n",
      "Iteration 243/1072, Loss: 7.7187\n",
      "Iteration 244/1072, Loss: 7.6490\n",
      "Iteration 245/1072, Loss: 7.6761\n",
      "Iteration 246/1072, Loss: 7.7263\n",
      "Iteration 247/1072, Loss: 7.6983\n",
      "Iteration 248/1072, Loss: 7.6552\n",
      "Iteration 249/1072, Loss: 7.7296\n",
      "Iteration 250/1072, Loss: 7.6136\n",
      "Iteration 251/1072, Loss: 7.7937\n",
      "Iteration 252/1072, Loss: 7.6808\n",
      "Iteration 253/1072, Loss: 7.6673\n",
      "Iteration 254/1072, Loss: 7.5927\n",
      "Iteration 255/1072, Loss: 7.6150\n",
      "Iteration 256/1072, Loss: 7.6129\n",
      "Iteration 257/1072, Loss: 7.6482\n",
      "Iteration 258/1072, Loss: 7.7584\n",
      "Iteration 259/1072, Loss: 7.5243\n",
      "Iteration 260/1072, Loss: 7.6607\n",
      "Iteration 261/1072, Loss: 7.6284\n",
      "Iteration 262/1072, Loss: 7.5591\n",
      "Iteration 263/1072, Loss: 7.7526\n",
      "Iteration 264/1072, Loss: 7.6944\n",
      "Iteration 265/1072, Loss: 7.7373\n",
      "Iteration 266/1072, Loss: 7.6738\n",
      "Iteration 267/1072, Loss: 7.6863\n",
      "Iteration 268/1072, Loss: 7.6422\n",
      "Iteration 269/1072, Loss: 7.5868\n",
      "Iteration 270/1072, Loss: 7.6632\n",
      "Iteration 271/1072, Loss: 7.6512\n",
      "Iteration 272/1072, Loss: 7.7285\n",
      "Iteration 273/1072, Loss: 7.7504\n",
      "Iteration 274/1072, Loss: 7.6864\n",
      "Iteration 275/1072, Loss: 7.6255\n",
      "Iteration 276/1072, Loss: 7.5846\n",
      "Iteration 277/1072, Loss: 7.6849\n",
      "Iteration 278/1072, Loss: 7.7482\n",
      "Iteration 279/1072, Loss: 7.7588\n",
      "Iteration 280/1072, Loss: 7.7052\n",
      "Iteration 281/1072, Loss: 7.6954\n",
      "Iteration 282/1072, Loss: 7.7710\n",
      "Iteration 283/1072, Loss: 7.7522\n",
      "Iteration 284/1072, Loss: 7.6686\n",
      "Iteration 285/1072, Loss: 7.7525\n",
      "Iteration 286/1072, Loss: 7.6765\n",
      "Iteration 287/1072, Loss: 7.6863\n",
      "Iteration 288/1072, Loss: 7.6522\n",
      "Iteration 289/1072, Loss: 7.6020\n",
      "Iteration 290/1072, Loss: 7.5818\n",
      "Iteration 291/1072, Loss: 7.6616\n",
      "Iteration 292/1072, Loss: 7.6094\n",
      "Iteration 293/1072, Loss: 7.4862\n",
      "Iteration 294/1072, Loss: 7.6442\n",
      "Iteration 295/1072, Loss: 7.6348\n",
      "Iteration 296/1072, Loss: 7.7153\n",
      "Iteration 297/1072, Loss: 7.6184\n",
      "Iteration 298/1072, Loss: 7.6909\n",
      "Iteration 299/1072, Loss: 7.6404\n",
      "Iteration 300/1072, Loss: 7.5301\n",
      "Iteration 301/1072, Loss: 7.6779\n",
      "Iteration 302/1072, Loss: 7.6233\n",
      "Iteration 303/1072, Loss: 7.6093\n",
      "Iteration 304/1072, Loss: 7.6785\n",
      "Iteration 305/1072, Loss: 7.7308\n",
      "Iteration 306/1072, Loss: 7.6330\n",
      "Iteration 307/1072, Loss: 7.6929\n",
      "Iteration 308/1072, Loss: 7.6816\n",
      "Iteration 309/1072, Loss: 7.6108\n",
      "Iteration 310/1072, Loss: 7.6757\n",
      "Iteration 311/1072, Loss: 7.6726\n",
      "Iteration 312/1072, Loss: 7.6236\n",
      "Iteration 313/1072, Loss: 7.7351\n",
      "Iteration 314/1072, Loss: 7.6009\n",
      "Iteration 315/1072, Loss: 7.6629\n",
      "Iteration 316/1072, Loss: 7.7115\n",
      "Iteration 317/1072, Loss: 7.5719\n",
      "Iteration 318/1072, Loss: 7.6554\n",
      "Iteration 319/1072, Loss: 7.6462\n",
      "Iteration 320/1072, Loss: 7.6522\n",
      "Iteration 321/1072, Loss: 7.6462\n",
      "Iteration 322/1072, Loss: 7.6914\n",
      "Iteration 323/1072, Loss: 7.6754\n",
      "Iteration 324/1072, Loss: 7.6249\n",
      "Iteration 325/1072, Loss: 7.7517\n",
      "Iteration 326/1072, Loss: 7.7541\n",
      "Iteration 327/1072, Loss: 7.6469\n",
      "Iteration 328/1072, Loss: 7.6565\n",
      "Iteration 329/1072, Loss: 7.7322\n",
      "Iteration 330/1072, Loss: 7.6679\n",
      "Iteration 331/1072, Loss: 7.7249\n",
      "Iteration 332/1072, Loss: 7.7741\n",
      "Iteration 333/1072, Loss: 7.6748\n",
      "Iteration 334/1072, Loss: 7.6321\n",
      "Iteration 335/1072, Loss: 7.6914\n",
      "Iteration 336/1072, Loss: 7.5942\n",
      "Iteration 337/1072, Loss: 7.6784\n",
      "Iteration 338/1072, Loss: 7.6798\n",
      "Iteration 339/1072, Loss: 7.6487\n",
      "Iteration 340/1072, Loss: 7.7734\n",
      "Iteration 341/1072, Loss: 7.6486\n",
      "Iteration 342/1072, Loss: 7.7049\n",
      "Iteration 343/1072, Loss: 7.7299\n",
      "Iteration 344/1072, Loss: 7.5341\n",
      "Iteration 345/1072, Loss: 7.6574\n",
      "Iteration 346/1072, Loss: 7.5967\n",
      "Iteration 347/1072, Loss: 7.7249\n",
      "Iteration 348/1072, Loss: 7.7182\n",
      "Iteration 349/1072, Loss: 7.7336\n",
      "Iteration 350/1072, Loss: 7.6696\n",
      "Iteration 351/1072, Loss: 7.6335\n",
      "Iteration 352/1072, Loss: 7.6116\n",
      "Iteration 353/1072, Loss: 7.6026\n",
      "Iteration 354/1072, Loss: 7.6934\n",
      "Iteration 355/1072, Loss: 7.6465\n",
      "Iteration 356/1072, Loss: 7.6867\n",
      "Iteration 357/1072, Loss: 7.6848\n",
      "Iteration 358/1072, Loss: 7.6248\n",
      "Iteration 359/1072, Loss: 7.7913\n",
      "Iteration 360/1072, Loss: 7.6459\n",
      "Iteration 361/1072, Loss: 7.7013\n",
      "Iteration 362/1072, Loss: 7.6491\n",
      "Iteration 363/1072, Loss: 7.5962\n",
      "Iteration 364/1072, Loss: 7.7595\n",
      "Iteration 365/1072, Loss: 7.6235\n",
      "Iteration 366/1072, Loss: 7.6693\n",
      "Iteration 367/1072, Loss: 7.7851\n",
      "Iteration 368/1072, Loss: 7.5893\n",
      "Iteration 369/1072, Loss: 7.7790\n",
      "Iteration 370/1072, Loss: 7.6945\n",
      "Iteration 371/1072, Loss: 7.6257\n",
      "Iteration 372/1072, Loss: 7.6033\n",
      "Iteration 373/1072, Loss: 7.6682\n",
      "Iteration 374/1072, Loss: 7.7006\n",
      "Iteration 375/1072, Loss: 7.6566\n",
      "Iteration 376/1072, Loss: 7.6969\n",
      "Iteration 377/1072, Loss: 7.6159\n",
      "Iteration 378/1072, Loss: 7.5180\n",
      "Iteration 379/1072, Loss: 7.6931\n",
      "Iteration 380/1072, Loss: 7.7432\n",
      "Iteration 381/1072, Loss: 7.6460\n",
      "Iteration 382/1072, Loss: 7.6582\n",
      "Iteration 383/1072, Loss: 7.6203\n",
      "Iteration 384/1072, Loss: 7.5821\n",
      "Iteration 385/1072, Loss: 7.6554\n",
      "Iteration 386/1072, Loss: 7.7109\n",
      "Iteration 387/1072, Loss: 7.5895\n",
      "Iteration 388/1072, Loss: 7.6754\n",
      "Iteration 389/1072, Loss: 7.7975\n",
      "Iteration 390/1072, Loss: 7.7139\n",
      "Iteration 391/1072, Loss: 7.6461\n",
      "Iteration 392/1072, Loss: 7.7283\n",
      "Iteration 393/1072, Loss: 7.6806\n",
      "Iteration 394/1072, Loss: 7.6534\n",
      "Iteration 395/1072, Loss: 7.6858\n",
      "Iteration 396/1072, Loss: 7.6766\n",
      "Iteration 397/1072, Loss: 7.6960\n",
      "Iteration 398/1072, Loss: 7.5759\n",
      "Iteration 399/1072, Loss: 7.6315\n",
      "Iteration 400/1072, Loss: 7.6334\n",
      "Iteration 401/1072, Loss: 7.6114\n",
      "Iteration 402/1072, Loss: 7.6907\n",
      "Iteration 403/1072, Loss: 7.6834\n",
      "Iteration 404/1072, Loss: 7.5968\n",
      "Iteration 405/1072, Loss: 7.6740\n",
      "Iteration 406/1072, Loss: 7.6742\n",
      "Iteration 407/1072, Loss: 7.7499\n",
      "Iteration 408/1072, Loss: 7.6429\n",
      "Iteration 409/1072, Loss: 7.7059\n",
      "Iteration 410/1072, Loss: 7.7378\n",
      "Iteration 411/1072, Loss: 7.6526\n",
      "Iteration 412/1072, Loss: 7.5753\n",
      "Iteration 413/1072, Loss: 7.6143\n",
      "Iteration 414/1072, Loss: 7.6344\n",
      "Iteration 415/1072, Loss: 7.6971\n",
      "Iteration 416/1072, Loss: 7.6707\n",
      "Iteration 417/1072, Loss: 7.6956\n",
      "Iteration 418/1072, Loss: 7.6538\n",
      "Iteration 419/1072, Loss: 7.6505\n",
      "Iteration 420/1072, Loss: 7.6171\n",
      "Iteration 421/1072, Loss: 7.6881\n",
      "Iteration 422/1072, Loss: 7.6298\n",
      "Iteration 423/1072, Loss: 7.6698\n",
      "Iteration 424/1072, Loss: 7.7315\n",
      "Iteration 425/1072, Loss: 7.7236\n",
      "Iteration 426/1072, Loss: 7.7020\n",
      "Iteration 427/1072, Loss: 7.5581\n",
      "Iteration 428/1072, Loss: 7.6304\n",
      "Iteration 429/1072, Loss: 7.6869\n",
      "Iteration 430/1072, Loss: 7.6117\n",
      "Iteration 431/1072, Loss: 7.6075\n",
      "Iteration 432/1072, Loss: 7.6552\n",
      "Iteration 433/1072, Loss: 7.6691\n",
      "Iteration 434/1072, Loss: 7.6772\n",
      "Iteration 435/1072, Loss: 7.5498\n",
      "Iteration 436/1072, Loss: 7.6304\n",
      "Iteration 437/1072, Loss: 7.6727\n",
      "Iteration 438/1072, Loss: 7.6690\n",
      "Iteration 439/1072, Loss: 7.6053\n",
      "Iteration 440/1072, Loss: 7.6116\n",
      "Iteration 441/1072, Loss: 7.6438\n",
      "Iteration 442/1072, Loss: 7.7620\n",
      "Iteration 443/1072, Loss: 7.6744\n",
      "Iteration 444/1072, Loss: 7.5707\n",
      "Iteration 445/1072, Loss: 7.7872\n",
      "Iteration 446/1072, Loss: 7.6007\n",
      "Iteration 447/1072, Loss: 7.6433\n",
      "Iteration 448/1072, Loss: 7.6887\n",
      "Iteration 449/1072, Loss: 7.7383\n",
      "Iteration 450/1072, Loss: 7.6733\n",
      "Iteration 451/1072, Loss: 7.7075\n",
      "Iteration 452/1072, Loss: 7.6275\n",
      "Iteration 453/1072, Loss: 7.6367\n",
      "Iteration 454/1072, Loss: 7.6424\n",
      "Iteration 455/1072, Loss: 7.6730\n",
      "Iteration 456/1072, Loss: 7.5997\n",
      "Iteration 457/1072, Loss: 7.6163\n",
      "Iteration 458/1072, Loss: 7.5462\n",
      "Iteration 459/1072, Loss: 7.6520\n",
      "Iteration 460/1072, Loss: 7.7030\n",
      "Iteration 461/1072, Loss: 7.6750\n",
      "Iteration 462/1072, Loss: 7.6475\n",
      "Iteration 463/1072, Loss: 7.6955\n",
      "Iteration 464/1072, Loss: 7.6167\n",
      "Iteration 465/1072, Loss: 7.7467\n",
      "Iteration 466/1072, Loss: 7.6468\n",
      "Iteration 467/1072, Loss: 7.6123\n",
      "Iteration 468/1072, Loss: 7.7475\n",
      "Iteration 469/1072, Loss: 7.5281\n",
      "Iteration 470/1072, Loss: 7.7351\n",
      "Iteration 471/1072, Loss: 7.6457\n",
      "Iteration 472/1072, Loss: 7.6689\n",
      "Iteration 473/1072, Loss: 7.7137\n",
      "Iteration 474/1072, Loss: 7.7633\n",
      "Iteration 475/1072, Loss: 7.6568\n",
      "Iteration 476/1072, Loss: 7.6930\n",
      "Iteration 477/1072, Loss: 7.6213\n",
      "Iteration 478/1072, Loss: 7.5582\n",
      "Iteration 479/1072, Loss: 7.7621\n",
      "Iteration 480/1072, Loss: 7.6227\n",
      "Iteration 481/1072, Loss: 7.6685\n",
      "Iteration 482/1072, Loss: 7.6328\n",
      "Iteration 483/1072, Loss: 7.6604\n",
      "Iteration 484/1072, Loss: 7.6067\n",
      "Iteration 485/1072, Loss: 7.7171\n",
      "Iteration 486/1072, Loss: 7.6087\n",
      "Iteration 487/1072, Loss: 7.6662\n",
      "Iteration 488/1072, Loss: 7.7298\n",
      "Iteration 489/1072, Loss: 7.6851\n",
      "Iteration 490/1072, Loss: 7.5534\n",
      "Iteration 491/1072, Loss: 7.6650\n",
      "Iteration 492/1072, Loss: 7.6415\n",
      "Iteration 493/1072, Loss: 7.5871\n",
      "Iteration 494/1072, Loss: 7.7020\n",
      "Iteration 495/1072, Loss: 7.5773\n",
      "Iteration 496/1072, Loss: 7.5854\n",
      "Iteration 497/1072, Loss: 7.7257\n",
      "Iteration 498/1072, Loss: 7.5597\n",
      "Iteration 499/1072, Loss: 7.6564\n",
      "Iteration 500/1072, Loss: 7.7073\n",
      "Iteration 501/1072, Loss: 7.5922\n",
      "Iteration 502/1072, Loss: 7.6989\n",
      "Iteration 503/1072, Loss: 7.6229\n",
      "Iteration 504/1072, Loss: 7.7596\n",
      "Iteration 505/1072, Loss: 7.6972\n",
      "Iteration 506/1072, Loss: 7.5698\n",
      "Iteration 507/1072, Loss: 7.7235\n",
      "Iteration 508/1072, Loss: 7.5713\n",
      "Iteration 509/1072, Loss: 7.7231\n",
      "Iteration 510/1072, Loss: 7.5310\n",
      "Iteration 511/1072, Loss: 7.6673\n",
      "Iteration 512/1072, Loss: 7.5411\n",
      "Iteration 513/1072, Loss: 7.6141\n",
      "Iteration 514/1072, Loss: 7.6255\n",
      "Iteration 515/1072, Loss: 7.7122\n",
      "Iteration 516/1072, Loss: 7.6134\n",
      "Iteration 517/1072, Loss: 7.5776\n",
      "Iteration 518/1072, Loss: 7.7291\n",
      "Iteration 519/1072, Loss: 7.6778\n",
      "Iteration 520/1072, Loss: 7.6643\n",
      "Iteration 521/1072, Loss: 7.6624\n",
      "Iteration 522/1072, Loss: 7.7925\n",
      "Iteration 523/1072, Loss: 7.6516\n",
      "Iteration 524/1072, Loss: 7.6776\n",
      "Iteration 525/1072, Loss: 7.6371\n",
      "Iteration 526/1072, Loss: 7.6499\n",
      "Iteration 527/1072, Loss: 7.6284\n",
      "Iteration 528/1072, Loss: 7.6722\n",
      "Iteration 529/1072, Loss: 7.6649\n",
      "Iteration 530/1072, Loss: 7.7446\n",
      "Iteration 531/1072, Loss: 7.6043\n",
      "Iteration 532/1072, Loss: 7.6675\n",
      "Iteration 533/1072, Loss: 7.6389\n",
      "Iteration 534/1072, Loss: 7.6329\n",
      "Iteration 535/1072, Loss: 7.5016\n",
      "Iteration 536/1072, Loss: 7.6830\n",
      "Iteration 537/1072, Loss: 7.6331\n",
      "Iteration 538/1072, Loss: 7.5912\n",
      "Iteration 539/1072, Loss: 7.6366\n",
      "Iteration 540/1072, Loss: 7.6632\n",
      "Iteration 541/1072, Loss: 7.6359\n",
      "Iteration 542/1072, Loss: 7.6271\n",
      "Iteration 543/1072, Loss: 7.6179\n",
      "Iteration 544/1072, Loss: 7.5464\n",
      "Iteration 545/1072, Loss: 7.6248\n",
      "Iteration 546/1072, Loss: 7.6771\n",
      "Iteration 547/1072, Loss: 7.5673\n",
      "Iteration 548/1072, Loss: 7.6847\n",
      "Iteration 549/1072, Loss: 7.6619\n",
      "Iteration 550/1072, Loss: 7.6036\n",
      "Iteration 551/1072, Loss: 7.6464\n",
      "Iteration 552/1072, Loss: 7.6466\n",
      "Iteration 553/1072, Loss: 7.6133\n",
      "Iteration 554/1072, Loss: 7.7534\n",
      "Iteration 555/1072, Loss: 7.7212\n",
      "Iteration 556/1072, Loss: 7.6439\n",
      "Iteration 557/1072, Loss: 7.6208\n",
      "Iteration 558/1072, Loss: 7.6449\n",
      "Iteration 559/1072, Loss: 7.7487\n",
      "Iteration 560/1072, Loss: 7.6287\n",
      "Iteration 561/1072, Loss: 7.6852\n",
      "Iteration 562/1072, Loss: 7.6243\n",
      "Iteration 563/1072, Loss: 7.7225\n",
      "Iteration 564/1072, Loss: 7.6933\n",
      "Iteration 565/1072, Loss: 7.7229\n",
      "Iteration 566/1072, Loss: 7.7556\n",
      "Iteration 567/1072, Loss: 7.6439\n",
      "Iteration 568/1072, Loss: 7.6879\n",
      "Iteration 569/1072, Loss: 7.6033\n",
      "Iteration 570/1072, Loss: 7.6882\n",
      "Iteration 571/1072, Loss: 7.6792\n",
      "Iteration 572/1072, Loss: 7.6845\n",
      "Iteration 573/1072, Loss: 7.7543\n",
      "Iteration 574/1072, Loss: 7.5831\n",
      "Iteration 575/1072, Loss: 7.6888\n",
      "Iteration 576/1072, Loss: 7.6226\n",
      "Iteration 577/1072, Loss: 7.6759\n",
      "Iteration 578/1072, Loss: 7.5072\n",
      "Iteration 579/1072, Loss: 7.6368\n",
      "Iteration 580/1072, Loss: 7.6841\n",
      "Iteration 581/1072, Loss: 7.6308\n",
      "Iteration 582/1072, Loss: 7.6253\n",
      "Iteration 583/1072, Loss: 7.5861\n",
      "Iteration 584/1072, Loss: 7.6062\n",
      "Iteration 585/1072, Loss: 7.6586\n",
      "Iteration 586/1072, Loss: 7.5289\n",
      "Iteration 587/1072, Loss: 7.6496\n",
      "Iteration 588/1072, Loss: 7.6969\n",
      "Iteration 589/1072, Loss: 7.6683\n",
      "Iteration 590/1072, Loss: 7.6710\n",
      "Iteration 591/1072, Loss: 7.6630\n",
      "Iteration 592/1072, Loss: 7.7484\n",
      "Iteration 593/1072, Loss: 7.7249\n",
      "Iteration 594/1072, Loss: 7.6373\n",
      "Iteration 595/1072, Loss: 7.7242\n",
      "Iteration 596/1072, Loss: 7.6168\n",
      "Iteration 597/1072, Loss: 7.6152\n",
      "Iteration 598/1072, Loss: 7.5842\n",
      "Iteration 599/1072, Loss: 7.6850\n",
      "Iteration 600/1072, Loss: 7.5705\n",
      "Iteration 601/1072, Loss: 7.6672\n",
      "Iteration 602/1072, Loss: 7.6458\n",
      "Iteration 603/1072, Loss: 7.5936\n",
      "Iteration 604/1072, Loss: 7.5923\n",
      "Iteration 605/1072, Loss: 7.5802\n",
      "Iteration 606/1072, Loss: 7.5837\n",
      "Iteration 607/1072, Loss: 7.6659\n",
      "Iteration 608/1072, Loss: 7.6897\n",
      "Iteration 609/1072, Loss: 7.6630\n",
      "Iteration 610/1072, Loss: 7.6883\n",
      "Iteration 611/1072, Loss: 7.6000\n",
      "Iteration 612/1072, Loss: 7.6888\n",
      "Iteration 613/1072, Loss: 7.7482\n",
      "Iteration 614/1072, Loss: 7.5490\n",
      "Iteration 615/1072, Loss: 7.4912\n",
      "Iteration 616/1072, Loss: 7.6505\n",
      "Iteration 617/1072, Loss: 7.6285\n",
      "Iteration 618/1072, Loss: 7.6451\n",
      "Iteration 619/1072, Loss: 7.6774\n",
      "Iteration 620/1072, Loss: 7.5484\n",
      "Iteration 621/1072, Loss: 7.5682\n",
      "Iteration 622/1072, Loss: 7.5236\n",
      "Iteration 623/1072, Loss: 7.6493\n",
      "Iteration 624/1072, Loss: 7.6505\n",
      "Iteration 625/1072, Loss: 7.6418\n",
      "Iteration 626/1072, Loss: 7.6808\n",
      "Iteration 627/1072, Loss: 7.6423\n",
      "Iteration 628/1072, Loss: 7.6781\n",
      "Iteration 629/1072, Loss: 7.7208\n",
      "Iteration 630/1072, Loss: 7.7711\n",
      "Iteration 631/1072, Loss: 7.6821\n",
      "Iteration 632/1072, Loss: 7.5803\n",
      "Iteration 633/1072, Loss: 7.6504\n",
      "Iteration 634/1072, Loss: 7.6158\n",
      "Iteration 635/1072, Loss: 7.6175\n",
      "Iteration 636/1072, Loss: 7.6786\n",
      "Iteration 637/1072, Loss: 7.6233\n",
      "Iteration 638/1072, Loss: 7.5514\n",
      "Iteration 639/1072, Loss: 7.6391\n",
      "Iteration 640/1072, Loss: 7.6357\n",
      "Iteration 641/1072, Loss: 7.7117\n",
      "Iteration 642/1072, Loss: 7.6278\n",
      "Iteration 643/1072, Loss: 7.7852\n",
      "Iteration 644/1072, Loss: 7.7159\n",
      "Iteration 645/1072, Loss: 7.6143\n",
      "Iteration 646/1072, Loss: 7.5223\n",
      "Iteration 647/1072, Loss: 7.5455\n",
      "Iteration 648/1072, Loss: 7.6786\n",
      "Iteration 649/1072, Loss: 7.7415\n",
      "Iteration 650/1072, Loss: 7.5653\n",
      "Iteration 651/1072, Loss: 7.6402\n",
      "Iteration 652/1072, Loss: 7.6370\n",
      "Iteration 653/1072, Loss: 7.6339\n",
      "Iteration 654/1072, Loss: 7.5281\n",
      "Iteration 655/1072, Loss: 7.5735\n",
      "Iteration 656/1072, Loss: 7.7061\n",
      "Iteration 657/1072, Loss: 7.6982\n",
      "Iteration 658/1072, Loss: 7.6666\n",
      "Iteration 659/1072, Loss: 7.6730\n",
      "Iteration 660/1072, Loss: 7.6549\n",
      "Iteration 661/1072, Loss: 7.6612\n",
      "Iteration 662/1072, Loss: 7.6498\n",
      "Iteration 663/1072, Loss: 7.6806\n",
      "Iteration 664/1072, Loss: 7.6819\n",
      "Iteration 665/1072, Loss: 7.7185\n",
      "Iteration 666/1072, Loss: 7.6415\n",
      "Iteration 667/1072, Loss: 7.7263\n",
      "Iteration 668/1072, Loss: 7.5489\n",
      "Iteration 669/1072, Loss: 7.6889\n",
      "Iteration 670/1072, Loss: 7.7763\n",
      "Iteration 671/1072, Loss: 7.5853\n",
      "Iteration 672/1072, Loss: 7.6702\n",
      "Iteration 673/1072, Loss: 7.6846\n",
      "Iteration 674/1072, Loss: 7.6712\n",
      "Iteration 675/1072, Loss: 7.4712\n",
      "Iteration 676/1072, Loss: 7.6381\n",
      "Iteration 677/1072, Loss: 7.5387\n",
      "Iteration 678/1072, Loss: 7.5739\n",
      "Iteration 679/1072, Loss: 7.6645\n",
      "Iteration 680/1072, Loss: 7.6897\n",
      "Iteration 681/1072, Loss: 7.6973\n",
      "Iteration 682/1072, Loss: 7.7058\n",
      "Iteration 683/1072, Loss: 7.6040\n",
      "Iteration 684/1072, Loss: 7.4396\n",
      "Iteration 685/1072, Loss: 7.7080\n",
      "Iteration 686/1072, Loss: 7.7343\n",
      "Iteration 687/1072, Loss: 7.6448\n",
      "Iteration 688/1072, Loss: 7.6197\n",
      "Iteration 689/1072, Loss: 7.4796\n",
      "Iteration 690/1072, Loss: 7.6471\n",
      "Iteration 691/1072, Loss: 7.6312\n",
      "Iteration 692/1072, Loss: 7.7316\n",
      "Iteration 693/1072, Loss: 7.4899\n",
      "Iteration 694/1072, Loss: 7.5513\n",
      "Iteration 695/1072, Loss: 7.5481\n",
      "Iteration 696/1072, Loss: 7.6528\n",
      "Iteration 697/1072, Loss: 7.7739\n",
      "Iteration 698/1072, Loss: 7.6229\n",
      "Iteration 699/1072, Loss: 7.6219\n",
      "Iteration 700/1072, Loss: 7.6766\n",
      "Iteration 701/1072, Loss: 7.5596\n",
      "Iteration 702/1072, Loss: 7.5399\n",
      "Iteration 703/1072, Loss: 7.6078\n",
      "Iteration 704/1072, Loss: 7.6871\n",
      "Iteration 705/1072, Loss: 7.7198\n",
      "Iteration 706/1072, Loss: 7.6745\n",
      "Iteration 707/1072, Loss: 7.7192\n",
      "Iteration 708/1072, Loss: 7.6609\n",
      "Iteration 709/1072, Loss: 7.6922\n",
      "Iteration 710/1072, Loss: 7.7004\n",
      "Iteration 711/1072, Loss: 7.5670\n",
      "Iteration 712/1072, Loss: 7.7033\n",
      "Iteration 713/1072, Loss: 7.5604\n",
      "Iteration 714/1072, Loss: 7.6708\n",
      "Iteration 715/1072, Loss: 7.5759\n",
      "Iteration 716/1072, Loss: 7.6755\n",
      "Iteration 717/1072, Loss: 7.6749\n",
      "Iteration 718/1072, Loss: 7.6807\n",
      "Iteration 719/1072, Loss: 7.4704\n",
      "Iteration 720/1072, Loss: 7.6154\n",
      "Iteration 721/1072, Loss: 7.6661\n",
      "Iteration 722/1072, Loss: 7.6584\n",
      "Iteration 723/1072, Loss: 7.6746\n",
      "Iteration 724/1072, Loss: 7.6311\n",
      "Iteration 725/1072, Loss: 7.6878\n",
      "Iteration 726/1072, Loss: 7.5921\n",
      "Iteration 727/1072, Loss: 7.5900\n",
      "Iteration 728/1072, Loss: 7.6019\n",
      "Iteration 729/1072, Loss: 7.6290\n",
      "Iteration 730/1072, Loss: 7.6143\n",
      "Iteration 731/1072, Loss: 7.6840\n",
      "Iteration 732/1072, Loss: 7.7160\n",
      "Iteration 733/1072, Loss: 7.5658\n",
      "Iteration 734/1072, Loss: 7.5780\n",
      "Iteration 735/1072, Loss: 7.6027\n",
      "Iteration 736/1072, Loss: 7.5410\n",
      "Iteration 737/1072, Loss: 7.5622\n",
      "Iteration 738/1072, Loss: 7.5897\n",
      "Iteration 739/1072, Loss: 7.6760\n",
      "Iteration 740/1072, Loss: 7.7450\n",
      "Iteration 741/1072, Loss: 7.7296\n",
      "Iteration 742/1072, Loss: 7.6238\n",
      "Iteration 743/1072, Loss: 7.5655\n",
      "Iteration 744/1072, Loss: 7.6157\n",
      "Iteration 745/1072, Loss: 7.7634\n",
      "Iteration 746/1072, Loss: 7.6259\n",
      "Iteration 747/1072, Loss: 7.6198\n",
      "Iteration 748/1072, Loss: 7.5588\n",
      "Iteration 749/1072, Loss: 7.5268\n",
      "Iteration 750/1072, Loss: 7.6205\n",
      "Iteration 751/1072, Loss: 7.5768\n",
      "Iteration 752/1072, Loss: 7.7561\n",
      "Iteration 753/1072, Loss: 7.6652\n",
      "Iteration 754/1072, Loss: 7.6549\n",
      "Iteration 755/1072, Loss: 7.5636\n",
      "Iteration 756/1072, Loss: 7.5947\n",
      "Iteration 757/1072, Loss: 7.6313\n",
      "Iteration 758/1072, Loss: 7.7040\n",
      "Iteration 759/1072, Loss: 7.5679\n",
      "Iteration 760/1072, Loss: 7.5276\n",
      "Iteration 761/1072, Loss: 7.6216\n",
      "Iteration 762/1072, Loss: 7.6455\n",
      "Iteration 763/1072, Loss: 7.7369\n",
      "Iteration 764/1072, Loss: 7.6290\n",
      "Iteration 765/1072, Loss: 7.7508\n",
      "Iteration 766/1072, Loss: 7.6215\n",
      "Iteration 767/1072, Loss: 7.6997\n",
      "Iteration 768/1072, Loss: 7.6124\n",
      "Iteration 769/1072, Loss: 7.6629\n",
      "Iteration 770/1072, Loss: 7.6534\n",
      "Iteration 771/1072, Loss: 7.7288\n",
      "Iteration 772/1072, Loss: 7.8124\n",
      "Iteration 773/1072, Loss: 7.6827\n",
      "Iteration 774/1072, Loss: 7.5336\n",
      "Iteration 775/1072, Loss: 7.6099\n",
      "Iteration 776/1072, Loss: 7.5987\n",
      "Iteration 777/1072, Loss: 7.6136\n",
      "Iteration 778/1072, Loss: 7.5473\n",
      "Iteration 779/1072, Loss: 7.6365\n",
      "Iteration 780/1072, Loss: 7.6057\n",
      "Iteration 781/1072, Loss: 7.6346\n",
      "Iteration 782/1072, Loss: 7.6127\n",
      "Iteration 783/1072, Loss: 7.5816\n",
      "Iteration 784/1072, Loss: 7.6111\n",
      "Iteration 785/1072, Loss: 7.6375\n",
      "Iteration 786/1072, Loss: 7.5523\n",
      "Iteration 787/1072, Loss: 7.6880\n",
      "Iteration 788/1072, Loss: 7.6630\n",
      "Iteration 789/1072, Loss: 7.5959\n",
      "Iteration 790/1072, Loss: 7.4480\n",
      "Iteration 791/1072, Loss: 7.6396\n",
      "Iteration 792/1072, Loss: 7.5539\n",
      "Iteration 793/1072, Loss: 7.5202\n",
      "Iteration 794/1072, Loss: 7.5752\n",
      "Iteration 795/1072, Loss: 7.5887\n",
      "Iteration 796/1072, Loss: 7.5420\n",
      "Iteration 797/1072, Loss: 7.6414\n",
      "Iteration 798/1072, Loss: 7.6606\n",
      "Iteration 799/1072, Loss: 7.6608\n",
      "Iteration 800/1072, Loss: 7.6124\n",
      "Iteration 801/1072, Loss: 7.7875\n",
      "Iteration 802/1072, Loss: 7.5784\n",
      "Iteration 803/1072, Loss: 7.7178\n",
      "Iteration 804/1072, Loss: 7.6374\n",
      "Iteration 805/1072, Loss: 7.6196\n",
      "Iteration 806/1072, Loss: 7.5648\n",
      "Iteration 807/1072, Loss: 7.5869\n",
      "Iteration 808/1072, Loss: 7.6910\n",
      "Iteration 809/1072, Loss: 7.7412\n",
      "Iteration 810/1072, Loss: 7.5748\n",
      "Iteration 811/1072, Loss: 7.6709\n",
      "Iteration 812/1072, Loss: 7.5605\n",
      "Iteration 813/1072, Loss: 7.5372\n",
      "Iteration 814/1072, Loss: 7.5836\n",
      "Iteration 815/1072, Loss: 7.7075\n",
      "Iteration 816/1072, Loss: 7.5276\n",
      "Iteration 817/1072, Loss: 7.6693\n",
      "Iteration 818/1072, Loss: 7.6944\n",
      "Iteration 819/1072, Loss: 7.6982\n",
      "Iteration 820/1072, Loss: 7.6208\n",
      "Iteration 821/1072, Loss: 7.6291\n",
      "Iteration 822/1072, Loss: 7.6499\n",
      "Iteration 823/1072, Loss: 7.6854\n",
      "Iteration 824/1072, Loss: 7.6447\n",
      "Iteration 825/1072, Loss: 7.5790\n",
      "Iteration 826/1072, Loss: 7.5866\n",
      "Iteration 827/1072, Loss: 7.6986\n",
      "Iteration 828/1072, Loss: 7.6972\n",
      "Iteration 829/1072, Loss: 7.6544\n",
      "Iteration 830/1072, Loss: 7.5386\n",
      "Iteration 831/1072, Loss: 7.6902\n",
      "Iteration 832/1072, Loss: 7.6666\n",
      "Iteration 833/1072, Loss: 7.6634\n",
      "Iteration 834/1072, Loss: 7.5784\n",
      "Iteration 835/1072, Loss: 7.6803\n",
      "Iteration 836/1072, Loss: 7.5973\n",
      "Iteration 837/1072, Loss: 7.6662\n",
      "Iteration 838/1072, Loss: 7.5811\n",
      "Iteration 839/1072, Loss: 7.6378\n",
      "Iteration 840/1072, Loss: 7.7334\n",
      "Iteration 841/1072, Loss: 7.6260\n",
      "Iteration 842/1072, Loss: 7.6432\n",
      "Iteration 843/1072, Loss: 7.6471\n",
      "Iteration 844/1072, Loss: 7.7293\n",
      "Iteration 845/1072, Loss: 7.6847\n",
      "Iteration 846/1072, Loss: 7.6460\n",
      "Iteration 847/1072, Loss: 7.6175\n",
      "Iteration 848/1072, Loss: 7.6690\n",
      "Iteration 849/1072, Loss: 7.6050\n",
      "Iteration 850/1072, Loss: 7.6297\n",
      "Iteration 851/1072, Loss: 7.7144\n",
      "Iteration 852/1072, Loss: 7.6004\n",
      "Iteration 853/1072, Loss: 7.6855\n",
      "Iteration 854/1072, Loss: 7.6103\n",
      "Iteration 855/1072, Loss: 7.6266\n",
      "Iteration 856/1072, Loss: 7.6518\n",
      "Iteration 857/1072, Loss: 7.7421\n",
      "Iteration 858/1072, Loss: 7.5451\n",
      "Iteration 859/1072, Loss: 7.6471\n",
      "Iteration 860/1072, Loss: 7.6566\n",
      "Iteration 861/1072, Loss: 7.5126\n",
      "Iteration 862/1072, Loss: 7.5635\n",
      "Iteration 863/1072, Loss: 7.5141\n",
      "Iteration 864/1072, Loss: 7.7255\n",
      "Iteration 865/1072, Loss: 7.6629\n",
      "Iteration 866/1072, Loss: 7.6136\n",
      "Iteration 867/1072, Loss: 7.6094\n",
      "Iteration 868/1072, Loss: 7.5797\n",
      "Iteration 869/1072, Loss: 7.6364\n",
      "Iteration 870/1072, Loss: 7.5517\n",
      "Iteration 871/1072, Loss: 7.6869\n",
      "Iteration 872/1072, Loss: 7.7073\n",
      "Iteration 873/1072, Loss: 7.5674\n",
      "Iteration 874/1072, Loss: 7.5349\n",
      "Iteration 875/1072, Loss: 7.5421\n",
      "Iteration 876/1072, Loss: 7.5927\n",
      "Iteration 877/1072, Loss: 7.6741\n",
      "Iteration 878/1072, Loss: 7.6436\n",
      "Iteration 879/1072, Loss: 7.5702\n",
      "Iteration 880/1072, Loss: 7.6626\n",
      "Iteration 881/1072, Loss: 7.4984\n",
      "Iteration 882/1072, Loss: 7.5693\n",
      "Iteration 883/1072, Loss: 7.5756\n",
      "Iteration 884/1072, Loss: 7.6891\n",
      "Iteration 885/1072, Loss: 7.6584\n",
      "Iteration 886/1072, Loss: 7.6103\n",
      "Iteration 887/1072, Loss: 7.7041\n",
      "Iteration 888/1072, Loss: 7.5487\n",
      "Iteration 889/1072, Loss: 7.6129\n",
      "Iteration 890/1072, Loss: 7.7235\n",
      "Iteration 891/1072, Loss: 7.5893\n",
      "Iteration 892/1072, Loss: 7.7579\n",
      "Iteration 893/1072, Loss: 7.6252\n",
      "Iteration 894/1072, Loss: 7.7055\n",
      "Iteration 895/1072, Loss: 7.5835\n",
      "Iteration 896/1072, Loss: 7.7025\n",
      "Iteration 897/1072, Loss: 7.6753\n",
      "Iteration 898/1072, Loss: 7.4956\n",
      "Iteration 899/1072, Loss: 7.6448\n",
      "Iteration 900/1072, Loss: 7.6890\n",
      "Iteration 901/1072, Loss: 7.5205\n",
      "Iteration 902/1072, Loss: 7.6271\n",
      "Iteration 903/1072, Loss: 7.7656\n",
      "Iteration 904/1072, Loss: 7.6811\n",
      "Iteration 905/1072, Loss: 7.7180\n",
      "Iteration 906/1072, Loss: 7.6513\n",
      "Iteration 907/1072, Loss: 7.5394\n",
      "Iteration 908/1072, Loss: 7.6515\n",
      "Iteration 909/1072, Loss: 7.6488\n",
      "Iteration 910/1072, Loss: 7.6309\n",
      "Iteration 911/1072, Loss: 7.5351\n",
      "Iteration 912/1072, Loss: 7.4974\n",
      "Iteration 913/1072, Loss: 7.6424\n",
      "Iteration 914/1072, Loss: 7.6670\n",
      "Iteration 915/1072, Loss: 7.6067\n",
      "Iteration 916/1072, Loss: 7.6441\n",
      "Iteration 917/1072, Loss: 7.6042\n",
      "Iteration 918/1072, Loss: 7.7348\n",
      "Iteration 919/1072, Loss: 7.6161\n",
      "Iteration 920/1072, Loss: 7.5965\n",
      "Iteration 921/1072, Loss: 7.5841\n",
      "Iteration 922/1072, Loss: 7.6600\n",
      "Iteration 923/1072, Loss: 7.6407\n",
      "Iteration 924/1072, Loss: 7.6065\n",
      "Iteration 925/1072, Loss: 7.6290\n",
      "Iteration 926/1072, Loss: 7.5728\n",
      "Iteration 927/1072, Loss: 7.6128\n",
      "Iteration 928/1072, Loss: 7.6646\n",
      "Iteration 929/1072, Loss: 7.7090\n",
      "Iteration 930/1072, Loss: 7.7123\n",
      "Iteration 931/1072, Loss: 7.6007\n",
      "Iteration 932/1072, Loss: 7.6015\n",
      "Iteration 933/1072, Loss: 7.6401\n",
      "Iteration 934/1072, Loss: 7.6295\n",
      "Iteration 935/1072, Loss: 7.5692\n",
      "Iteration 936/1072, Loss: 7.6729\n",
      "Iteration 937/1072, Loss: 7.6727\n",
      "Iteration 938/1072, Loss: 7.5715\n",
      "Iteration 939/1072, Loss: 7.6534\n",
      "Iteration 940/1072, Loss: 7.4588\n",
      "Iteration 941/1072, Loss: 7.6228\n",
      "Iteration 942/1072, Loss: 7.6181\n",
      "Iteration 943/1072, Loss: 7.5829\n",
      "Iteration 944/1072, Loss: 7.6514\n",
      "Iteration 945/1072, Loss: 7.6260\n",
      "Iteration 946/1072, Loss: 7.6744\n",
      "Iteration 947/1072, Loss: 7.7577\n",
      "Iteration 948/1072, Loss: 7.5709\n",
      "Iteration 949/1072, Loss: 7.6217\n",
      "Iteration 950/1072, Loss: 7.5281\n",
      "Iteration 951/1072, Loss: 7.6175\n",
      "Iteration 952/1072, Loss: 7.6239\n",
      "Iteration 953/1072, Loss: 7.5441\n",
      "Iteration 954/1072, Loss: 7.6586\n",
      "Iteration 955/1072, Loss: 7.5800\n",
      "Iteration 956/1072, Loss: 7.6640\n",
      "Iteration 957/1072, Loss: 7.5602\n",
      "Iteration 958/1072, Loss: 7.6304\n",
      "Iteration 959/1072, Loss: 7.5629\n",
      "Iteration 960/1072, Loss: 7.7471\n",
      "Iteration 961/1072, Loss: 7.6861\n",
      "Iteration 962/1072, Loss: 7.6552\n",
      "Iteration 963/1072, Loss: 7.6903\n",
      "Iteration 964/1072, Loss: 7.7082\n",
      "Iteration 965/1072, Loss: 7.6099\n",
      "Iteration 966/1072, Loss: 7.5821\n",
      "Iteration 967/1072, Loss: 7.5774\n",
      "Iteration 968/1072, Loss: 7.6508\n",
      "Iteration 969/1072, Loss: 7.7082\n",
      "Iteration 970/1072, Loss: 7.6135\n",
      "Iteration 971/1072, Loss: 7.6667\n",
      "Iteration 972/1072, Loss: 7.6157\n",
      "Iteration 973/1072, Loss: 7.6327\n",
      "Iteration 974/1072, Loss: 7.5119\n",
      "Iteration 975/1072, Loss: 7.6210\n",
      "Iteration 976/1072, Loss: 7.6842\n",
      "Iteration 977/1072, Loss: 7.6524\n",
      "Iteration 978/1072, Loss: 7.6448\n",
      "Iteration 979/1072, Loss: 7.6639\n",
      "Iteration 980/1072, Loss: 7.5498\n",
      "Iteration 981/1072, Loss: 7.5999\n",
      "Iteration 982/1072, Loss: 7.6610\n",
      "Iteration 983/1072, Loss: 7.5451\n",
      "Iteration 984/1072, Loss: 7.6514\n",
      "Iteration 985/1072, Loss: 7.4931\n",
      "Iteration 986/1072, Loss: 7.7323\n",
      "Iteration 987/1072, Loss: 7.5697\n",
      "Iteration 988/1072, Loss: 7.6785\n",
      "Iteration 989/1072, Loss: 7.6866\n",
      "Iteration 990/1072, Loss: 7.6597\n",
      "Iteration 991/1072, Loss: 7.7548\n",
      "Iteration 992/1072, Loss: 7.3957\n",
      "Iteration 993/1072, Loss: 7.6841\n",
      "Iteration 994/1072, Loss: 7.6721\n",
      "Iteration 995/1072, Loss: 7.6515\n",
      "Iteration 996/1072, Loss: 7.6016\n",
      "Iteration 997/1072, Loss: 7.6102\n",
      "Iteration 998/1072, Loss: 7.6584\n",
      "Iteration 999/1072, Loss: 7.5540\n",
      "Iteration 1000/1072, Loss: 7.4726\n",
      "Iteration 1001/1072, Loss: 7.6663\n",
      "Iteration 1002/1072, Loss: 7.6360\n",
      "Iteration 1003/1072, Loss: 7.5883\n",
      "Iteration 1004/1072, Loss: 7.6394\n",
      "Iteration 1005/1072, Loss: 7.6057\n",
      "Iteration 1006/1072, Loss: 7.5496\n",
      "Iteration 1007/1072, Loss: 7.6577\n",
      "Iteration 1008/1072, Loss: 7.6446\n",
      "Iteration 1009/1072, Loss: 7.6450\n",
      "Iteration 1010/1072, Loss: 7.5965\n",
      "Iteration 1011/1072, Loss: 7.5916\n",
      "Iteration 1012/1072, Loss: 7.4503\n",
      "Iteration 1013/1072, Loss: 7.5840\n",
      "Iteration 1014/1072, Loss: 7.5350\n",
      "Iteration 1015/1072, Loss: 7.6015\n",
      "Iteration 1016/1072, Loss: 7.6167\n",
      "Iteration 1017/1072, Loss: 7.6326\n",
      "Iteration 1018/1072, Loss: 7.6260\n",
      "Iteration 1019/1072, Loss: 7.7275\n",
      "Iteration 1020/1072, Loss: 7.6171\n",
      "Iteration 1021/1072, Loss: 7.6686\n",
      "Iteration 1022/1072, Loss: 7.6006\n",
      "Iteration 1023/1072, Loss: 7.6493\n",
      "Iteration 1024/1072, Loss: 7.6462\n",
      "Iteration 1025/1072, Loss: 7.5821\n",
      "Iteration 1026/1072, Loss: 7.6549\n",
      "Iteration 1027/1072, Loss: 7.6360\n",
      "Iteration 1028/1072, Loss: 7.5758\n",
      "Iteration 1029/1072, Loss: 7.5722\n",
      "Iteration 1030/1072, Loss: 7.5952\n",
      "Iteration 1031/1072, Loss: 7.5618\n",
      "Iteration 1032/1072, Loss: 7.6697\n",
      "Iteration 1033/1072, Loss: 7.6104\n",
      "Iteration 1034/1072, Loss: 7.6323\n",
      "Iteration 1035/1072, Loss: 7.5144\n",
      "Iteration 1036/1072, Loss: 7.5988\n",
      "Iteration 1037/1072, Loss: 7.6910\n",
      "Iteration 1038/1072, Loss: 7.5885\n",
      "Iteration 1039/1072, Loss: 7.6391\n",
      "Iteration 1040/1072, Loss: 7.6870\n",
      "Iteration 1041/1072, Loss: 7.6254\n",
      "Iteration 1042/1072, Loss: 7.5051\n",
      "Iteration 1043/1072, Loss: 7.6162\n",
      "Iteration 1044/1072, Loss: 7.5975\n",
      "Iteration 1045/1072, Loss: 7.6057\n",
      "Iteration 1046/1072, Loss: 7.5791\n",
      "Iteration 1047/1072, Loss: 7.6008\n",
      "Iteration 1048/1072, Loss: 7.6311\n",
      "Iteration 1049/1072, Loss: 7.5650\n",
      "Iteration 1050/1072, Loss: 7.5135\n",
      "Iteration 1051/1072, Loss: 7.5997\n",
      "Iteration 1052/1072, Loss: 7.7558\n",
      "Iteration 1053/1072, Loss: 7.5852\n",
      "Iteration 1054/1072, Loss: 7.5820\n",
      "Iteration 1055/1072, Loss: 7.6730\n",
      "Iteration 1056/1072, Loss: 7.6728\n",
      "Iteration 1057/1072, Loss: 7.4812\n",
      "Iteration 1058/1072, Loss: 7.7315\n",
      "Iteration 1059/1072, Loss: 7.6279\n",
      "Iteration 1060/1072, Loss: 7.6326\n",
      "Iteration 1061/1072, Loss: 7.6456\n",
      "Iteration 1062/1072, Loss: 7.7000\n",
      "Iteration 1063/1072, Loss: 7.5705\n",
      "Iteration 1064/1072, Loss: 7.5493\n",
      "Iteration 1065/1072, Loss: 7.6545\n",
      "Iteration 1066/1072, Loss: 7.6056\n",
      "Iteration 1067/1072, Loss: 7.5668\n",
      "Iteration 1068/1072, Loss: 7.6190\n",
      "Iteration 1069/1072, Loss: 7.6703\n",
      "Iteration 1070/1072, Loss: 7.6044\n",
      "Iteration 1071/1072, Loss: 7.6138\n",
      "Iteration 1072/1072, Loss: 7.9845\n",
      "Epoch 7/10, Loss: 7.6496\n",
      "Checkpoint saved at checkpoints/efficientnet-b4_epoch_7.pth\n",
      "Validation Accuracy: 3.86%\n",
      "Iteration 1/1072, Loss: 7.5894\n",
      "Iteration 2/1072, Loss: 7.5516\n",
      "Iteration 3/1072, Loss: 7.6731\n",
      "Iteration 4/1072, Loss: 7.8455\n",
      "Iteration 48/1072, Loss: 7.5562\n",
      "Iteration 49/1072, Loss: 7.6595\n",
      "Iteration 50/1072, Loss: 7.5582\n",
      "Iteration 51/1072, Loss: 7.6506\n",
      "Iteration 52/1072, Loss: 7.5298\n",
      "Iteration 53/1072, Loss: 7.5925\n",
      "Iteration 54/1072, Loss: 7.5987\n",
      "Iteration 55/1072, Loss: 7.4748\n",
      "Iteration 56/1072, Loss: 7.5082\n",
      "Iteration 57/1072, Loss: 7.5742\n",
      "Iteration 58/1072, Loss: 7.6486\n",
      "Iteration 59/1072, Loss: 7.4790\n",
      "Iteration 60/1072, Loss: 7.6048\n",
      "Iteration 61/1072, Loss: 7.6044\n",
      "Iteration 62/1072, Loss: 7.4088\n",
      "Iteration 63/1072, Loss: 7.4665\n",
      "Iteration 64/1072, Loss: 7.5989\n",
      "Iteration 65/1072, Loss: 7.6242\n",
      "Iteration 66/1072, Loss: 7.5697\n",
      "Iteration 67/1072, Loss: 7.4910\n",
      "Iteration 68/1072, Loss: 7.2945\n",
      "Iteration 69/1072, Loss: 7.5396\n",
      "Iteration 70/1072, Loss: 7.6467\n",
      "Iteration 71/1072, Loss: 7.6081\n",
      "Iteration 72/1072, Loss: 7.5143\n",
      "Iteration 73/1072, Loss: 7.5669\n",
      "Iteration 74/1072, Loss: 7.6092\n",
      "Iteration 75/1072, Loss: 7.6128\n",
      "Iteration 76/1072, Loss: 7.6163\n",
      "Iteration 77/1072, Loss: 7.5346\n",
      "Iteration 78/1072, Loss: 7.5320\n",
      "Iteration 79/1072, Loss: 7.5262\n",
      "Iteration 80/1072, Loss: 7.5675\n",
      "Iteration 81/1072, Loss: 7.4833\n",
      "Iteration 82/1072, Loss: 7.5640\n",
      "Iteration 83/1072, Loss: 7.6665\n",
      "Iteration 84/1072, Loss: 7.6345\n",
      "Iteration 85/1072, Loss: 7.5926\n",
      "Iteration 86/1072, Loss: 7.5099\n",
      "Iteration 87/1072, Loss: 7.4422\n",
      "Iteration 88/1072, Loss: 7.4718\n",
      "Iteration 89/1072, Loss: 7.4984\n",
      "Iteration 90/1072, Loss: 7.5540\n",
      "Iteration 91/1072, Loss: 7.6362\n",
      "Iteration 92/1072, Loss: 7.6348\n",
      "Iteration 93/1072, Loss: 7.5965\n",
      "Iteration 94/1072, Loss: 7.5383\n",
      "Iteration 95/1072, Loss: 7.6599\n",
      "Iteration 96/1072, Loss: 7.6037\n",
      "Iteration 97/1072, Loss: 7.6259\n",
      "Iteration 98/1072, Loss: 7.5442\n",
      "Iteration 99/1072, Loss: 7.4778\n",
      "Iteration 100/1072, Loss: 7.6999\n",
      "Iteration 101/1072, Loss: 7.5241\n",
      "Iteration 102/1072, Loss: 7.5659\n",
      "Iteration 103/1072, Loss: 7.6576\n",
      "Iteration 104/1072, Loss: 7.3950\n",
      "Iteration 105/1072, Loss: 7.5291\n",
      "Iteration 106/1072, Loss: 7.5156\n",
      "Iteration 107/1072, Loss: 7.4831\n",
      "Iteration 108/1072, Loss: 7.5835\n",
      "Iteration 109/1072, Loss: 7.4284\n",
      "Iteration 110/1072, Loss: 7.5274\n",
      "Iteration 111/1072, Loss: 7.5847\n",
      "Iteration 112/1072, Loss: 7.6009\n",
      "Iteration 113/1072, Loss: 7.5736\n",
      "Iteration 114/1072, Loss: 7.6452\n",
      "Iteration 115/1072, Loss: 7.6263\n",
      "Iteration 116/1072, Loss: 7.6625\n",
      "Iteration 117/1072, Loss: 7.5682\n",
      "Iteration 118/1072, Loss: 7.6044\n",
      "Iteration 119/1072, Loss: 7.7196\n",
      "Iteration 120/1072, Loss: 7.4669\n",
      "Iteration 121/1072, Loss: 7.6468\n",
      "Iteration 122/1072, Loss: 7.5666\n",
      "Iteration 123/1072, Loss: 7.5146\n",
      "Iteration 124/1072, Loss: 7.5832\n",
      "Iteration 125/1072, Loss: 7.5901\n",
      "Iteration 126/1072, Loss: 7.5104\n",
      "Iteration 127/1072, Loss: 7.6192\n",
      "Iteration 128/1072, Loss: 7.5144\n",
      "Iteration 129/1072, Loss: 7.5541\n",
      "Iteration 130/1072, Loss: 7.6619\n",
      "Iteration 131/1072, Loss: 7.6885\n",
      "Iteration 132/1072, Loss: 7.5454\n",
      "Iteration 133/1072, Loss: 7.4938\n",
      "Iteration 134/1072, Loss: 7.5718\n",
      "Iteration 135/1072, Loss: 7.6053\n",
      "Iteration 136/1072, Loss: 7.5947\n",
      "Iteration 137/1072, Loss: 7.5450\n",
      "Iteration 138/1072, Loss: 7.5332\n",
      "Iteration 139/1072, Loss: 7.5966\n",
      "Iteration 140/1072, Loss: 7.5832\n",
      "Iteration 141/1072, Loss: 7.6362\n",
      "Iteration 142/1072, Loss: 7.6407\n",
      "Iteration 143/1072, Loss: 7.6063\n",
      "Iteration 144/1072, Loss: 7.5196\n",
      "Iteration 145/1072, Loss: 7.4824\n",
      "Iteration 146/1072, Loss: 7.6009\n",
      "Iteration 147/1072, Loss: 7.6228\n",
      "Iteration 148/1072, Loss: 7.5558\n",
      "Iteration 149/1072, Loss: 7.4817\n",
      "Iteration 150/1072, Loss: 7.5457\n",
      "Iteration 151/1072, Loss: 7.5797\n",
      "Iteration 152/1072, Loss: 7.5794\n",
      "Iteration 153/1072, Loss: 7.4667\n",
      "Iteration 154/1072, Loss: 7.5935\n",
      "Iteration 155/1072, Loss: 7.4954\n",
      "Iteration 156/1072, Loss: 7.5500\n",
      "Iteration 157/1072, Loss: 7.5547\n",
      "Iteration 158/1072, Loss: 7.4188\n",
      "Iteration 159/1072, Loss: 7.4654\n",
      "Iteration 160/1072, Loss: 7.5579\n",
      "Iteration 161/1072, Loss: 7.5266\n",
      "Iteration 162/1072, Loss: 7.7102\n",
      "Iteration 163/1072, Loss: 7.6828\n",
      "Iteration 164/1072, Loss: 7.5453\n",
      "Iteration 165/1072, Loss: 7.6766\n",
      "Iteration 166/1072, Loss: 7.5214\n",
      "Iteration 167/1072, Loss: 7.6114\n",
      "Iteration 168/1072, Loss: 7.5021\n",
      "Iteration 169/1072, Loss: 7.6026\n",
      "Iteration 170/1072, Loss: 7.4758\n",
      "Iteration 171/1072, Loss: 7.5190\n",
      "Iteration 172/1072, Loss: 7.5762\n",
      "Iteration 173/1072, Loss: 7.5569\n",
      "Iteration 174/1072, Loss: 7.4994\n",
      "Iteration 175/1072, Loss: 7.5631\n",
      "Iteration 176/1072, Loss: 7.5885\n",
      "Iteration 177/1072, Loss: 7.6309\n",
      "Iteration 178/1072, Loss: 7.6268\n",
      "Iteration 179/1072, Loss: 7.5895\n",
      "Iteration 180/1072, Loss: 7.5090\n",
      "Iteration 181/1072, Loss: 7.5617\n",
      "Iteration 182/1072, Loss: 7.5214\n",
      "Iteration 183/1072, Loss: 7.6097\n",
      "Iteration 184/1072, Loss: 7.4408\n",
      "Iteration 185/1072, Loss: 7.5074\n",
      "Iteration 186/1072, Loss: 7.4842\n",
      "Iteration 187/1072, Loss: 7.6781\n",
      "Iteration 188/1072, Loss: 7.6735\n",
      "Iteration 189/1072, Loss: 7.5380\n",
      "Iteration 190/1072, Loss: 7.6575\n",
      "Iteration 191/1072, Loss: 7.2892\n",
      "Iteration 192/1072, Loss: 7.5958\n",
      "Iteration 193/1072, Loss: 7.6851\n",
      "Iteration 194/1072, Loss: 7.5225\n",
      "Iteration 195/1072, Loss: 7.5413\n",
      "Iteration 196/1072, Loss: 7.6607\n",
      "Iteration 197/1072, Loss: 7.6463\n",
      "Iteration 198/1072, Loss: 7.5486\n",
      "Iteration 199/1072, Loss: 7.4900\n",
      "Iteration 200/1072, Loss: 7.5659\n",
      "Iteration 201/1072, Loss: 7.5428\n",
      "Iteration 202/1072, Loss: 7.5948\n",
      "Iteration 203/1072, Loss: 7.5680\n",
      "Iteration 204/1072, Loss: 7.5891\n",
      "Iteration 205/1072, Loss: 7.5650\n",
      "Iteration 206/1072, Loss: 7.6172\n",
      "Iteration 207/1072, Loss: 7.6787\n",
      "Iteration 208/1072, Loss: 7.5468\n",
      "Iteration 209/1072, Loss: 7.6144\n",
      "Iteration 210/1072, Loss: 7.5324\n",
      "Iteration 211/1072, Loss: 7.7500\n",
      "Iteration 212/1072, Loss: 7.5743\n",
      "Iteration 213/1072, Loss: 7.5324\n",
      "Iteration 214/1072, Loss: 7.5500\n",
      "Iteration 215/1072, Loss: 7.5629\n",
      "Iteration 216/1072, Loss: 7.6066\n",
      "Iteration 217/1072, Loss: 7.6032\n",
      "Iteration 218/1072, Loss: 7.5682\n",
      "Iteration 219/1072, Loss: 7.4792\n",
      "Iteration 220/1072, Loss: 7.5830\n",
      "Iteration 221/1072, Loss: 7.5174\n",
      "Iteration 222/1072, Loss: 7.5833\n",
      "Iteration 223/1072, Loss: 7.4869\n",
      "Iteration 224/1072, Loss: 7.4850\n",
      "Iteration 225/1072, Loss: 7.6908\n",
      "Iteration 226/1072, Loss: 7.5583\n",
      "Iteration 227/1072, Loss: 7.5521\n",
      "Iteration 228/1072, Loss: 7.4961\n",
      "Iteration 229/1072, Loss: 7.5328\n",
      "Iteration 230/1072, Loss: 7.5905\n",
      "Iteration 231/1072, Loss: 7.5717\n",
      "Iteration 232/1072, Loss: 7.4993\n",
      "Iteration 233/1072, Loss: 7.5921\n",
      "Iteration 234/1072, Loss: 7.5790\n",
      "Iteration 235/1072, Loss: 7.4974\n",
      "Iteration 236/1072, Loss: 7.5428\n",
      "Iteration 237/1072, Loss: 7.5504\n",
      "Iteration 238/1072, Loss: 7.5310\n",
      "Iteration 239/1072, Loss: 7.6340\n",
      "Iteration 240/1072, Loss: 7.5593\n",
      "Iteration 241/1072, Loss: 7.4581\n",
      "Iteration 242/1072, Loss: 7.6087\n",
      "Iteration 243/1072, Loss: 7.6490\n",
      "Iteration 244/1072, Loss: 7.4846\n",
      "Iteration 245/1072, Loss: 7.4853\n",
      "Iteration 246/1072, Loss: 7.6014\n",
      "Iteration 247/1072, Loss: 7.6561\n",
      "Iteration 248/1072, Loss: 7.4782\n",
      "Iteration 249/1072, Loss: 7.5208\n",
      "Iteration 250/1072, Loss: 7.5099\n",
      "Iteration 251/1072, Loss: 7.5336\n",
      "Iteration 252/1072, Loss: 7.5128\n",
      "Iteration 253/1072, Loss: 7.5600\n",
      "Iteration 254/1072, Loss: 7.5620\n",
      "Iteration 255/1072, Loss: 7.6080\n",
      "Iteration 256/1072, Loss: 7.6467\n",
      "Iteration 257/1072, Loss: 7.5310\n",
      "Iteration 258/1072, Loss: 7.6095\n",
      "Iteration 259/1072, Loss: 7.5818\n",
      "Iteration 260/1072, Loss: 7.6113\n",
      "Iteration 261/1072, Loss: 7.6138\n",
      "Iteration 262/1072, Loss: 7.5482\n",
      "Iteration 263/1072, Loss: 7.4894\n",
      "Iteration 264/1072, Loss: 7.4882\n",
      "Iteration 265/1072, Loss: 7.7214\n",
      "Iteration 266/1072, Loss: 7.5576\n",
      "Iteration 267/1072, Loss: 7.6400\n",
      "Iteration 268/1072, Loss: 7.5699\n",
      "Iteration 269/1072, Loss: 7.5873\n",
      "Iteration 270/1072, Loss: 7.5471\n",
      "Iteration 271/1072, Loss: 7.6861\n",
      "Iteration 272/1072, Loss: 7.4785\n",
      "Iteration 273/1072, Loss: 7.4905\n",
      "Iteration 274/1072, Loss: 7.5079\n",
      "Iteration 275/1072, Loss: 7.5696\n",
      "Iteration 276/1072, Loss: 7.6772\n",
      "Iteration 277/1072, Loss: 7.4716\n",
      "Iteration 278/1072, Loss: 7.6240\n",
      "Iteration 279/1072, Loss: 7.5311\n",
      "Iteration 280/1072, Loss: 7.4925\n",
      "Iteration 281/1072, Loss: 7.4943\n",
      "Iteration 282/1072, Loss: 7.4928\n",
      "Iteration 283/1072, Loss: 7.5564\n",
      "Iteration 284/1072, Loss: 7.6302\n",
      "Iteration 285/1072, Loss: 7.6494\n",
      "Iteration 286/1072, Loss: 7.4412\n",
      "Iteration 287/1072, Loss: 7.5531\n",
      "Iteration 288/1072, Loss: 7.5684\n",
      "Iteration 289/1072, Loss: 7.5708\n",
      "Iteration 290/1072, Loss: 7.6965\n",
      "Iteration 291/1072, Loss: 7.5721\n",
      "Iteration 292/1072, Loss: 7.5539\n",
      "Iteration 293/1072, Loss: 7.5820\n",
      "Iteration 294/1072, Loss: 7.6683\n",
      "Iteration 295/1072, Loss: 7.4283\n",
      "Iteration 296/1072, Loss: 7.6379\n",
      "Iteration 297/1072, Loss: 7.5550\n",
      "Iteration 298/1072, Loss: 7.7390\n",
      "Iteration 299/1072, Loss: 7.3092\n",
      "Iteration 300/1072, Loss: 7.6376\n",
      "Iteration 301/1072, Loss: 7.6564\n",
      "Iteration 302/1072, Loss: 7.5489\n",
      "Iteration 303/1072, Loss: 7.5841\n",
      "Iteration 304/1072, Loss: 7.5220\n",
      "Iteration 305/1072, Loss: 7.4908\n",
      "Iteration 306/1072, Loss: 7.5644\n",
      "Iteration 307/1072, Loss: 7.6302\n",
      "Iteration 308/1072, Loss: 7.5552\n",
      "Iteration 309/1072, Loss: 7.6460\n",
      "Iteration 310/1072, Loss: 7.5980\n",
      "Iteration 311/1072, Loss: 7.5745\n",
      "Iteration 312/1072, Loss: 7.4988\n",
      "Iteration 313/1072, Loss: 7.4959\n",
      "Iteration 314/1072, Loss: 7.4528\n",
      "Iteration 315/1072, Loss: 7.5619\n",
      "Iteration 316/1072, Loss: 7.5340\n",
      "Iteration 317/1072, Loss: 7.4915\n",
      "Iteration 318/1072, Loss: 7.5321\n",
      "Iteration 319/1072, Loss: 7.5861\n",
      "Iteration 320/1072, Loss: 7.5651\n",
      "Iteration 321/1072, Loss: 7.7006\n",
      "Iteration 322/1072, Loss: 7.4604\n",
      "Iteration 323/1072, Loss: 7.4528\n",
      "Iteration 324/1072, Loss: 7.4641\n",
      "Iteration 325/1072, Loss: 7.6046\n",
      "Iteration 326/1072, Loss: 7.6521\n",
      "Iteration 327/1072, Loss: 7.6262\n",
      "Iteration 328/1072, Loss: 7.6645\n",
      "Iteration 329/1072, Loss: 7.5493\n",
      "Iteration 330/1072, Loss: 7.5516\n",
      "Iteration 331/1072, Loss: 7.5138\n",
      "Iteration 332/1072, Loss: 7.5141\n",
      "Iteration 333/1072, Loss: 7.6164\n",
      "Iteration 334/1072, Loss: 7.4991\n",
      "Iteration 335/1072, Loss: 7.6154\n",
      "Iteration 336/1072, Loss: 7.4209\n",
      "Iteration 337/1072, Loss: 7.6053\n",
      "Iteration 338/1072, Loss: 7.5804\n",
      "Iteration 339/1072, Loss: 7.4776\n",
      "Iteration 340/1072, Loss: 7.5511\n",
      "Iteration 341/1072, Loss: 7.5083\n",
      "Iteration 342/1072, Loss: 7.6549\n",
      "Iteration 343/1072, Loss: 7.6174\n",
      "Iteration 344/1072, Loss: 7.4790\n",
      "Iteration 345/1072, Loss: 7.5816\n",
      "Iteration 346/1072, Loss: 7.4541\n",
      "Iteration 347/1072, Loss: 7.4059\n",
      "Iteration 348/1072, Loss: 7.4706\n",
      "Iteration 349/1072, Loss: 7.4938\n",
      "Iteration 350/1072, Loss: 7.5147\n",
      "Iteration 351/1072, Loss: 7.3498\n",
      "Iteration 352/1072, Loss: 7.5451\n",
      "Iteration 353/1072, Loss: 7.5390\n",
      "Iteration 354/1072, Loss: 7.5817\n",
      "Iteration 355/1072, Loss: 7.4386\n",
      "Iteration 356/1072, Loss: 7.5252\n",
      "Iteration 357/1072, Loss: 7.6321\n",
      "Iteration 358/1072, Loss: 7.4970\n",
      "Iteration 359/1072, Loss: 7.5979\n",
      "Iteration 360/1072, Loss: 7.4813\n",
      "Iteration 361/1072, Loss: 7.5913\n",
      "Iteration 362/1072, Loss: 7.5953\n",
      "Iteration 363/1072, Loss: 7.5508\n",
      "Iteration 364/1072, Loss: 7.6122\n",
      "Iteration 365/1072, Loss: 7.5668\n",
      "Iteration 366/1072, Loss: 7.4967\n",
      "Iteration 367/1072, Loss: 7.3816\n",
      "Iteration 368/1072, Loss: 7.6869\n",
      "Iteration 369/1072, Loss: 7.6037\n",
      "Iteration 370/1072, Loss: 7.6949\n",
      "Iteration 371/1072, Loss: 7.5440\n",
      "Iteration 372/1072, Loss: 7.5197\n",
      "Iteration 373/1072, Loss: 7.5272\n",
      "Iteration 374/1072, Loss: 7.5278\n",
      "Iteration 375/1072, Loss: 7.6162\n",
      "Iteration 376/1072, Loss: 7.5349\n",
      "Iteration 377/1072, Loss: 7.5157\n",
      "Iteration 378/1072, Loss: 7.6473\n",
      "Iteration 379/1072, Loss: 7.4833\n",
      "Iteration 380/1072, Loss: 7.4395\n",
      "Iteration 381/1072, Loss: 7.6120\n",
      "Iteration 382/1072, Loss: 7.4642\n",
      "Iteration 383/1072, Loss: 7.6622\n",
      "Iteration 384/1072, Loss: 7.4375\n",
      "Iteration 385/1072, Loss: 7.5821\n",
      "Iteration 386/1072, Loss: 7.6448\n",
      "Iteration 387/1072, Loss: 7.4710\n",
      "Iteration 388/1072, Loss: 7.6363\n",
      "Iteration 389/1072, Loss: 7.4887\n",
      "Iteration 390/1072, Loss: 7.5741\n",
      "Iteration 391/1072, Loss: 7.6726\n",
      "Iteration 392/1072, Loss: 7.4638\n",
      "Iteration 393/1072, Loss: 7.6543\n",
      "Iteration 394/1072, Loss: 7.5796\n",
      "Iteration 395/1072, Loss: 7.6288\n",
      "Iteration 396/1072, Loss: 7.4956\n",
      "Iteration 397/1072, Loss: 7.5685\n",
      "Iteration 398/1072, Loss: 7.5394\n",
      "Iteration 399/1072, Loss: 7.5860\n",
      "Iteration 400/1072, Loss: 7.6018\n",
      "Iteration 401/1072, Loss: 7.5506\n",
      "Iteration 402/1072, Loss: 7.4800\n",
      "Iteration 403/1072, Loss: 7.6509\n",
      "Iteration 404/1072, Loss: 7.5265\n",
      "Iteration 405/1072, Loss: 7.5962\n",
      "Iteration 406/1072, Loss: 7.5320\n",
      "Iteration 407/1072, Loss: 7.6037\n",
      "Iteration 408/1072, Loss: 7.4285\n",
      "Iteration 409/1072, Loss: 7.5380\n",
      "Iteration 410/1072, Loss: 7.5524\n",
      "Iteration 411/1072, Loss: 7.6493\n",
      "Iteration 412/1072, Loss: 7.5176\n",
      "Iteration 413/1072, Loss: 7.4900\n",
      "Iteration 414/1072, Loss: 7.4614\n",
      "Iteration 415/1072, Loss: 7.5375\n",
      "Iteration 416/1072, Loss: 7.6434\n",
      "Iteration 417/1072, Loss: 7.5843\n",
      "Iteration 418/1072, Loss: 7.5121\n",
      "Iteration 419/1072, Loss: 7.5082\n",
      "Iteration 420/1072, Loss: 7.4233\n",
      "Iteration 421/1072, Loss: 7.4704\n",
      "Iteration 422/1072, Loss: 7.2482\n",
      "Iteration 423/1072, Loss: 7.5270\n",
      "Iteration 424/1072, Loss: 7.4563\n",
      "Iteration 425/1072, Loss: 7.5036\n",
      "Iteration 426/1072, Loss: 7.5788\n",
      "Iteration 427/1072, Loss: 7.6425\n",
      "Iteration 428/1072, Loss: 7.5517\n",
      "Iteration 429/1072, Loss: 7.4639\n",
      "Iteration 430/1072, Loss: 7.5120\n",
      "Iteration 431/1072, Loss: 7.4415\n",
      "Iteration 432/1072, Loss: 7.5935\n",
      "Iteration 433/1072, Loss: 7.4992\n",
      "Iteration 434/1072, Loss: 7.6194\n",
      "Iteration 435/1072, Loss: 7.6651\n",
      "Iteration 436/1072, Loss: 7.5831\n",
      "Iteration 437/1072, Loss: 7.4573\n",
      "Iteration 438/1072, Loss: 7.4287\n",
      "Iteration 439/1072, Loss: 7.4777\n",
      "Iteration 440/1072, Loss: 7.4556\n",
      "Iteration 441/1072, Loss: 7.5231\n",
      "Iteration 442/1072, Loss: 7.6322\n",
      "Iteration 443/1072, Loss: 7.5066\n",
      "Iteration 444/1072, Loss: 7.6758\n",
      "Iteration 445/1072, Loss: 7.5214\n",
      "Iteration 446/1072, Loss: 7.4689\n",
      "Iteration 447/1072, Loss: 7.4983\n",
      "Iteration 448/1072, Loss: 7.5469\n",
      "Iteration 449/1072, Loss: 7.5323\n",
      "Iteration 450/1072, Loss: 7.5783\n",
      "Iteration 451/1072, Loss: 7.5200\n",
      "Iteration 452/1072, Loss: 7.5720\n",
      "Iteration 453/1072, Loss: 7.5719\n",
      "Iteration 454/1072, Loss: 7.6965\n",
      "Iteration 455/1072, Loss: 7.3144\n",
      "Iteration 456/1072, Loss: 7.4753\n",
      "Iteration 457/1072, Loss: 7.5217\n",
      "Iteration 458/1072, Loss: 7.5784\n",
      "Iteration 459/1072, Loss: 7.3780\n",
      "Iteration 460/1072, Loss: 7.5982\n",
      "Iteration 461/1072, Loss: 7.5621\n",
      "Iteration 462/1072, Loss: 7.4952\n",
      "Iteration 463/1072, Loss: 7.6433\n",
      "Iteration 464/1072, Loss: 7.6126\n",
      "Iteration 465/1072, Loss: 7.4675\n",
      "Iteration 466/1072, Loss: 7.5884\n",
      "Iteration 467/1072, Loss: 7.6768\n",
      "Iteration 468/1072, Loss: 7.5668\n",
      "Iteration 469/1072, Loss: 7.5293\n",
      "Iteration 470/1072, Loss: 7.4979\n",
      "Iteration 471/1072, Loss: 7.6130\n",
      "Iteration 472/1072, Loss: 7.5533\n",
      "Iteration 473/1072, Loss: 7.6162\n",
      "Iteration 474/1072, Loss: 7.5570\n",
      "Iteration 475/1072, Loss: 7.6664\n",
      "Iteration 476/1072, Loss: 7.5538\n",
      "Iteration 477/1072, Loss: 7.5855\n",
      "Iteration 478/1072, Loss: 7.6398\n",
      "Iteration 479/1072, Loss: 7.6335\n",
      "Iteration 480/1072, Loss: 7.6210\n",
      "Iteration 481/1072, Loss: 7.6052\n",
      "Iteration 482/1072, Loss: 7.4088\n",
      "Iteration 483/1072, Loss: 7.5872\n",
      "Iteration 484/1072, Loss: 7.4219\n",
      "Iteration 485/1072, Loss: 7.6268\n",
      "Iteration 486/1072, Loss: 7.5044\n",
      "Iteration 487/1072, Loss: 7.4982\n",
      "Iteration 488/1072, Loss: 7.4775\n",
      "Iteration 489/1072, Loss: 7.4931\n",
      "Iteration 490/1072, Loss: 7.5477\n",
      "Iteration 491/1072, Loss: 7.4681\n",
      "Iteration 492/1072, Loss: 7.5280\n",
      "Iteration 493/1072, Loss: 7.6440\n",
      "Iteration 494/1072, Loss: 7.5707\n",
      "Iteration 495/1072, Loss: 7.6232\n",
      "Iteration 496/1072, Loss: 7.5060\n",
      "Iteration 497/1072, Loss: 7.6424\n",
      "Iteration 498/1072, Loss: 7.6586\n",
      "Iteration 499/1072, Loss: 7.5797\n",
      "Iteration 500/1072, Loss: 7.5584\n",
      "Iteration 501/1072, Loss: 7.5275\n",
      "Iteration 502/1072, Loss: 7.5691\n",
      "Iteration 503/1072, Loss: 7.4038\n",
      "Iteration 504/1072, Loss: 7.5715\n",
      "Iteration 505/1072, Loss: 7.4126\n",
      "Iteration 506/1072, Loss: 7.5936\n",
      "Iteration 507/1072, Loss: 7.6233\n",
      "Iteration 508/1072, Loss: 7.4987\n",
      "Iteration 509/1072, Loss: 7.5485\n",
      "Iteration 510/1072, Loss: 7.3756\n",
      "Iteration 511/1072, Loss: 7.5431\n",
      "Iteration 512/1072, Loss: 7.4869\n",
      "Iteration 513/1072, Loss: 7.4867\n",
      "Iteration 514/1072, Loss: 7.5569\n",
      "Iteration 515/1072, Loss: 7.4381\n",
      "Iteration 516/1072, Loss: 7.5838\n",
      "Iteration 517/1072, Loss: 7.4644\n",
      "Iteration 518/1072, Loss: 7.5736\n",
      "Iteration 519/1072, Loss: 7.4930\n",
      "Iteration 520/1072, Loss: 7.3345\n",
      "Iteration 521/1072, Loss: 7.4644\n",
      "Iteration 522/1072, Loss: 7.5129\n",
      "Iteration 523/1072, Loss: 7.5397\n",
      "Iteration 524/1072, Loss: 7.3812\n",
      "Iteration 525/1072, Loss: 7.5538\n",
      "Iteration 526/1072, Loss: 7.5583\n",
      "Iteration 527/1072, Loss: 7.4495\n",
      "Iteration 528/1072, Loss: 7.5146\n",
      "Iteration 529/1072, Loss: 7.4860\n",
      "Iteration 530/1072, Loss: 7.4520\n",
      "Iteration 531/1072, Loss: 7.5545\n",
      "Iteration 532/1072, Loss: 7.5530\n",
      "Iteration 533/1072, Loss: 7.4945\n",
      "Iteration 534/1072, Loss: 7.3490\n",
      "Iteration 535/1072, Loss: 7.4960\n",
      "Iteration 536/1072, Loss: 7.5242\n",
      "Iteration 537/1072, Loss: 7.5414\n",
      "Iteration 538/1072, Loss: 7.4186\n",
      "Iteration 539/1072, Loss: 7.5234\n",
      "Iteration 540/1072, Loss: 7.5408\n",
      "Iteration 541/1072, Loss: 7.5062\n",
      "Iteration 542/1072, Loss: 7.5188\n",
      "Iteration 543/1072, Loss: 7.5173\n",
      "Iteration 544/1072, Loss: 7.4561\n",
      "Iteration 545/1072, Loss: 7.6145\n",
      "Iteration 546/1072, Loss: 7.5527\n",
      "Iteration 547/1072, Loss: 7.4198\n",
      "Iteration 548/1072, Loss: 7.4929\n",
      "Iteration 549/1072, Loss: 7.4808\n",
      "Iteration 550/1072, Loss: 7.4960\n",
      "Iteration 551/1072, Loss: 7.4133\n",
      "Iteration 552/1072, Loss: 7.5194\n",
      "Iteration 553/1072, Loss: 7.5331\n",
      "Iteration 554/1072, Loss: 7.4649\n",
      "Iteration 555/1072, Loss: 7.2864\n",
      "Iteration 556/1072, Loss: 7.5626\n",
      "Iteration 557/1072, Loss: 7.5264\n",
      "Iteration 558/1072, Loss: 7.4376\n",
      "Iteration 559/1072, Loss: 7.5182\n",
      "Iteration 560/1072, Loss: 7.4336\n",
      "Iteration 561/1072, Loss: 7.5576\n",
      "Iteration 562/1072, Loss: 7.4813\n",
      "Iteration 563/1072, Loss: 7.5504\n",
      "Iteration 564/1072, Loss: 7.5480\n",
      "Iteration 565/1072, Loss: 7.4971\n",
      "Iteration 566/1072, Loss: 7.4967\n",
      "Iteration 567/1072, Loss: 7.5605\n",
      "Iteration 568/1072, Loss: 7.6084\n",
      "Iteration 569/1072, Loss: 7.5229\n",
      "Iteration 570/1072, Loss: 7.5505\n",
      "Iteration 571/1072, Loss: 7.6218\n",
      "Iteration 572/1072, Loss: 7.5452\n",
      "Iteration 573/1072, Loss: 7.5451\n",
      "Iteration 574/1072, Loss: 7.5140\n",
      "Iteration 575/1072, Loss: 7.5270\n",
      "Iteration 576/1072, Loss: 7.6237\n",
      "Iteration 577/1072, Loss: 7.6045\n",
      "Iteration 578/1072, Loss: 7.5006\n",
      "Iteration 579/1072, Loss: 7.4038\n",
      "Iteration 580/1072, Loss: 7.5216\n",
      "Iteration 581/1072, Loss: 7.5618\n",
      "Iteration 582/1072, Loss: 7.4535\n",
      "Iteration 583/1072, Loss: 7.4556\n",
      "Iteration 584/1072, Loss: 7.4759\n",
      "Iteration 585/1072, Loss: 7.5132\n",
      "Iteration 586/1072, Loss: 7.5559\n",
      "Iteration 587/1072, Loss: 7.5709\n",
      "Iteration 588/1072, Loss: 7.4803\n",
      "Iteration 589/1072, Loss: 7.6844\n",
      "Iteration 590/1072, Loss: 7.4188\n",
      "Iteration 591/1072, Loss: 7.4850\n",
      "Iteration 592/1072, Loss: 7.5119\n",
      "Iteration 593/1072, Loss: 7.6577\n",
      "Iteration 594/1072, Loss: 7.5719\n",
      "Iteration 595/1072, Loss: 7.5234\n",
      "Iteration 596/1072, Loss: 7.6282\n",
      "Iteration 597/1072, Loss: 7.4838\n",
      "Iteration 598/1072, Loss: 7.7540\n",
      "Iteration 599/1072, Loss: 7.4597\n",
      "Iteration 600/1072, Loss: 7.4668\n",
      "Iteration 601/1072, Loss: 7.5792\n",
      "Iteration 602/1072, Loss: 7.5824\n",
      "Iteration 603/1072, Loss: 7.6502\n",
      "Iteration 604/1072, Loss: 7.6164\n",
      "Iteration 605/1072, Loss: 7.5438\n",
      "Iteration 606/1072, Loss: 7.4705\n",
      "Iteration 607/1072, Loss: 7.4681\n",
      "Iteration 608/1072, Loss: 7.5207\n",
      "Iteration 609/1072, Loss: 7.5875\n",
      "Iteration 610/1072, Loss: 7.6674\n",
      "Iteration 611/1072, Loss: 7.6100\n",
      "Iteration 612/1072, Loss: 7.4525\n",
      "Iteration 613/1072, Loss: 7.6212\n",
      "Iteration 614/1072, Loss: 7.4174\n",
      "Iteration 615/1072, Loss: 7.4115\n",
      "Iteration 616/1072, Loss: 7.5521\n",
      "Iteration 617/1072, Loss: 7.4429\n",
      "Iteration 618/1072, Loss: 7.4803\n",
      "Iteration 619/1072, Loss: 7.5060\n",
      "Iteration 620/1072, Loss: 7.6580\n",
      "Iteration 621/1072, Loss: 7.5828\n",
      "Iteration 622/1072, Loss: 7.6230\n",
      "Iteration 623/1072, Loss: 7.5180\n",
      "Iteration 624/1072, Loss: 7.4076\n",
      "Iteration 625/1072, Loss: 7.4018\n",
      "Iteration 626/1072, Loss: 7.5198\n",
      "Iteration 627/1072, Loss: 7.4474\n",
      "Iteration 628/1072, Loss: 7.4826\n",
      "Iteration 629/1072, Loss: 7.4963\n",
      "Iteration 630/1072, Loss: 7.6023\n",
      "Iteration 631/1072, Loss: 7.6410\n",
      "Iteration 632/1072, Loss: 7.5718\n",
      "Iteration 633/1072, Loss: 7.4587\n",
      "Iteration 634/1072, Loss: 7.5228\n",
      "Iteration 635/1072, Loss: 7.5569\n",
      "Iteration 636/1072, Loss: 7.5033\n",
      "Iteration 637/1072, Loss: 7.6358\n",
      "Iteration 638/1072, Loss: 7.6152\n",
      "Iteration 639/1072, Loss: 7.6701\n",
      "Iteration 640/1072, Loss: 7.5005\n",
      "Iteration 641/1072, Loss: 7.5251\n",
      "Iteration 642/1072, Loss: 7.5610\n",
      "Iteration 643/1072, Loss: 7.6389\n",
      "Iteration 644/1072, Loss: 7.4820\n",
      "Iteration 645/1072, Loss: 7.4748\n",
      "Iteration 646/1072, Loss: 7.5786\n",
      "Iteration 647/1072, Loss: 7.4243\n",
      "Iteration 648/1072, Loss: 7.5785\n",
      "Iteration 649/1072, Loss: 7.4999\n",
      "Iteration 650/1072, Loss: 7.4983\n",
      "Iteration 651/1072, Loss: 7.3660\n",
      "Iteration 652/1072, Loss: 7.5256\n",
      "Iteration 653/1072, Loss: 7.4204\n",
      "Iteration 654/1072, Loss: 7.5926\n",
      "Iteration 655/1072, Loss: 7.5308\n",
      "Iteration 656/1072, Loss: 7.4485\n",
      "Iteration 657/1072, Loss: 7.4494\n",
      "Iteration 658/1072, Loss: 7.5857\n",
      "Iteration 659/1072, Loss: 7.5413\n",
      "Iteration 660/1072, Loss: 7.6175\n",
      "Iteration 661/1072, Loss: 7.4807\n",
      "Iteration 662/1072, Loss: 7.4307\n",
      "Iteration 663/1072, Loss: 7.4625\n",
      "Iteration 664/1072, Loss: 7.6126\n",
      "Iteration 665/1072, Loss: 7.5652\n",
      "Iteration 666/1072, Loss: 7.6033\n",
      "Iteration 667/1072, Loss: 7.5055\n",
      "Iteration 668/1072, Loss: 7.5086\n",
      "Iteration 669/1072, Loss: 7.4915\n",
      "Iteration 670/1072, Loss: 7.4084\n",
      "Iteration 671/1072, Loss: 7.5989\n",
      "Iteration 672/1072, Loss: 7.5150\n",
      "Iteration 673/1072, Loss: 7.5395\n",
      "Iteration 674/1072, Loss: 7.5010\n",
      "Iteration 675/1072, Loss: 7.6186\n",
      "Iteration 676/1072, Loss: 7.6000\n",
      "Iteration 677/1072, Loss: 7.5178\n",
      "Iteration 678/1072, Loss: 7.5699\n",
      "Iteration 679/1072, Loss: 7.5642\n",
      "Iteration 680/1072, Loss: 7.3843\n",
      "Iteration 681/1072, Loss: 7.5552\n",
      "Iteration 682/1072, Loss: 7.4357\n",
      "Iteration 683/1072, Loss: 7.5292\n",
      "Iteration 684/1072, Loss: 7.4091\n",
      "Iteration 685/1072, Loss: 7.6219\n",
      "Iteration 686/1072, Loss: 7.4576\n",
      "Iteration 687/1072, Loss: 7.4181\n",
      "Iteration 688/1072, Loss: 7.6216\n",
      "Iteration 689/1072, Loss: 7.2615\n",
      "Iteration 690/1072, Loss: 7.5785\n",
      "Iteration 691/1072, Loss: 7.4722\n",
      "Iteration 692/1072, Loss: 7.5851\n",
      "Iteration 693/1072, Loss: 7.4185\n",
      "Iteration 694/1072, Loss: 7.5687\n",
      "Iteration 695/1072, Loss: 7.7171\n",
      "Iteration 696/1072, Loss: 7.5583\n",
      "Iteration 697/1072, Loss: 7.5716\n",
      "Iteration 698/1072, Loss: 7.4602\n",
      "Iteration 699/1072, Loss: 7.5407\n",
      "Iteration 700/1072, Loss: 7.3408\n",
      "Iteration 701/1072, Loss: 7.3880\n",
      "Iteration 702/1072, Loss: 7.4645\n",
      "Iteration 703/1072, Loss: 7.5371\n",
      "Iteration 704/1072, Loss: 7.5388\n",
      "Iteration 705/1072, Loss: 7.4097\n",
      "Iteration 706/1072, Loss: 7.6143\n",
      "Iteration 707/1072, Loss: 7.6608\n",
      "Iteration 708/1072, Loss: 7.3267\n",
      "Iteration 709/1072, Loss: 7.6665\n",
      "Iteration 710/1072, Loss: 7.4837\n",
      "Iteration 711/1072, Loss: 7.4965\n",
      "Iteration 712/1072, Loss: 7.5307\n",
      "Iteration 713/1072, Loss: 7.6245\n",
      "Iteration 714/1072, Loss: 7.6239\n",
      "Iteration 715/1072, Loss: 7.3615\n",
      "Iteration 716/1072, Loss: 7.4201\n",
      "Iteration 717/1072, Loss: 7.6543\n",
      "Iteration 718/1072, Loss: 7.5299\n",
      "Iteration 719/1072, Loss: 7.4727\n",
      "Iteration 720/1072, Loss: 7.5998\n",
      "Iteration 721/1072, Loss: 7.6552\n",
      "Iteration 722/1072, Loss: 7.5529\n",
      "Iteration 723/1072, Loss: 7.4756\n",
      "Iteration 724/1072, Loss: 7.5280\n",
      "Iteration 725/1072, Loss: 7.5111\n",
      "Iteration 726/1072, Loss: 7.2854\n",
      "Iteration 727/1072, Loss: 7.6121\n",
      "Iteration 728/1072, Loss: 7.5801\n",
      "Iteration 729/1072, Loss: 7.2932\n",
      "Iteration 730/1072, Loss: 7.4079\n",
      "Iteration 731/1072, Loss: 7.5719\n",
      "Iteration 732/1072, Loss: 7.6225\n",
      "Iteration 733/1072, Loss: 7.6141\n",
      "Iteration 734/1072, Loss: 7.4564\n",
      "Iteration 735/1072, Loss: 7.5199\n",
      "Iteration 736/1072, Loss: 7.6275\n",
      "Iteration 737/1072, Loss: 7.4433\n",
      "Iteration 738/1072, Loss: 7.4682\n",
      "Iteration 739/1072, Loss: 7.4702\n",
      "Iteration 740/1072, Loss: 7.5958\n",
      "Iteration 741/1072, Loss: 7.5476\n",
      "Iteration 742/1072, Loss: 7.3171\n",
      "Iteration 743/1072, Loss: 7.4289\n",
      "Iteration 744/1072, Loss: 7.4770\n",
      "Iteration 745/1072, Loss: 7.6686\n",
      "Iteration 746/1072, Loss: 7.5242\n",
      "Iteration 747/1072, Loss: 7.5000\n",
      "Iteration 748/1072, Loss: 7.5090\n",
      "Iteration 749/1072, Loss: 7.5917\n",
      "Iteration 750/1072, Loss: 7.5937\n",
      "Iteration 751/1072, Loss: 7.5094\n",
      "Iteration 752/1072, Loss: 7.4554\n",
      "Iteration 753/1072, Loss: 7.5218\n",
      "Iteration 754/1072, Loss: 7.7233\n",
      "Iteration 755/1072, Loss: 7.5161\n",
      "Iteration 756/1072, Loss: 7.4349\n",
      "Iteration 757/1072, Loss: 7.4931\n",
      "Iteration 758/1072, Loss: 7.5903\n",
      "Iteration 759/1072, Loss: 7.5890\n",
      "Iteration 760/1072, Loss: 7.4599\n",
      "Iteration 761/1072, Loss: 7.4878\n",
      "Iteration 762/1072, Loss: 7.5543\n",
      "Iteration 763/1072, Loss: 7.4239\n",
      "Iteration 764/1072, Loss: 7.5054\n",
      "Iteration 765/1072, Loss: 7.5951\n",
      "Iteration 766/1072, Loss: 7.4085\n",
      "Iteration 767/1072, Loss: 7.5955\n",
      "Iteration 768/1072, Loss: 7.5563\n",
      "Iteration 769/1072, Loss: 7.3986\n",
      "Iteration 770/1072, Loss: 7.4981\n",
      "Iteration 771/1072, Loss: 7.5950\n",
      "Iteration 772/1072, Loss: 7.5579\n",
      "Iteration 773/1072, Loss: 7.6360\n",
      "Iteration 774/1072, Loss: 7.3634\n",
      "Iteration 775/1072, Loss: 7.5005\n",
      "Iteration 776/1072, Loss: 7.6177\n",
      "Iteration 777/1072, Loss: 7.4746\n",
      "Iteration 778/1072, Loss: 7.5174\n",
      "Iteration 779/1072, Loss: 7.6391\n",
      "Iteration 780/1072, Loss: 7.3644\n",
      "Iteration 781/1072, Loss: 7.5467\n",
      "Iteration 782/1072, Loss: 7.5722\n",
      "Iteration 783/1072, Loss: 7.3975\n",
      "Iteration 784/1072, Loss: 7.4565\n",
      "Iteration 785/1072, Loss: 7.4761\n",
      "Iteration 786/1072, Loss: 7.4231\n",
      "Iteration 787/1072, Loss: 7.3148\n",
      "Iteration 788/1072, Loss: 7.4424\n",
      "Iteration 789/1072, Loss: 7.6238\n",
      "Iteration 790/1072, Loss: 7.6179\n",
      "Iteration 791/1072, Loss: 7.5232\n",
      "Iteration 792/1072, Loss: 7.5261\n",
      "Iteration 793/1072, Loss: 7.5401\n",
      "Iteration 794/1072, Loss: 7.5991\n",
      "Iteration 795/1072, Loss: 7.5536\n",
      "Iteration 796/1072, Loss: 7.5881\n",
      "Iteration 797/1072, Loss: 7.4766\n",
      "Iteration 798/1072, Loss: 7.4677\n",
      "Iteration 799/1072, Loss: 7.5288\n",
      "Iteration 800/1072, Loss: 7.6410\n",
      "Iteration 801/1072, Loss: 7.5288\n",
      "Iteration 802/1072, Loss: 7.5178\n",
      "Iteration 803/1072, Loss: 7.5111\n",
      "Iteration 804/1072, Loss: 7.5048\n",
      "Iteration 805/1072, Loss: 7.3816\n",
      "Iteration 806/1072, Loss: 7.4732\n",
      "Iteration 807/1072, Loss: 7.6023\n",
      "Iteration 808/1072, Loss: 7.5859\n",
      "Iteration 809/1072, Loss: 7.4877\n",
      "Iteration 810/1072, Loss: 7.5177\n",
      "Iteration 811/1072, Loss: 7.4444\n",
      "Iteration 812/1072, Loss: 7.5236\n",
      "Iteration 813/1072, Loss: 7.5486\n",
      "Iteration 814/1072, Loss: 7.4812\n",
      "Iteration 815/1072, Loss: 7.3198\n",
      "Iteration 816/1072, Loss: 7.4135\n",
      "Iteration 817/1072, Loss: 7.2082\n",
      "Iteration 818/1072, Loss: 7.4074\n",
      "Iteration 819/1072, Loss: 7.5512\n",
      "Iteration 820/1072, Loss: 7.4518\n",
      "Iteration 821/1072, Loss: 7.4032\n",
      "Iteration 822/1072, Loss: 7.5235\n",
      "Iteration 823/1072, Loss: 7.4856\n",
      "Iteration 824/1072, Loss: 7.5430\n",
      "Iteration 825/1072, Loss: 7.5272\n",
      "Iteration 826/1072, Loss: 7.4377\n",
      "Iteration 827/1072, Loss: 7.5312\n",
      "Iteration 828/1072, Loss: 7.4827\n",
      "Iteration 829/1072, Loss: 7.5951\n",
      "Iteration 830/1072, Loss: 7.4589\n",
      "Iteration 831/1072, Loss: 7.4466\n",
      "Iteration 832/1072, Loss: 7.5181\n",
      "Iteration 833/1072, Loss: 7.3797\n",
      "Iteration 834/1072, Loss: 7.6579\n",
      "Iteration 835/1072, Loss: 7.5086\n",
      "Iteration 836/1072, Loss: 7.4480\n",
      "Iteration 837/1072, Loss: 7.4186\n",
      "Iteration 838/1072, Loss: 7.4313\n",
      "Iteration 839/1072, Loss: 7.3106\n",
      "Iteration 840/1072, Loss: 7.4456\n",
      "Iteration 841/1072, Loss: 7.4890\n",
      "Iteration 842/1072, Loss: 7.4740\n",
      "Iteration 843/1072, Loss: 7.5159\n",
      "Iteration 844/1072, Loss: 7.4363\n",
      "Iteration 845/1072, Loss: 7.5628\n",
      "Iteration 846/1072, Loss: 7.2968\n",
      "Iteration 847/1072, Loss: 7.6408\n",
      "Iteration 848/1072, Loss: 7.5368\n",
      "Iteration 849/1072, Loss: 7.4882\n",
      "Iteration 850/1072, Loss: 7.6074\n",
      "Iteration 851/1072, Loss: 7.5096\n",
      "Iteration 852/1072, Loss: 7.5920\n",
      "Iteration 853/1072, Loss: 7.4707\n",
      "Iteration 854/1072, Loss: 7.4384\n",
      "Iteration 855/1072, Loss: 7.4357\n",
      "Iteration 856/1072, Loss: 7.4391\n",
      "Iteration 857/1072, Loss: 7.5884\n",
      "Iteration 858/1072, Loss: 7.4899\n",
      "Iteration 859/1072, Loss: 7.5215\n",
      "Iteration 860/1072, Loss: 7.5282\n",
      "Iteration 861/1072, Loss: 7.5136\n",
      "Iteration 862/1072, Loss: 7.5072\n",
      "Iteration 863/1072, Loss: 7.4536\n",
      "Iteration 864/1072, Loss: 7.5705\n",
      "Iteration 865/1072, Loss: 7.4352\n",
      "Iteration 866/1072, Loss: 7.4448\n",
      "Iteration 867/1072, Loss: 7.6920\n",
      "Iteration 868/1072, Loss: 7.5128\n",
      "Iteration 869/1072, Loss: 7.5107\n",
      "Iteration 870/1072, Loss: 7.4980\n",
      "Iteration 871/1072, Loss: 7.5064\n",
      "Iteration 872/1072, Loss: 7.5173\n",
      "Iteration 873/1072, Loss: 7.3490\n",
      "Iteration 874/1072, Loss: 7.5413\n",
      "Iteration 875/1072, Loss: 7.4533\n",
      "Iteration 876/1072, Loss: 7.4885\n",
      "Iteration 877/1072, Loss: 7.6247\n",
      "Iteration 878/1072, Loss: 7.5635\n",
      "Iteration 879/1072, Loss: 7.5150\n",
      "Iteration 880/1072, Loss: 7.5526\n",
      "Iteration 881/1072, Loss: 7.5378\n",
      "Iteration 882/1072, Loss: 7.5726\n",
      "Iteration 883/1072, Loss: 7.5445\n",
      "Iteration 884/1072, Loss: 7.5846\n",
      "Iteration 885/1072, Loss: 7.6734\n",
      "Iteration 886/1072, Loss: 7.4786\n",
      "Iteration 887/1072, Loss: 7.5203\n",
      "Iteration 888/1072, Loss: 7.7703\n",
      "Iteration 889/1072, Loss: 7.4509\n",
      "Iteration 890/1072, Loss: 7.6008\n",
      "Iteration 891/1072, Loss: 7.5027\n",
      "Iteration 892/1072, Loss: 7.4719\n",
      "Iteration 893/1072, Loss: 7.6043\n",
      "Iteration 894/1072, Loss: 7.5797\n",
      "Iteration 895/1072, Loss: 7.5519\n",
      "Iteration 896/1072, Loss: 7.6131\n",
      "Iteration 897/1072, Loss: 7.4935\n",
      "Iteration 898/1072, Loss: 7.2767\n",
      "Iteration 899/1072, Loss: 7.5118\n",
      "Iteration 900/1072, Loss: 7.5637\n",
      "Iteration 901/1072, Loss: 7.3786\n",
      "Iteration 902/1072, Loss: 7.5724\n",
      "Iteration 903/1072, Loss: 7.5613\n",
      "Iteration 904/1072, Loss: 7.4755\n",
      "Iteration 905/1072, Loss: 7.3103\n",
      "Iteration 906/1072, Loss: 7.3824\n",
      "Iteration 907/1072, Loss: 7.5552\n",
      "Iteration 908/1072, Loss: 7.3200\n",
      "Iteration 909/1072, Loss: 7.4750\n",
      "Iteration 910/1072, Loss: 7.4716\n",
      "Iteration 911/1072, Loss: 7.5396\n",
      "Iteration 912/1072, Loss: 7.5593\n",
      "Iteration 913/1072, Loss: 7.4919\n",
      "Iteration 914/1072, Loss: 7.3327\n",
      "Iteration 915/1072, Loss: 7.4699\n",
      "Iteration 916/1072, Loss: 7.5483\n",
      "Iteration 917/1072, Loss: 7.4613\n",
      "Iteration 918/1072, Loss: 7.5055\n",
      "Iteration 919/1072, Loss: 7.5446\n",
      "Iteration 920/1072, Loss: 7.6074\n",
      "Iteration 921/1072, Loss: 7.4446\n",
      "Iteration 922/1072, Loss: 7.2993\n",
      "Iteration 923/1072, Loss: 7.4292\n",
      "Iteration 924/1072, Loss: 7.4093\n",
      "Iteration 925/1072, Loss: 7.5443\n",
      "Iteration 926/1072, Loss: 7.5515\n",
      "Iteration 927/1072, Loss: 7.5034\n",
      "Iteration 928/1072, Loss: 7.4403\n",
      "Iteration 929/1072, Loss: 7.4938\n",
      "Iteration 930/1072, Loss: 7.4086\n",
      "Iteration 967/1072, Loss: 7.6034\n",
      "Iteration 968/1072, Loss: 7.6257\n",
      "Iteration 969/1072, Loss: 7.4095\n",
      "Iteration 970/1072, Loss: 7.4984\n",
      "Iteration 971/1072, Loss: 7.2792\n",
      "Iteration 972/1072, Loss: 7.5658\n",
      "Iteration 973/1072, Loss: 7.4744\n",
      "Iteration 974/1072, Loss: 7.5794\n",
      "Iteration 975/1072, Loss: 7.5661\n",
      "Iteration 976/1072, Loss: 7.6064\n",
      "Iteration 977/1072, Loss: 7.5925\n",
      "Iteration 978/1072, Loss: 7.5759\n",
      "Iteration 979/1072, Loss: 7.4160\n",
      "Iteration 980/1072, Loss: 7.5676\n",
      "Iteration 981/1072, Loss: 7.3845\n",
      "Iteration 982/1072, Loss: 7.5836\n",
      "Iteration 983/1072, Loss: 7.6680\n",
      "Iteration 984/1072, Loss: 7.5197\n",
      "Iteration 985/1072, Loss: 7.5137\n",
      "Iteration 986/1072, Loss: 7.4836\n",
      "Iteration 987/1072, Loss: 7.5228\n",
      "Iteration 988/1072, Loss: 7.5852\n",
      "Iteration 989/1072, Loss: 7.4308\n",
      "Iteration 990/1072, Loss: 7.5158\n",
      "Iteration 991/1072, Loss: 7.5398\n",
      "Iteration 992/1072, Loss: 7.6489\n",
      "Iteration 993/1072, Loss: 7.5798\n",
      "Iteration 994/1072, Loss: 7.5423\n",
      "Iteration 995/1072, Loss: 7.4074\n",
      "Iteration 996/1072, Loss: 7.5131\n",
      "Iteration 997/1072, Loss: 7.3234\n",
      "Iteration 998/1072, Loss: 7.5055\n",
      "Iteration 999/1072, Loss: 7.5892\n",
      "Iteration 1000/1072, Loss: 7.4755\n",
      "Iteration 1001/1072, Loss: 7.5005\n",
      "Iteration 1002/1072, Loss: 7.5479\n",
      "Iteration 1003/1072, Loss: 7.5802\n",
      "Iteration 1004/1072, Loss: 7.5480\n",
      "Iteration 1005/1072, Loss: 7.3965\n",
      "Iteration 1006/1072, Loss: 7.5276\n",
      "Iteration 1007/1072, Loss: 7.5626\n",
      "Iteration 1052/1072, Loss: 7.5803\n",
      "Iteration 1053/1072, Loss: 7.4648\n",
      "Iteration 1054/1072, Loss: 7.4276\n",
      "Iteration 1055/1072, Loss: 7.2910\n",
      "Iteration 1056/1072, Loss: 7.5613\n",
      "Iteration 1057/1072, Loss: 7.4388\n",
      "Iteration 1058/1072, Loss: 7.4847\n",
      "Iteration 1059/1072, Loss: 7.5017\n",
      "Iteration 1060/1072, Loss: 7.3334\n",
      "Iteration 1061/1072, Loss: 7.5607\n",
      "Iteration 1062/1072, Loss: 7.3245\n",
      "Iteration 1063/1072, Loss: 7.4986\n",
      "Iteration 1064/1072, Loss: 7.3885\n",
      "Iteration 1065/1072, Loss: 7.5277\n",
      "Iteration 1066/1072, Loss: 7.3844\n",
      "Iteration 1067/1072, Loss: 7.4587\n",
      "Iteration 1068/1072, Loss: 7.5953\n",
      "Iteration 1069/1072, Loss: 7.3170\n",
      "Iteration 1070/1072, Loss: 7.4844\n",
      "Iteration 1071/1072, Loss: 7.4099\n",
      "Iteration 1072/1072, Loss: 7.5754\n",
      "Epoch 8/10, Loss: 7.5307\n",
      "Checkpoint saved at checkpoints/efficientnet-b4_epoch_8.pth\n",
      "Validation Accuracy: 4.56%\n",
      "Iteration 1/1072, Loss: 7.4697\n",
      "Iteration 2/1072, Loss: 7.4976\n",
      "Iteration 3/1072, Loss: 7.4931\n",
      "Iteration 4/1072, Loss: 7.4479\n",
      "Iteration 5/1072, Loss: 7.3892\n",
      "Iteration 6/1072, Loss: 7.3851\n",
      "Iteration 7/1072, Loss: 7.3396\n",
      "Iteration 8/1072, Loss: 7.5020\n",
      "Iteration 9/1072, Loss: 7.5691\n",
      "Iteration 10/1072, Loss: 7.4302\n",
      "Iteration 11/1072, Loss: 7.5439\n",
      "Iteration 12/1072, Loss: 7.3042\n",
      "Iteration 13/1072, Loss: 7.4939\n",
      "Iteration 14/1072, Loss: 7.4368\n",
      "Iteration 15/1072, Loss: 7.3846\n",
      "Iteration 16/1072, Loss: 7.4650\n",
      "Iteration 17/1072, Loss: 7.3862\n",
      "Iteration 18/1072, Loss: 7.4755\n",
      "Iteration 19/1072, Loss: 7.4454\n",
      "Iteration 20/1072, Loss: 7.3943\n",
      "Iteration 21/1072, Loss: 7.3155\n",
      "Iteration 22/1072, Loss: 7.4612\n",
      "Iteration 23/1072, Loss: 7.3540\n",
      "Iteration 24/1072, Loss: 7.4440\n",
      "Iteration 25/1072, Loss: 7.3748\n",
      "Iteration 26/1072, Loss: 7.3306\n",
      "Iteration 27/1072, Loss: 7.4591\n",
      "Iteration 28/1072, Loss: 7.5140\n",
      "Iteration 29/1072, Loss: 7.3937\n",
      "Iteration 30/1072, Loss: 7.3680\n",
      "Iteration 31/1072, Loss: 7.3952\n",
      "Iteration 32/1072, Loss: 7.3920\n",
      "Iteration 33/1072, Loss: 7.4671\n",
      "Iteration 34/1072, Loss: 7.3538\n",
      "Iteration 35/1072, Loss: 7.4792\n",
      "Iteration 36/1072, Loss: 7.4351\n",
      "Iteration 37/1072, Loss: 7.3725\n",
      "Iteration 38/1072, Loss: 7.2658\n",
      "Iteration 39/1072, Loss: 7.3796\n",
      "Iteration 40/1072, Loss: 7.3826\n",
      "Iteration 41/1072, Loss: 7.4248\n",
      "Iteration 42/1072, Loss: 7.4493\n",
      "Iteration 43/1072, Loss: 7.4337\n",
      "Iteration 44/1072, Loss: 7.3472\n",
      "Iteration 45/1072, Loss: 7.4800\n",
      "Iteration 46/1072, Loss: 7.4861\n",
      "Iteration 47/1072, Loss: 7.4795\n",
      "Iteration 48/1072, Loss: 7.3335\n",
      "Iteration 49/1072, Loss: 7.5043\n",
      "Iteration 50/1072, Loss: 7.3631\n",
      "Iteration 51/1072, Loss: 7.4352\n",
      "Iteration 52/1072, Loss: 7.4874\n",
      "Iteration 53/1072, Loss: 7.1928\n",
      "Iteration 54/1072, Loss: 7.5519\n",
      "Iteration 55/1072, Loss: 7.4682\n",
      "Iteration 56/1072, Loss: 7.3723\n",
      "Iteration 57/1072, Loss: 7.5163\n",
      "Iteration 58/1072, Loss: 7.2735\n",
      "Iteration 59/1072, Loss: 7.3384\n",
      "Iteration 60/1072, Loss: 7.3463\n",
      "Iteration 61/1072, Loss: 7.4803\n",
      "Iteration 62/1072, Loss: 7.2099\n",
      "Iteration 63/1072, Loss: 7.1965\n",
      "Iteration 64/1072, Loss: 7.4179\n",
      "Iteration 65/1072, Loss: 7.5470\n",
      "Iteration 66/1072, Loss: 7.4467\n",
      "Iteration 67/1072, Loss: 7.3442\n",
      "Iteration 68/1072, Loss: 7.3594\n",
      "Iteration 69/1072, Loss: 7.4193\n",
      "Iteration 70/1072, Loss: 7.3474\n",
      "Iteration 71/1072, Loss: 7.4777\n",
      "Iteration 72/1072, Loss: 7.5472\n",
      "Iteration 73/1072, Loss: 7.3450\n",
      "Iteration 74/1072, Loss: 7.5233\n",
      "Iteration 75/1072, Loss: 7.4478\n",
      "Iteration 76/1072, Loss: 7.4949\n",
      "Iteration 77/1072, Loss: 7.3615\n",
      "Iteration 78/1072, Loss: 7.3616\n",
      "Iteration 79/1072, Loss: 7.3700\n",
      "Iteration 80/1072, Loss: 7.3630\n",
      "Iteration 81/1072, Loss: 7.4885\n",
      "Iteration 82/1072, Loss: 7.3738\n",
      "Iteration 83/1072, Loss: 7.2907\n",
      "Iteration 84/1072, Loss: 7.4033\n",
      "Iteration 85/1072, Loss: 7.4333\n",
      "Iteration 86/1072, Loss: 7.4726\n",
      "Iteration 87/1072, Loss: 7.4971\n",
      "Iteration 88/1072, Loss: 7.5924\n",
      "Iteration 89/1072, Loss: 7.4221\n",
      "Iteration 90/1072, Loss: 7.2664\n",
      "Iteration 91/1072, Loss: 7.4385\n",
      "Iteration 92/1072, Loss: 7.4585\n",
      "Iteration 93/1072, Loss: 7.3653\n",
      "Iteration 94/1072, Loss: 7.4979\n",
      "Iteration 95/1072, Loss: 7.3456\n",
      "Iteration 96/1072, Loss: 7.4520\n",
      "Iteration 97/1072, Loss: 7.5267\n",
      "Iteration 98/1072, Loss: 7.5248\n",
      "Iteration 99/1072, Loss: 7.3958\n",
      "Iteration 100/1072, Loss: 7.4626\n",
      "Iteration 101/1072, Loss: 7.4530\n",
      "Iteration 102/1072, Loss: 7.4967\n",
      "Iteration 103/1072, Loss: 7.3395\n",
      "Iteration 104/1072, Loss: 7.3768\n",
      "Iteration 105/1072, Loss: 7.2703\n",
      "Iteration 106/1072, Loss: 7.4347\n",
      "Iteration 107/1072, Loss: 7.3603\n",
      "Iteration 108/1072, Loss: 7.4685\n",
      "Iteration 109/1072, Loss: 7.4083\n",
      "Iteration 110/1072, Loss: 7.3982\n",
      "Iteration 111/1072, Loss: 7.4346\n",
      "Iteration 112/1072, Loss: 7.5175\n",
      "Iteration 113/1072, Loss: 7.5038\n",
      "Iteration 114/1072, Loss: 7.4294\n",
      "Iteration 115/1072, Loss: 7.4493\n",
      "Iteration 116/1072, Loss: 7.5084\n",
      "Iteration 117/1072, Loss: 7.3948\n",
      "Iteration 118/1072, Loss: 7.5111\n",
      "Iteration 119/1072, Loss: 7.5734\n",
      "Iteration 120/1072, Loss: 7.3751\n",
      "Iteration 121/1072, Loss: 7.4434\n",
      "Iteration 122/1072, Loss: 7.4651\n",
      "Iteration 123/1072, Loss: 7.3505\n",
      "Iteration 124/1072, Loss: 7.4272\n",
      "Iteration 125/1072, Loss: 7.2897\n",
      "Iteration 126/1072, Loss: 7.3275\n",
      "Iteration 127/1072, Loss: 7.4056\n",
      "Iteration 128/1072, Loss: 7.5387\n",
      "Iteration 129/1072, Loss: 7.3963\n",
      "Iteration 130/1072, Loss: 7.5451\n",
      "Iteration 131/1072, Loss: 7.5414\n",
      "Iteration 132/1072, Loss: 7.5207\n",
      "Iteration 133/1072, Loss: 7.4391\n",
      "Iteration 134/1072, Loss: 7.3873\n",
      "Iteration 135/1072, Loss: 7.3469\n",
      "Iteration 136/1072, Loss: 7.3290\n",
      "Iteration 137/1072, Loss: 7.4276\n",
      "Iteration 138/1072, Loss: 7.6123\n",
      "Iteration 139/1072, Loss: 7.3826\n",
      "Iteration 140/1072, Loss: 7.3858\n",
      "Iteration 141/1072, Loss: 7.4868\n",
      "Iteration 142/1072, Loss: 7.4064\n",
      "Iteration 143/1072, Loss: 7.2986\n",
      "Iteration 144/1072, Loss: 7.4776\n",
      "Iteration 145/1072, Loss: 7.3654\n",
      "Iteration 146/1072, Loss: 7.4864\n",
      "Iteration 147/1072, Loss: 7.3067\n",
      "Iteration 148/1072, Loss: 7.2119\n",
      "Iteration 149/1072, Loss: 7.3720\n",
      "Iteration 150/1072, Loss: 7.1181\n",
      "Iteration 151/1072, Loss: 7.3833\n",
      "Iteration 152/1072, Loss: 7.4084\n",
      "Iteration 153/1072, Loss: 7.4039\n",
      "Iteration 154/1072, Loss: 7.4718\n",
      "Iteration 155/1072, Loss: 7.3158\n",
      "Iteration 156/1072, Loss: 7.4218\n",
      "Iteration 157/1072, Loss: 7.4662\n",
      "Iteration 158/1072, Loss: 7.3382\n",
      "Iteration 159/1072, Loss: 7.4462\n",
      "Iteration 160/1072, Loss: 7.5054\n",
      "Iteration 161/1072, Loss: 7.3759\n",
      "Iteration 162/1072, Loss: 7.6259\n",
      "Iteration 163/1072, Loss: 7.4429\n",
      "Iteration 164/1072, Loss: 7.4156\n",
      "Iteration 165/1072, Loss: 7.4134\n",
      "Iteration 166/1072, Loss: 7.3170\n",
      "Iteration 167/1072, Loss: 7.5138\n",
      "Iteration 168/1072, Loss: 7.4176\n",
      "Iteration 169/1072, Loss: 7.4554\n",
      "Iteration 170/1072, Loss: 7.4318\n",
      "Iteration 171/1072, Loss: 7.3071\n",
      "Iteration 172/1072, Loss: 7.6053\n",
      "Iteration 173/1072, Loss: 7.5498\n",
      "Iteration 174/1072, Loss: 7.3507\n",
      "Iteration 175/1072, Loss: 7.1599\n",
      "Iteration 176/1072, Loss: 7.4703\n",
      "Iteration 177/1072, Loss: 7.5099\n",
      "Iteration 178/1072, Loss: 7.3556\n",
      "Iteration 179/1072, Loss: 7.3844\n",
      "Iteration 180/1072, Loss: 7.5944\n",
      "Iteration 181/1072, Loss: 7.5043\n",
      "Iteration 182/1072, Loss: 7.4557\n",
      "Iteration 183/1072, Loss: 7.4312\n",
      "Iteration 184/1072, Loss: 7.3003\n",
      "Iteration 185/1072, Loss: 7.3528\n",
      "Iteration 186/1072, Loss: 7.5205\n",
      "Iteration 187/1072, Loss: 7.4739\n",
      "Iteration 188/1072, Loss: 7.3227\n",
      "Iteration 189/1072, Loss: 7.5285\n",
      "Iteration 190/1072, Loss: 7.4527\n",
      "Iteration 191/1072, Loss: 7.3232\n",
      "Iteration 192/1072, Loss: 7.5199\n",
      "Iteration 193/1072, Loss: 7.4215\n",
      "Iteration 194/1072, Loss: 7.4775\n",
      "Iteration 195/1072, Loss: 7.4581\n",
      "Iteration 196/1072, Loss: 7.3694\n",
      "Iteration 197/1072, Loss: 7.3804\n",
      "Iteration 198/1072, Loss: 7.5018\n",
      "Iteration 199/1072, Loss: 7.3974\n",
      "Iteration 200/1072, Loss: 7.2783\n",
      "Iteration 201/1072, Loss: 7.5247\n",
      "Iteration 202/1072, Loss: 7.4490\n",
      "Iteration 203/1072, Loss: 7.5300\n",
      "Iteration 204/1072, Loss: 7.3108\n",
      "Iteration 205/1072, Loss: 7.3442\n",
      "Iteration 206/1072, Loss: 7.5196\n",
      "Iteration 207/1072, Loss: 7.4429\n",
      "Iteration 208/1072, Loss: 7.4486\n",
      "Iteration 209/1072, Loss: 7.4778\n",
      "Iteration 210/1072, Loss: 7.4877\n",
      "Iteration 211/1072, Loss: 7.3039\n",
      "Iteration 212/1072, Loss: 7.5546\n",
      "Iteration 213/1072, Loss: 7.4170\n",
      "Iteration 214/1072, Loss: 7.4554\n",
      "Iteration 215/1072, Loss: 7.4031\n",
      "Iteration 216/1072, Loss: 7.4212\n",
      "Iteration 217/1072, Loss: 7.5058\n",
      "Iteration 218/1072, Loss: 7.2656\n",
      "Iteration 219/1072, Loss: 7.3615\n",
      "Iteration 220/1072, Loss: 7.4109\n",
      "Iteration 221/1072, Loss: 7.6003\n",
      "Iteration 222/1072, Loss: 7.5237\n",
      "Iteration 223/1072, Loss: 7.3368\n",
      "Iteration 224/1072, Loss: 7.3266\n",
      "Iteration 225/1072, Loss: 7.3844\n",
      "Iteration 226/1072, Loss: 7.4355\n",
      "Iteration 227/1072, Loss: 7.4196\n",
      "Iteration 228/1072, Loss: 7.2860\n",
      "Iteration 229/1072, Loss: 7.4564\n",
      "Iteration 230/1072, Loss: 7.4719\n",
      "Iteration 231/1072, Loss: 7.3904\n",
      "Iteration 232/1072, Loss: 7.3801\n",
      "Iteration 233/1072, Loss: 7.3815\n",
      "Iteration 234/1072, Loss: 7.4155\n",
      "Iteration 235/1072, Loss: 7.3697\n",
      "Iteration 236/1072, Loss: 7.4116\n",
      "Iteration 237/1072, Loss: 7.3712\n",
      "Iteration 238/1072, Loss: 7.5062\n",
      "Iteration 239/1072, Loss: 7.5001\n",
      "Iteration 240/1072, Loss: 7.5460\n",
      "Iteration 241/1072, Loss: 7.4863\n",
      "Iteration 242/1072, Loss: 7.4690\n",
      "Iteration 243/1072, Loss: 7.5377\n",
      "Iteration 244/1072, Loss: 7.4701\n",
      "Iteration 245/1072, Loss: 7.4037\n",
      "Iteration 246/1072, Loss: 7.4435\n",
      "Iteration 247/1072, Loss: 7.4061\n",
      "Iteration 248/1072, Loss: 7.4721\n",
      "Iteration 249/1072, Loss: 7.3351\n",
      "Iteration 250/1072, Loss: 7.2457\n",
      "Iteration 251/1072, Loss: 7.2541\n",
      "Iteration 252/1072, Loss: 7.4059\n",
      "Iteration 253/1072, Loss: 7.4079\n",
      "Iteration 254/1072, Loss: 7.4356\n",
      "Iteration 255/1072, Loss: 7.3497\n",
      "Iteration 256/1072, Loss: 7.4488\n",
      "Iteration 257/1072, Loss: 7.3941\n",
      "Iteration 258/1072, Loss: 7.3918\n",
      "Iteration 259/1072, Loss: 7.4726\n",
      "Iteration 260/1072, Loss: 7.1698\n",
      "Iteration 261/1072, Loss: 7.3741\n",
      "Iteration 262/1072, Loss: 7.3341\n",
      "Iteration 263/1072, Loss: 7.4093\n",
      "Iteration 264/1072, Loss: 7.5672\n",
      "Iteration 265/1072, Loss: 7.4366\n",
      "Iteration 266/1072, Loss: 7.6027\n",
      "Iteration 267/1072, Loss: 7.2199\n",
      "Iteration 268/1072, Loss: 7.3798\n",
      "Iteration 269/1072, Loss: 7.3236\n",
      "Iteration 270/1072, Loss: 7.3861\n",
      "Iteration 271/1072, Loss: 7.3558\n",
      "Iteration 272/1072, Loss: 7.2533\n",
      "Iteration 273/1072, Loss: 7.3170\n",
      "Iteration 274/1072, Loss: 7.3532\n",
      "Iteration 275/1072, Loss: 7.5675\n",
      "Iteration 276/1072, Loss: 7.6075\n",
      "Iteration 277/1072, Loss: 7.5629\n",
      "Iteration 278/1072, Loss: 7.4677\n",
      "Iteration 279/1072, Loss: 7.5673\n",
      "Iteration 280/1072, Loss: 7.4193\n",
      "Iteration 281/1072, Loss: 7.5853\n",
      "Iteration 282/1072, Loss: 7.4089\n",
      "Iteration 283/1072, Loss: 7.5485\n",
      "Iteration 284/1072, Loss: 7.3736\n",
      "Iteration 285/1072, Loss: 7.5315\n",
      "Iteration 286/1072, Loss: 7.4776\n",
      "Iteration 287/1072, Loss: 7.5201\n",
      "Iteration 288/1072, Loss: 7.3712\n",
      "Iteration 289/1072, Loss: 7.2916\n",
      "Iteration 290/1072, Loss: 7.3071\n",
      "Iteration 291/1072, Loss: 7.4158\n",
      "Iteration 292/1072, Loss: 7.3281\n",
      "Iteration 293/1072, Loss: 7.4272\n",
      "Iteration 294/1072, Loss: 7.3135\n",
      "Iteration 295/1072, Loss: 7.3356\n",
      "Iteration 296/1072, Loss: 7.4920\n",
      "Iteration 297/1072, Loss: 7.3495\n",
      "Iteration 298/1072, Loss: 7.4305\n",
      "Iteration 299/1072, Loss: 7.3580\n",
      "Iteration 300/1072, Loss: 7.2255\n",
      "Iteration 301/1072, Loss: 7.3779\n",
      "Iteration 302/1072, Loss: 7.5311\n",
      "Iteration 303/1072, Loss: 7.3595\n",
      "Iteration 304/1072, Loss: 7.3529\n",
      "Iteration 305/1072, Loss: 7.5017\n",
      "Iteration 306/1072, Loss: 7.5054\n",
      "Iteration 307/1072, Loss: 7.2772\n",
      "Iteration 308/1072, Loss: 7.5108\n",
      "Iteration 309/1072, Loss: 7.5832\n",
      "Iteration 310/1072, Loss: 7.4337\n",
      "Iteration 311/1072, Loss: 7.3910\n",
      "Iteration 312/1072, Loss: 7.4163\n",
      "Iteration 313/1072, Loss: 7.4082\n",
      "Iteration 314/1072, Loss: 7.4052\n",
      "Iteration 315/1072, Loss: 7.4260\n",
      "Iteration 316/1072, Loss: 7.5834\n",
      "Iteration 317/1072, Loss: 7.4974\n",
      "Iteration 318/1072, Loss: 7.4412\n",
      "Iteration 319/1072, Loss: 7.3398\n",
      "Iteration 320/1072, Loss: 7.3280\n",
      "Iteration 321/1072, Loss: 7.5163\n",
      "Iteration 322/1072, Loss: 7.3849\n",
      "Iteration 323/1072, Loss: 7.3066\n",
      "Iteration 324/1072, Loss: 7.5560\n",
      "Iteration 325/1072, Loss: 7.3669\n",
      "Iteration 326/1072, Loss: 7.2609\n",
      "Iteration 327/1072, Loss: 7.4943\n",
      "Iteration 328/1072, Loss: 7.3574\n",
      "Iteration 329/1072, Loss: 7.3374\n",
      "Iteration 330/1072, Loss: 7.3785\n",
      "Iteration 331/1072, Loss: 7.3446\n",
      "Iteration 332/1072, Loss: 7.4760\n",
      "Iteration 333/1072, Loss: 7.1722\n",
      "Iteration 334/1072, Loss: 7.4966\n",
      "Iteration 335/1072, Loss: 7.2910\n",
      "Iteration 336/1072, Loss: 7.3491\n",
      "Iteration 337/1072, Loss: 7.4734\n",
      "Iteration 338/1072, Loss: 7.4565\n",
      "Iteration 339/1072, Loss: 7.3611\n",
      "Iteration 340/1072, Loss: 7.4141\n",
      "Iteration 341/1072, Loss: 7.5317\n",
      "Iteration 342/1072, Loss: 7.2440\n",
      "Iteration 343/1072, Loss: 7.4478\n",
      "Iteration 344/1072, Loss: 7.5495\n",
      "Iteration 345/1072, Loss: 7.4399\n",
      "Iteration 346/1072, Loss: 7.2879\n",
      "Iteration 347/1072, Loss: 7.4547\n",
      "Iteration 348/1072, Loss: 7.3169\n",
      "Iteration 349/1072, Loss: 7.4103\n",
      "Iteration 350/1072, Loss: 7.4451\n",
      "Iteration 351/1072, Loss: 7.4688\n",
      "Iteration 352/1072, Loss: 7.3399\n",
      "Iteration 353/1072, Loss: 7.2633\n",
      "Iteration 354/1072, Loss: 7.4571\n",
      "Iteration 355/1072, Loss: 7.3985\n",
      "Iteration 356/1072, Loss: 7.4926\n",
      "Iteration 357/1072, Loss: 7.4687\n",
      "Iteration 358/1072, Loss: 7.3657\n",
      "Iteration 359/1072, Loss: 7.3634\n",
      "Iteration 360/1072, Loss: 7.4762\n",
      "Iteration 361/1072, Loss: 7.2714\n",
      "Iteration 362/1072, Loss: 7.4810\n",
      "Iteration 363/1072, Loss: 7.3538\n",
      "Iteration 364/1072, Loss: 7.3613\n",
      "Iteration 365/1072, Loss: 7.5544\n",
      "Iteration 366/1072, Loss: 7.4474\n",
      "Iteration 367/1072, Loss: 7.3646\n",
      "Iteration 368/1072, Loss: 7.5137\n",
      "Iteration 369/1072, Loss: 7.4293\n",
      "Iteration 370/1072, Loss: 7.3201\n",
      "Iteration 371/1072, Loss: 7.4383\n",
      "Iteration 372/1072, Loss: 7.4772\n",
      "Iteration 373/1072, Loss: 7.3962\n",
      "Iteration 374/1072, Loss: 7.3940\n",
      "Iteration 375/1072, Loss: 7.3706\n",
      "Iteration 376/1072, Loss: 7.5204\n",
      "Iteration 377/1072, Loss: 7.3889\n",
      "Iteration 378/1072, Loss: 7.4098\n",
      "Iteration 379/1072, Loss: 7.6021\n",
      "Iteration 380/1072, Loss: 7.4194\n",
      "Iteration 381/1072, Loss: 7.3652\n",
      "Iteration 382/1072, Loss: 7.1026\n",
      "Iteration 383/1072, Loss: 7.3868\n",
      "Iteration 384/1072, Loss: 7.3756\n",
      "Iteration 385/1072, Loss: 7.4371\n",
      "Iteration 386/1072, Loss: 7.3195\n",
      "Iteration 387/1072, Loss: 7.3599\n",
      "Iteration 388/1072, Loss: 7.4646\n",
      "Iteration 389/1072, Loss: 7.2918\n",
      "Iteration 390/1072, Loss: 7.4106\n",
      "Iteration 391/1072, Loss: 7.2849\n",
      "Iteration 392/1072, Loss: 7.4089\n",
      "Iteration 393/1072, Loss: 7.5140\n",
      "Iteration 394/1072, Loss: 7.5607\n",
      "Iteration 395/1072, Loss: 7.3364\n",
      "Iteration 396/1072, Loss: 7.3468\n",
      "Iteration 397/1072, Loss: 7.5828\n",
      "Iteration 398/1072, Loss: 7.4490\n",
      "Iteration 399/1072, Loss: 7.2854\n",
      "Iteration 400/1072, Loss: 7.3547\n",
      "Iteration 445/1072, Loss: 7.5677\n",
      "Iteration 446/1072, Loss: 7.4538\n",
      "Iteration 447/1072, Loss: 7.3539\n",
      "Iteration 448/1072, Loss: 7.4432\n",
      "Iteration 449/1072, Loss: 7.4812\n",
      "Iteration 450/1072, Loss: 7.3865\n",
      "Iteration 451/1072, Loss: 7.3945\n",
      "Iteration 452/1072, Loss: 7.5752\n",
      "Iteration 453/1072, Loss: 7.4718\n",
      "Iteration 454/1072, Loss: 7.3358\n",
      "Iteration 455/1072, Loss: 7.3593\n",
      "Iteration 456/1072, Loss: 7.3491\n",
      "Iteration 457/1072, Loss: 7.4874\n",
      "Iteration 458/1072, Loss: 7.3712\n",
      "Iteration 459/1072, Loss: 7.5802\n",
      "Iteration 460/1072, Loss: 7.4173\n",
      "Iteration 461/1072, Loss: 7.4347\n",
      "Iteration 462/1072, Loss: 7.0941\n",
      "Iteration 463/1072, Loss: 7.1819\n",
      "Iteration 464/1072, Loss: 7.0992\n",
      "Iteration 465/1072, Loss: 7.3549\n",
      "Iteration 466/1072, Loss: 7.2809\n",
      "Iteration 467/1072, Loss: 7.2046\n",
      "Iteration 468/1072, Loss: 6.9951\n",
      "Iteration 469/1072, Loss: 7.2852\n",
      "Iteration 470/1072, Loss: 7.5274\n",
      "Iteration 471/1072, Loss: 7.4273\n",
      "Iteration 472/1072, Loss: 7.4339\n",
      "Iteration 473/1072, Loss: 7.3333\n",
      "Iteration 474/1072, Loss: 7.3223\n",
      "Iteration 475/1072, Loss: 7.1284\n",
      "Iteration 476/1072, Loss: 7.1841\n",
      "Iteration 477/1072, Loss: 7.4190\n",
      "Iteration 478/1072, Loss: 7.3928\n",
      "Iteration 479/1072, Loss: 7.5053\n",
      "Iteration 480/1072, Loss: 7.2772\n",
      "Iteration 481/1072, Loss: 7.4659\n",
      "Iteration 482/1072, Loss: 7.3299\n",
      "Iteration 483/1072, Loss: 7.4959\n",
      "Iteration 484/1072, Loss: 7.4661\n",
      "Iteration 485/1072, Loss: 7.4986\n",
      "Iteration 486/1072, Loss: 7.1467\n",
      "Iteration 487/1072, Loss: 7.6047\n",
      "Iteration 488/1072, Loss: 7.3731\n",
      "Iteration 489/1072, Loss: 7.3515\n",
      "Iteration 490/1072, Loss: 7.4434\n",
      "Iteration 491/1072, Loss: 7.3058\n",
      "Iteration 492/1072, Loss: 7.5247\n",
      "Iteration 493/1072, Loss: 7.2035\n",
      "Iteration 494/1072, Loss: 7.3645\n",
      "Iteration 495/1072, Loss: 7.4164\n",
      "Iteration 496/1072, Loss: 7.4710\n",
      "Iteration 497/1072, Loss: 7.4769\n",
      "Iteration 498/1072, Loss: 7.5775\n",
      "Iteration 499/1072, Loss: 7.5588\n",
      "Iteration 500/1072, Loss: 7.4130\n",
      "Iteration 501/1072, Loss: 7.2680\n",
      "Iteration 502/1072, Loss: 7.2772\n",
      "Iteration 503/1072, Loss: 7.4162\n",
      "Iteration 504/1072, Loss: 7.5011\n",
      "Iteration 505/1072, Loss: 7.3398\n",
      "Iteration 506/1072, Loss: 7.4186\n",
      "Iteration 507/1072, Loss: 7.2988\n",
      "Iteration 508/1072, Loss: 7.3702\n",
      "Iteration 509/1072, Loss: 7.3528\n",
      "Iteration 510/1072, Loss: 7.2335\n",
      "Iteration 511/1072, Loss: 7.3525\n",
      "Iteration 512/1072, Loss: 7.2794\n",
      "Iteration 513/1072, Loss: 7.5097\n",
      "Iteration 514/1072, Loss: 7.4355\n",
      "Iteration 515/1072, Loss: 7.3488\n",
      "Iteration 516/1072, Loss: 7.3725\n",
      "Iteration 517/1072, Loss: 6.9055\n",
      "Iteration 518/1072, Loss: 7.1624\n",
      "Iteration 519/1072, Loss: 7.3360\n",
      "Iteration 520/1072, Loss: 7.3439\n",
      "Iteration 521/1072, Loss: 7.2787\n",
      "Iteration 522/1072, Loss: 7.4021\n",
      "Iteration 523/1072, Loss: 7.6844\n",
      "Iteration 524/1072, Loss: 7.5319\n",
      "Iteration 525/1072, Loss: 7.2241\n",
      "Iteration 526/1072, Loss: 7.5343\n",
      "Iteration 527/1072, Loss: 7.2996\n",
      "Iteration 528/1072, Loss: 7.4065\n",
      "Iteration 529/1072, Loss: 7.2721\n",
      "Iteration 530/1072, Loss: 7.4317\n",
      "Iteration 531/1072, Loss: 7.2970\n",
      "Iteration 532/1072, Loss: 7.4522\n",
      "Iteration 533/1072, Loss: 7.2998\n",
      "Iteration 534/1072, Loss: 7.2425\n",
      "Iteration 535/1072, Loss: 7.2941\n",
      "Iteration 536/1072, Loss: 7.4751\n",
      "Iteration 537/1072, Loss: 7.3358\n",
      "Iteration 538/1072, Loss: 7.5429\n",
      "Iteration 539/1072, Loss: 7.2915\n",
      "Iteration 540/1072, Loss: 7.1705\n",
      "Iteration 541/1072, Loss: 7.4888\n",
      "Iteration 542/1072, Loss: 7.0847\n",
      "Iteration 543/1072, Loss: 7.3686\n",
      "Iteration 544/1072, Loss: 7.4633\n",
      "Iteration 545/1072, Loss: 7.4259\n",
      "Iteration 546/1072, Loss: 7.2987\n",
      "Iteration 547/1072, Loss: 7.3759\n",
      "Iteration 548/1072, Loss: 7.4269\n",
      "Iteration 549/1072, Loss: 7.4248\n",
      "Iteration 550/1072, Loss: 7.3297\n",
      "Iteration 551/1072, Loss: 7.4018\n",
      "Iteration 552/1072, Loss: 7.3228\n",
      "Iteration 553/1072, Loss: 7.4978\n",
      "Iteration 554/1072, Loss: 7.4191\n",
      "Iteration 555/1072, Loss: 7.4643\n",
      "Iteration 556/1072, Loss: 7.2444\n",
      "Iteration 557/1072, Loss: 7.5549\n",
      "Iteration 558/1072, Loss: 7.3489\n",
      "Iteration 559/1072, Loss: 7.3301\n",
      "Iteration 560/1072, Loss: 7.3839\n",
      "Iteration 561/1072, Loss: 7.4108\n",
      "Iteration 562/1072, Loss: 7.3759\n",
      "Iteration 563/1072, Loss: 7.4416\n",
      "Iteration 564/1072, Loss: 7.4502\n",
      "Iteration 565/1072, Loss: 7.3522\n",
      "Iteration 566/1072, Loss: 7.1700\n",
      "Iteration 567/1072, Loss: 7.4665\n",
      "Iteration 568/1072, Loss: 7.1996\n",
      "Iteration 569/1072, Loss: 7.3535\n",
      "Iteration 570/1072, Loss: 7.5656\n",
      "Iteration 571/1072, Loss: 7.3671\n",
      "Iteration 572/1072, Loss: 7.5299\n",
      "Iteration 573/1072, Loss: 7.5424\n",
      "Iteration 574/1072, Loss: 7.3994\n",
      "Iteration 575/1072, Loss: 7.4760\n",
      "Iteration 576/1072, Loss: 7.2533\n",
      "Iteration 577/1072, Loss: 7.3428\n",
      "Iteration 578/1072, Loss: 7.4843\n",
      "Iteration 579/1072, Loss: 7.3549\n",
      "Iteration 580/1072, Loss: 7.3304\n",
      "Iteration 581/1072, Loss: 7.2822\n",
      "Iteration 582/1072, Loss: 7.3703\n",
      "Iteration 583/1072, Loss: 7.3924\n",
      "Iteration 584/1072, Loss: 7.5434\n",
      "Iteration 585/1072, Loss: 7.2559\n",
      "Iteration 586/1072, Loss: 7.4127\n",
      "Iteration 587/1072, Loss: 7.2761\n",
      "Iteration 588/1072, Loss: 7.2926\n",
      "Iteration 589/1072, Loss: 7.1768\n",
      "Iteration 590/1072, Loss: 7.5510\n",
      "Iteration 591/1072, Loss: 7.5650\n",
      "Iteration 592/1072, Loss: 7.4379\n",
      "Iteration 593/1072, Loss: 7.3324\n",
      "Iteration 594/1072, Loss: 7.3957\n",
      "Iteration 595/1072, Loss: 7.5179\n",
      "Iteration 596/1072, Loss: 7.2472\n",
      "Iteration 597/1072, Loss: 7.3922\n",
      "Iteration 598/1072, Loss: 7.5493\n",
      "Iteration 599/1072, Loss: 7.3767\n",
      "Iteration 600/1072, Loss: 7.4924\n",
      "Iteration 601/1072, Loss: 7.2484\n",
      "Iteration 602/1072, Loss: 7.4460\n",
      "Iteration 603/1072, Loss: 7.3684\n",
      "Iteration 604/1072, Loss: 7.2657\n",
      "Iteration 605/1072, Loss: 7.5495\n",
      "Iteration 606/1072, Loss: 7.4701\n",
      "Iteration 607/1072, Loss: 7.1165\n",
      "Iteration 608/1072, Loss: 7.3917\n",
      "Iteration 609/1072, Loss: 7.4842\n",
      "Iteration 610/1072, Loss: 7.3490\n",
      "Iteration 611/1072, Loss: 7.3328\n",
      "Iteration 612/1072, Loss: 7.3209\n",
      "Iteration 613/1072, Loss: 7.6065\n",
      "Iteration 614/1072, Loss: 7.3981\n",
      "Iteration 615/1072, Loss: 7.4521\n",
      "Iteration 616/1072, Loss: 7.4007\n",
      "Iteration 617/1072, Loss: 7.4448\n",
      "Iteration 618/1072, Loss: 7.4668\n",
      "Iteration 619/1072, Loss: 7.4373\n",
      "Iteration 620/1072, Loss: 7.2867\n",
      "Iteration 621/1072, Loss: 7.3480\n",
      "Iteration 622/1072, Loss: 7.3810\n",
      "Iteration 623/1072, Loss: 7.4573\n",
      "Iteration 624/1072, Loss: 7.5110\n",
      "Iteration 625/1072, Loss: 7.3869\n",
      "Iteration 626/1072, Loss: 7.2614\n",
      "Iteration 627/1072, Loss: 7.4754\n",
      "Iteration 628/1072, Loss: 7.4040\n",
      "Iteration 629/1072, Loss: 7.2922\n",
      "Iteration 630/1072, Loss: 7.2187\n",
      "Iteration 631/1072, Loss: 7.3548\n",
      "Iteration 632/1072, Loss: 7.4325\n",
      "Iteration 633/1072, Loss: 7.4157\n",
      "Iteration 634/1072, Loss: 7.5082\n",
      "Iteration 635/1072, Loss: 7.5209\n",
      "Iteration 636/1072, Loss: 7.2435\n",
      "Iteration 637/1072, Loss: 7.3824\n",
      "Iteration 638/1072, Loss: 7.4807\n",
      "Iteration 639/1072, Loss: 7.3477\n",
      "Iteration 640/1072, Loss: 7.3911\n",
      "Iteration 641/1072, Loss: 7.4102\n",
      "Iteration 642/1072, Loss: 7.4235\n",
      "Iteration 643/1072, Loss: 7.4421\n",
      "Iteration 644/1072, Loss: 7.3722\n",
      "Iteration 645/1072, Loss: 7.3616\n",
      "Iteration 646/1072, Loss: 7.2830\n",
      "Iteration 647/1072, Loss: 7.3342\n",
      "Iteration 648/1072, Loss: 7.5010\n",
      "Iteration 649/1072, Loss: 7.1758\n",
      "Iteration 650/1072, Loss: 7.2595\n",
      "Iteration 651/1072, Loss: 7.4912\n",
      "Iteration 652/1072, Loss: 7.4707\n",
      "Iteration 653/1072, Loss: 7.2235\n",
      "Iteration 654/1072, Loss: 7.4043\n",
      "Iteration 655/1072, Loss: 7.4942\n",
      "Iteration 656/1072, Loss: 7.3877\n",
      "Iteration 657/1072, Loss: 7.2568\n",
      "Iteration 658/1072, Loss: 7.3088\n",
      "Iteration 659/1072, Loss: 7.5820\n",
      "Iteration 660/1072, Loss: 7.4723\n",
      "Iteration 661/1072, Loss: 7.3926\n",
      "Iteration 662/1072, Loss: 7.4337\n",
      "Iteration 663/1072, Loss: 7.1469\n",
      "Iteration 664/1072, Loss: 7.5423\n",
      "Iteration 665/1072, Loss: 7.2222\n",
      "Iteration 666/1072, Loss: 7.3217\n",
      "Iteration 667/1072, Loss: 7.4012\n",
      "Iteration 668/1072, Loss: 7.4776\n",
      "Iteration 669/1072, Loss: 7.4004\n",
      "Iteration 670/1072, Loss: 7.1772\n",
      "Iteration 671/1072, Loss: 7.3289\n",
      "Iteration 672/1072, Loss: 7.2178\n",
      "Iteration 673/1072, Loss: 7.3104\n",
      "Iteration 674/1072, Loss: 7.3696\n",
      "Iteration 675/1072, Loss: 7.4919\n",
      "Iteration 676/1072, Loss: 7.4206\n",
      "Iteration 677/1072, Loss: 7.2892\n",
      "Iteration 678/1072, Loss: 7.4236\n",
      "Iteration 679/1072, Loss: 7.4430\n",
      "Iteration 680/1072, Loss: 7.0385\n",
      "Iteration 681/1072, Loss: 7.4843\n",
      "Iteration 682/1072, Loss: 7.2823\n",
      "Iteration 683/1072, Loss: 7.2250\n",
      "Iteration 684/1072, Loss: 7.3249\n",
      "Iteration 685/1072, Loss: 7.3750\n",
      "Iteration 686/1072, Loss: 7.4531\n",
      "Iteration 687/1072, Loss: 7.4398\n",
      "Iteration 688/1072, Loss: 7.3465\n",
      "Iteration 689/1072, Loss: 7.3233\n",
      "Iteration 690/1072, Loss: 7.2664\n",
      "Iteration 691/1072, Loss: 7.2519\n",
      "Iteration 692/1072, Loss: 7.3933\n",
      "Iteration 693/1072, Loss: 7.2351\n",
      "Iteration 694/1072, Loss: 7.4014\n",
      "Iteration 695/1072, Loss: 7.4715\n",
      "Iteration 696/1072, Loss: 7.2019\n",
      "Iteration 697/1072, Loss: 7.4049\n",
      "Iteration 698/1072, Loss: 7.3212\n",
      "Iteration 699/1072, Loss: 7.3651\n",
      "Iteration 700/1072, Loss: 7.3808\n",
      "Iteration 701/1072, Loss: 7.4943\n",
      "Iteration 702/1072, Loss: 7.4156\n",
      "Iteration 703/1072, Loss: 7.2938\n",
      "Iteration 704/1072, Loss: 7.4606\n",
      "Iteration 705/1072, Loss: 7.4382\n",
      "Iteration 706/1072, Loss: 7.3247\n",
      "Iteration 707/1072, Loss: 7.4440\n",
      "Iteration 708/1072, Loss: 7.5175\n",
      "Iteration 709/1072, Loss: 7.3626\n",
      "Iteration 710/1072, Loss: 7.4232\n",
      "Iteration 711/1072, Loss: 7.3702\n",
      "Iteration 712/1072, Loss: 7.3965\n",
      "Iteration 713/1072, Loss: 7.4493\n",
      "Iteration 714/1072, Loss: 7.3641\n",
      "Iteration 715/1072, Loss: 7.6050\n",
      "Iteration 716/1072, Loss: 7.3464\n",
      "Iteration 717/1072, Loss: 7.3109\n",
      "Iteration 718/1072, Loss: 7.3071\n",
      "Iteration 719/1072, Loss: 7.3622\n",
      "Iteration 720/1072, Loss: 7.4239\n",
      "Iteration 721/1072, Loss: 7.4440\n",
      "Iteration 722/1072, Loss: 7.2847\n",
      "Iteration 723/1072, Loss: 7.4430\n",
      "Iteration 724/1072, Loss: 7.3132\n",
      "Iteration 725/1072, Loss: 7.4147\n",
      "Iteration 726/1072, Loss: 7.3268\n",
      "Iteration 727/1072, Loss: 7.1811\n",
      "Iteration 728/1072, Loss: 7.3613\n",
      "Iteration 729/1072, Loss: 7.1789\n",
      "Iteration 730/1072, Loss: 6.9799\n",
      "Iteration 731/1072, Loss: 7.3283\n",
      "Iteration 732/1072, Loss: 7.2621\n",
      "Iteration 733/1072, Loss: 7.4589\n",
      "Iteration 734/1072, Loss: 7.5186\n",
      "Iteration 735/1072, Loss: 7.4880\n",
      "Iteration 736/1072, Loss: 7.4329\n",
      "Iteration 737/1072, Loss: 7.1252\n",
      "Iteration 738/1072, Loss: 7.2787\n",
      "Iteration 739/1072, Loss: 7.3300\n",
      "Iteration 740/1072, Loss: 7.3364\n",
      "Iteration 741/1072, Loss: 7.2039\n",
      "Iteration 742/1072, Loss: 7.5740\n",
      "Iteration 743/1072, Loss: 7.2297\n",
      "Iteration 744/1072, Loss: 7.3459\n",
      "Iteration 745/1072, Loss: 7.3944\n",
      "Iteration 746/1072, Loss: 7.3071\n",
      "Iteration 747/1072, Loss: 7.2853\n",
      "Iteration 748/1072, Loss: 7.3030\n",
      "Iteration 749/1072, Loss: 7.2235\n",
      "Iteration 750/1072, Loss: 7.4214\n",
      "Iteration 751/1072, Loss: 7.3064\n",
      "Iteration 752/1072, Loss: 7.3365\n",
      "Iteration 753/1072, Loss: 7.4849\n",
      "Iteration 754/1072, Loss: 7.4093\n",
      "Iteration 755/1072, Loss: 7.2547\n",
      "Iteration 756/1072, Loss: 7.2179\n",
      "Iteration 757/1072, Loss: 7.4030\n",
      "Iteration 758/1072, Loss: 7.3283\n",
      "Iteration 759/1072, Loss: 7.4962\n",
      "Iteration 760/1072, Loss: 7.2278\n",
      "Iteration 761/1072, Loss: 7.3379\n",
      "Iteration 762/1072, Loss: 7.3691\n",
      "Iteration 763/1072, Loss: 7.4762\n",
      "Iteration 764/1072, Loss: 7.4148\n",
      "Iteration 765/1072, Loss: 7.4298\n",
      "Iteration 766/1072, Loss: 7.4171\n",
      "Iteration 767/1072, Loss: 7.5331\n",
      "Iteration 768/1072, Loss: 7.3698\n",
      "Iteration 769/1072, Loss: 7.4311\n",
      "Iteration 770/1072, Loss: 7.4719\n",
      "Iteration 771/1072, Loss: 7.2921\n",
      "Iteration 772/1072, Loss: 7.5260\n",
      "Iteration 773/1072, Loss: 7.2925\n",
      "Iteration 774/1072, Loss: 7.3332\n",
      "Iteration 775/1072, Loss: 7.4808\n",
      "Iteration 776/1072, Loss: 7.4109\n",
      "Iteration 777/1072, Loss: 7.4608\n",
      "Iteration 778/1072, Loss: 7.1846\n",
      "Iteration 779/1072, Loss: 7.5055\n",
      "Iteration 780/1072, Loss: 7.3549\n",
      "Iteration 781/1072, Loss: 7.5560\n",
      "Iteration 782/1072, Loss: 7.0923\n",
      "Iteration 783/1072, Loss: 7.3306\n",
      "Iteration 784/1072, Loss: 7.3770\n",
      "Iteration 785/1072, Loss: 7.2344\n",
      "Iteration 786/1072, Loss: 7.2085\n",
      "Iteration 787/1072, Loss: 7.1657\n",
      "Iteration 788/1072, Loss: 7.4625\n",
      "Iteration 789/1072, Loss: 7.3474\n",
      "Iteration 790/1072, Loss: 7.4371\n",
      "Iteration 791/1072, Loss: 7.2436\n",
      "Iteration 792/1072, Loss: 7.3937\n",
      "Iteration 793/1072, Loss: 7.4392\n",
      "Iteration 794/1072, Loss: 7.3867\n",
      "Iteration 795/1072, Loss: 7.6043\n",
      "Iteration 796/1072, Loss: 7.2841\n",
      "Iteration 797/1072, Loss: 7.5428\n",
      "Iteration 798/1072, Loss: 7.3368\n",
      "Iteration 799/1072, Loss: 7.5959\n",
      "Iteration 800/1072, Loss: 7.1669\n",
      "Iteration 801/1072, Loss: 7.3940\n",
      "Iteration 802/1072, Loss: 7.1805\n",
      "Iteration 803/1072, Loss: 7.3368\n",
      "Iteration 804/1072, Loss: 7.4634\n",
      "Iteration 805/1072, Loss: 7.3993\n",
      "Iteration 806/1072, Loss: 7.3625\n",
      "Iteration 807/1072, Loss: 7.5385\n",
      "Iteration 808/1072, Loss: 7.3205\n",
      "Iteration 809/1072, Loss: 7.3243\n",
      "Iteration 810/1072, Loss: 7.5273\n",
      "Iteration 811/1072, Loss: 7.3396\n",
      "Iteration 812/1072, Loss: 7.3764\n",
      "Iteration 813/1072, Loss: 7.3541\n",
      "Iteration 814/1072, Loss: 7.4843\n",
      "Iteration 815/1072, Loss: 7.3734\n",
      "Iteration 816/1072, Loss: 7.3734\n",
      "Iteration 817/1072, Loss: 7.3108\n",
      "Iteration 818/1072, Loss: 7.4643\n",
      "Iteration 819/1072, Loss: 7.3423\n",
      "Iteration 820/1072, Loss: 7.5629\n",
      "Iteration 821/1072, Loss: 7.4146\n",
      "Iteration 822/1072, Loss: 7.3727\n",
      "Iteration 823/1072, Loss: 7.3468\n",
      "Iteration 824/1072, Loss: 7.2569\n",
      "Iteration 825/1072, Loss: 7.2541\n",
      "Iteration 826/1072, Loss: 7.1622\n",
      "Iteration 827/1072, Loss: 7.4720\n",
      "Iteration 828/1072, Loss: 7.2210\n",
      "Iteration 829/1072, Loss: 7.2787\n",
      "Iteration 830/1072, Loss: 7.3958\n",
      "Iteration 831/1072, Loss: 7.3166\n",
      "Iteration 832/1072, Loss: 7.2617\n",
      "Iteration 833/1072, Loss: 7.3419\n",
      "Iteration 834/1072, Loss: 7.2454\n",
      "Iteration 835/1072, Loss: 7.3010\n",
      "Iteration 836/1072, Loss: 7.5224\n",
      "Iteration 837/1072, Loss: 7.3718\n",
      "Iteration 838/1072, Loss: 7.6364\n",
      "Iteration 839/1072, Loss: 7.4486\n",
      "Iteration 840/1072, Loss: 7.3956\n",
      "Iteration 841/1072, Loss: 7.5380\n",
      "Iteration 842/1072, Loss: 7.3367\n",
      "Iteration 843/1072, Loss: 7.3834\n",
      "Iteration 844/1072, Loss: 7.3589\n",
      "Iteration 845/1072, Loss: 7.3398\n",
      "Iteration 846/1072, Loss: 7.3762\n",
      "Iteration 847/1072, Loss: 7.3382\n",
      "Iteration 848/1072, Loss: 7.4695\n",
      "Iteration 849/1072, Loss: 7.3768\n",
      "Iteration 850/1072, Loss: 7.4860\n",
      "Iteration 851/1072, Loss: 7.2608\n",
      "Iteration 852/1072, Loss: 7.3933\n",
      "Iteration 853/1072, Loss: 7.2813\n",
      "Iteration 854/1072, Loss: 7.2960\n",
      "Iteration 855/1072, Loss: 7.1516\n",
      "Iteration 856/1072, Loss: 7.2457\n",
      "Iteration 857/1072, Loss: 7.2895\n",
      "Iteration 858/1072, Loss: 7.3299\n",
      "Iteration 859/1072, Loss: 7.3975\n",
      "Iteration 860/1072, Loss: 7.2374\n",
      "Iteration 861/1072, Loss: 7.3248\n",
      "Iteration 862/1072, Loss: 7.3057\n",
      "Iteration 863/1072, Loss: 7.3637\n",
      "Iteration 864/1072, Loss: 7.4245\n",
      "Iteration 865/1072, Loss: 7.4815\n",
      "Iteration 866/1072, Loss: 7.2523\n",
      "Iteration 867/1072, Loss: 7.3574\n",
      "Iteration 868/1072, Loss: 7.4051\n",
      "Iteration 869/1072, Loss: 7.2984\n",
      "Iteration 870/1072, Loss: 7.2826\n",
      "Iteration 871/1072, Loss: 7.3143\n",
      "Iteration 872/1072, Loss: 7.2543\n",
      "Iteration 873/1072, Loss: 7.4654\n",
      "Iteration 874/1072, Loss: 7.4053\n",
      "Iteration 875/1072, Loss: 7.4875\n",
      "Iteration 876/1072, Loss: 7.4516\n",
      "Iteration 877/1072, Loss: 7.2432\n",
      "Iteration 878/1072, Loss: 7.5527\n",
      "Iteration 879/1072, Loss: 7.4125\n",
      "Iteration 880/1072, Loss: 7.3772\n",
      "Iteration 881/1072, Loss: 7.4015\n",
      "Iteration 882/1072, Loss: 7.5007\n",
      "Iteration 883/1072, Loss: 7.2856\n",
      "Iteration 884/1072, Loss: 6.9891\n",
      "Iteration 885/1072, Loss: 7.5047\n",
      "Iteration 886/1072, Loss: 7.3690\n",
      "Iteration 887/1072, Loss: 7.2370\n",
      "Iteration 888/1072, Loss: 7.5497\n",
      "Iteration 889/1072, Loss: 7.4907\n",
      "Iteration 890/1072, Loss: 7.4426\n",
      "Iteration 891/1072, Loss: 7.4868\n",
      "Iteration 892/1072, Loss: 7.5006\n",
      "Iteration 893/1072, Loss: 7.5237\n",
      "Iteration 894/1072, Loss: 7.2092\n",
      "Iteration 895/1072, Loss: 7.2058\n",
      "Iteration 896/1072, Loss: 7.2718\n",
      "Iteration 897/1072, Loss: 7.3512\n",
      "Iteration 898/1072, Loss: 7.3926\n",
      "Iteration 899/1072, Loss: 7.3366\n",
      "Iteration 900/1072, Loss: 7.2528\n",
      "Iteration 901/1072, Loss: 7.3434\n",
      "Iteration 902/1072, Loss: 7.4052\n",
      "Iteration 903/1072, Loss: 7.4713\n",
      "Iteration 904/1072, Loss: 7.1982\n",
      "Iteration 905/1072, Loss: 7.3484\n",
      "Iteration 906/1072, Loss: 7.5125\n",
      "Iteration 907/1072, Loss: 7.2767\n",
      "Iteration 908/1072, Loss: 7.4176\n",
      "Iteration 909/1072, Loss: 7.2893\n",
      "Iteration 910/1072, Loss: 7.3173\n",
      "Iteration 911/1072, Loss: 7.3513\n",
      "Iteration 912/1072, Loss: 7.3744\n",
      "Iteration 913/1072, Loss: 7.4552\n",
      "Iteration 914/1072, Loss: 7.3687\n",
      "Iteration 915/1072, Loss: 7.3150\n",
      "Iteration 916/1072, Loss: 7.1971\n",
      "Iteration 917/1072, Loss: 7.4144\n",
      "Iteration 918/1072, Loss: 7.3276\n",
      "Iteration 919/1072, Loss: 7.2453\n",
      "Iteration 920/1072, Loss: 7.3463\n",
      "Iteration 921/1072, Loss: 7.2846\n",
      "Iteration 922/1072, Loss: 7.5864\n",
      "Iteration 923/1072, Loss: 7.2899\n",
      "Iteration 924/1072, Loss: 7.3630\n",
      "Iteration 925/1072, Loss: 7.0457\n",
      "Iteration 926/1072, Loss: 7.2660\n",
      "Iteration 927/1072, Loss: 7.2525\n",
      "Iteration 928/1072, Loss: 7.3094\n",
      "Iteration 929/1072, Loss: 7.3257\n",
      "Iteration 930/1072, Loss: 7.6302\n",
      "Iteration 931/1072, Loss: 7.3912\n",
      "Iteration 932/1072, Loss: 7.5294\n",
      "Iteration 933/1072, Loss: 7.2465\n",
      "Iteration 934/1072, Loss: 7.3488\n",
      "Iteration 935/1072, Loss: 7.0014\n",
      "Iteration 936/1072, Loss: 7.5496\n",
      "Iteration 937/1072, Loss: 7.5648\n",
      "Iteration 938/1072, Loss: 7.3405\n",
      "Iteration 939/1072, Loss: 7.4698\n",
      "Iteration 940/1072, Loss: 7.3922\n",
      "Iteration 941/1072, Loss: 7.4582\n",
      "Iteration 942/1072, Loss: 7.3191\n",
      "Iteration 943/1072, Loss: 7.2693\n",
      "Iteration 944/1072, Loss: 7.3575\n",
      "Iteration 945/1072, Loss: 7.3662\n",
      "Iteration 946/1072, Loss: 7.4987\n",
      "Iteration 947/1072, Loss: 7.4196\n",
      "Iteration 948/1072, Loss: 7.2263\n",
      "Iteration 949/1072, Loss: 7.3230\n",
      "Iteration 950/1072, Loss: 7.5015\n",
      "Iteration 951/1072, Loss: 7.2039\n",
      "Iteration 952/1072, Loss: 7.5211\n",
      "Iteration 953/1072, Loss: 7.3107\n",
      "Iteration 954/1072, Loss: 7.6065\n",
      "Iteration 955/1072, Loss: 7.4591\n",
      "Iteration 956/1072, Loss: 7.4185\n",
      "Iteration 957/1072, Loss: 7.2173\n",
      "Iteration 958/1072, Loss: 7.5151\n",
      "Iteration 959/1072, Loss: 7.3879\n",
      "Iteration 960/1072, Loss: 7.5450\n",
      "Iteration 961/1072, Loss: 7.4745\n",
      "Iteration 962/1072, Loss: 7.5423\n",
      "Iteration 963/1072, Loss: 7.3782\n",
      "Iteration 964/1072, Loss: 7.2068\n",
      "Iteration 965/1072, Loss: 7.4640\n",
      "Iteration 966/1072, Loss: 7.2535\n",
      "Iteration 967/1072, Loss: 7.4945\n",
      "Iteration 968/1072, Loss: 7.3765\n",
      "Iteration 969/1072, Loss: 7.4177\n",
      "Iteration 970/1072, Loss: 7.1127\n",
      "Iteration 971/1072, Loss: 7.4425\n",
      "Iteration 972/1072, Loss: 7.3592\n",
      "Iteration 973/1072, Loss: 7.3797\n",
      "Iteration 974/1072, Loss: 7.2752\n",
      "Iteration 975/1072, Loss: 7.1871\n",
      "Iteration 976/1072, Loss: 7.3712\n",
      "Iteration 977/1072, Loss: 7.4087\n",
      "Iteration 978/1072, Loss: 7.4151\n",
      "Iteration 979/1072, Loss: 7.3265\n",
      "Iteration 980/1072, Loss: 7.4327\n",
      "Iteration 981/1072, Loss: 7.5325\n",
      "Iteration 982/1072, Loss: 7.4154\n",
      "Iteration 983/1072, Loss: 7.3862\n",
      "Iteration 984/1072, Loss: 7.3944\n",
      "Iteration 985/1072, Loss: 7.4771\n",
      "Iteration 986/1072, Loss: 7.0416\n",
      "Iteration 987/1072, Loss: 7.5064\n",
      "Iteration 988/1072, Loss: 7.3272\n",
      "Iteration 989/1072, Loss: 7.1457\n",
      "Iteration 990/1072, Loss: 7.3731\n",
      "Iteration 991/1072, Loss: 7.2599\n",
      "Iteration 992/1072, Loss: 7.2055\n",
      "Iteration 993/1072, Loss: 6.9787\n",
      "Iteration 994/1072, Loss: 7.5281\n",
      "Iteration 995/1072, Loss: 7.2732\n",
      "Iteration 996/1072, Loss: 7.4710\n",
      "Iteration 997/1072, Loss: 7.3588\n",
      "Iteration 998/1072, Loss: 7.2997\n",
      "Iteration 999/1072, Loss: 7.4671\n",
      "Iteration 1000/1072, Loss: 7.3944\n",
      "Iteration 1001/1072, Loss: 7.2628\n",
      "Iteration 1002/1072, Loss: 7.3225\n",
      "Iteration 1003/1072, Loss: 7.3670\n",
      "Iteration 1004/1072, Loss: 7.3853\n",
      "Iteration 1005/1072, Loss: 7.4273\n",
      "Iteration 1006/1072, Loss: 7.2107\n",
      "Iteration 1007/1072, Loss: 7.2888\n",
      "Iteration 1008/1072, Loss: 7.5457\n",
      "Iteration 1009/1072, Loss: 7.2323\n",
      "Iteration 1010/1072, Loss: 7.3293\n",
      "Iteration 1011/1072, Loss: 7.1730\n",
      "Iteration 1012/1072, Loss: 7.2169\n",
      "Iteration 1013/1072, Loss: 7.3694\n",
      "Iteration 1014/1072, Loss: 7.1042\n",
      "Iteration 913/1072, Loss: 7.2862\n",
      "Iteration 914/1072, Loss: 6.9315\n",
      "Iteration 915/1072, Loss: 7.0604\n",
      "Iteration 916/1072, Loss: 7.3122\n",
      "Iteration 917/1072, Loss: 7.2524\n",
      "Iteration 918/1072, Loss: 7.0394\n",
      "Iteration 919/1072, Loss: 6.9799\n",
      "Iteration 920/1072, Loss: 7.2469\n",
      "Iteration 921/1072, Loss: 7.2056\n",
      "Iteration 922/1072, Loss: 7.3625\n",
      "Iteration 923/1072, Loss: 6.8272\n",
      "Iteration 924/1072, Loss: 7.4127\n",
      "Iteration 925/1072, Loss: 7.0853\n",
      "Iteration 926/1072, Loss: 7.1674\n",
      "Iteration 927/1072, Loss: 7.0542\n",
      "Iteration 928/1072, Loss: 7.2581\n",
      "Iteration 929/1072, Loss: 7.1412\n",
      "Iteration 930/1072, Loss: 7.3222\n",
      "Iteration 931/1072, Loss: 6.8457\n",
      "Iteration 932/1072, Loss: 7.1699\n",
      "Iteration 933/1072, Loss: 6.9897\n",
      "Iteration 934/1072, Loss: 7.1744\n",
      "Iteration 935/1072, Loss: 6.8878\n",
      "Iteration 936/1072, Loss: 7.1763\n",
      "Iteration 937/1072, Loss: 7.4022\n",
      "Iteration 938/1072, Loss: 7.3389\n",
      "Iteration 939/1072, Loss: 7.0611\n",
      "Iteration 940/1072, Loss: 7.2106\n",
      "Iteration 941/1072, Loss: 7.3968\n",
      "Iteration 942/1072, Loss: 7.2321\n",
      "Iteration 943/1072, Loss: 7.2982\n",
      "Iteration 944/1072, Loss: 7.3578\n",
      "Iteration 945/1072, Loss: 7.2612\n",
      "Iteration 946/1072, Loss: 7.1538\n",
      "Iteration 947/1072, Loss: 7.2399\n",
      "Iteration 948/1072, Loss: 7.1536\n",
      "Iteration 949/1072, Loss: 7.2516\n",
      "Iteration 950/1072, Loss: 7.2740\n",
      "Iteration 951/1072, Loss: 7.0597\n",
      "Iteration 952/1072, Loss: 7.1207\n",
      "Iteration 953/1072, Loss: 7.0921\n",
      "Iteration 954/1072, Loss: 7.3840\n",
      "Iteration 955/1072, Loss: 7.4288\n",
      "Iteration 956/1072, Loss: 7.1902\n",
      "Iteration 957/1072, Loss: 7.1275\n",
      "Iteration 958/1072, Loss: 7.4503\n",
      "Iteration 959/1072, Loss: 7.2668\n",
      "Iteration 960/1072, Loss: 7.2066\n",
      "Iteration 961/1072, Loss: 7.0496\n",
      "Iteration 962/1072, Loss: 7.0497\n",
      "Iteration 963/1072, Loss: 7.3989\n",
      "Iteration 964/1072, Loss: 7.2443\n",
      "Iteration 965/1072, Loss: 7.0169\n",
      "Iteration 966/1072, Loss: 7.1870\n",
      "Iteration 967/1072, Loss: 7.2704\n",
      "Iteration 968/1072, Loss: 7.3678\n",
      "Iteration 969/1072, Loss: 7.4079\n",
      "Iteration 970/1072, Loss: 7.2274\n",
      "Iteration 971/1072, Loss: 7.2591\n",
      "Iteration 972/1072, Loss: 6.9815\n",
      "Iteration 973/1072, Loss: 7.2052\n",
      "Iteration 974/1072, Loss: 6.9119\n",
      "Iteration 975/1072, Loss: 7.1544\n",
      "Iteration 976/1072, Loss: 7.1162\n",
      "Iteration 977/1072, Loss: 7.1020\n",
      "Iteration 978/1072, Loss: 7.1517\n",
      "Iteration 979/1072, Loss: 6.9257\n",
      "Iteration 980/1072, Loss: 6.8162\n",
      "Iteration 981/1072, Loss: 7.2061\n",
      "Iteration 982/1072, Loss: 7.1058\n",
      "Iteration 983/1072, Loss: 7.0729\n",
      "Iteration 984/1072, Loss: 7.3780\n",
      "Iteration 985/1072, Loss: 7.2390\n",
      "Iteration 986/1072, Loss: 7.1693\n",
      "Iteration 987/1072, Loss: 7.3455\n",
      "Iteration 988/1072, Loss: 7.0855\n",
      "Iteration 989/1072, Loss: 6.9759\n",
      "Iteration 990/1072, Loss: 7.3333\n",
      "Iteration 991/1072, Loss: 7.0702\n",
      "Iteration 992/1072, Loss: 7.3594\n",
      "Iteration 993/1072, Loss: 7.0888\n",
      "Iteration 994/1072, Loss: 7.0867\n",
      "Iteration 995/1072, Loss: 6.8960\n",
      "Iteration 996/1072, Loss: 7.0230\n",
      "Iteration 997/1072, Loss: 7.3293\n",
      "Iteration 998/1072, Loss: 7.4058\n",
      "Iteration 999/1072, Loss: 7.3319\n",
      "Iteration 1000/1072, Loss: 6.9131\n",
      "Iteration 1001/1072, Loss: 7.1399\n",
      "Iteration 1002/1072, Loss: 7.1120\n",
      "Iteration 1003/1072, Loss: 7.2009\n",
      "Iteration 1004/1072, Loss: 7.2195\n",
      "Iteration 1005/1072, Loss: 7.4104\n",
      "Iteration 1006/1072, Loss: 6.9744\n",
      "Iteration 1007/1072, Loss: 7.0936\n",
      "Iteration 1008/1072, Loss: 7.1288\n",
      "Iteration 1009/1072, Loss: 6.8315\n",
      "Iteration 1010/1072, Loss: 6.9019\n",
      "Iteration 1011/1072, Loss: 7.0028\n",
      "Iteration 1012/1072, Loss: 7.2118\n",
      "Iteration 1013/1072, Loss: 7.0262\n",
      "Iteration 1014/1072, Loss: 7.2120\n",
      "Iteration 1015/1072, Loss: 7.3816\n",
      "Iteration 1016/1072, Loss: 7.0986\n",
      "Iteration 1017/1072, Loss: 7.1385\n",
      "Iteration 1018/1072, Loss: 7.2504\n",
      "Iteration 1019/1072, Loss: 7.0956\n",
      "Iteration 1020/1072, Loss: 7.1840\n",
      "Iteration 1021/1072, Loss: 7.2626\n",
      "Iteration 1022/1072, Loss: 6.9895\n",
      "Iteration 1023/1072, Loss: 7.1283\n",
      "Iteration 1024/1072, Loss: 7.1661\n",
      "Iteration 1025/1072, Loss: 7.3391\n",
      "Iteration 1026/1072, Loss: 6.9045\n",
      "Iteration 1027/1072, Loss: 7.3342\n",
      "Iteration 1028/1072, Loss: 7.3608\n",
      "Iteration 1029/1072, Loss: 6.9400\n",
      "Iteration 1030/1072, Loss: 7.2822\n",
      "Iteration 1031/1072, Loss: 7.3433\n",
      "Iteration 1032/1072, Loss: 7.3627\n",
      "Iteration 1033/1072, Loss: 7.3208\n",
      "Iteration 1034/1072, Loss: 7.2642\n",
      "Iteration 1035/1072, Loss: 6.9183\n",
      "Iteration 1036/1072, Loss: 7.2520\n",
      "Iteration 1037/1072, Loss: 7.5094\n",
      "Iteration 1038/1072, Loss: 6.8469\n",
      "Iteration 1039/1072, Loss: 7.0577\n",
      "Iteration 1040/1072, Loss: 7.0100\n",
      "Iteration 1041/1072, Loss: 7.4908\n",
      "Iteration 1042/1072, Loss: 6.9220\n",
      "Iteration 1043/1072, Loss: 7.1628\n",
      "Iteration 1044/1072, Loss: 7.1423\n",
      "Iteration 1045/1072, Loss: 7.0309\n",
      "Iteration 1046/1072, Loss: 7.2820\n",
      "Iteration 1047/1072, Loss: 7.3277\n",
      "Iteration 1048/1072, Loss: 7.0380\n",
      "Iteration 1049/1072, Loss: 7.2256\n",
      "Iteration 1050/1072, Loss: 7.3667\n",
      "Iteration 1051/1072, Loss: 6.9741\n",
      "Iteration 1052/1072, Loss: 7.0678\n",
      "Iteration 1053/1072, Loss: 7.3038\n",
      "Iteration 1054/1072, Loss: 7.2957\n",
      "Iteration 1055/1072, Loss: 7.1700\n",
      "Iteration 1056/1072, Loss: 7.0726\n",
      "Iteration 1057/1072, Loss: 7.0711\n",
      "Iteration 1058/1072, Loss: 7.2210\n",
      "Iteration 1059/1072, Loss: 7.1268\n",
      "Iteration 1060/1072, Loss: 7.1096\n",
      "Iteration 1061/1072, Loss: 7.3040\n",
      "Iteration 1062/1072, Loss: 7.3822\n",
      "Iteration 1063/1072, Loss: 6.9852\n",
      "Iteration 1064/1072, Loss: 7.1428\n",
      "Iteration 1065/1072, Loss: 7.0982\n",
      "Iteration 1066/1072, Loss: 7.2794\n",
      "Iteration 1067/1072, Loss: 7.0825\n",
      "Iteration 1068/1072, Loss: 7.2434\n",
      "Iteration 1069/1072, Loss: 7.0770\n",
      "Iteration 1070/1072, Loss: 7.2365\n",
      "Iteration 1071/1072, Loss: 7.0425\n",
      "Iteration 1072/1072, Loss: 7.3939\n",
      "Epoch 10/10, Loss: 7.2194\n",
      "Checkpoint saved at checkpoints/efficientnet-b4_epoch_10.pth\n",
      "Validation Accuracy: 6.11%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        print(f\"Iteration {i+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(epoch_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "    # Save model checkpoint after each epoch\n",
    "    checkpoint_path = os.path.join(\"checkpoints\", f\"efficientnet-b4_epoch_{epoch+1}.pth\")\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "    print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "    \n",
    "    # Validation\n",
    "    val_dataset = CustomDataset('validation.csv', 'final/validation_images', transform=data_transforms)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_running_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_loss = val_running_loss / len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Validation Accuracy: {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update best accuracy and save best model checkpoint\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_checkpoint_path = os.path.join(\"checkpoints\", \"efficientnet-b4_best_checkpoint.pth\")\n",
    "        torch.save(model.state_dict(), best_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77deebe8-af15-4d77-9597-c2914cecbcd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(1, num_epochs+1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, num_epochs+1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de82ff44-8602-43da-912e-d58eef69647f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-13.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m108"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
